{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b713e1a",
   "metadata": {},
   "source": [
    "# 1. Preparation\n",
    "\n",
    "## 1.1 Prepare for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99a3dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install llama-index-llms-azure-openai\n",
    "# %pip install llama-index-graph-stores-nebula\n",
    "# %pip install llama-index-llms-openai\n",
    "# %pip install llama-index-embeddings-azure-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a002bd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For OpenAI\n",
    "\n",
    "import os\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"INSERT YOUR KEY\"\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout, level=logging.INFO\n",
    ")  # logging.DEBUG for more verbose output\n",
    "\n",
    "from llama_index.core import (\n",
    "    KnowledgeGraphIndex,\n",
    "    VectorStoreIndex,\n",
    "    ServiceContext,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    PromptTemplate\n",
    ")\n",
    "from llama_index.graph_stores.nebula import NebulaGraphStore\n",
    "\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from finllmqa.api.core import LLM_API_URL\n",
    "\n",
    "from llama_index.core import Settings\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", api_base=LLM_API_URL, api_key='null')\n",
    "embed_model = OpenAIEmbedding(api_base=LLM_API_URL, api_key='null')\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2f2a17",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# For Azure OpenAI\n",
    "\n",
    "import os\n",
    "import json\n",
    "import openai\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    KnowledgeGraphIndex,\n",
    "    ServiceContext\n",
    ")\n",
    "from llama_index.core import set_global_service_context\n",
    "\n",
    "from llama_index.core.storage.storage_context import StorageContext\n",
    "from llama_index.graph_stores.nebula import NebulaGraphStore\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from finllmqa.api.core import LLM_API_URL\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout, level=logging.INFO\n",
    ")  # logging.DEBUG for more verbose output\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = LLM_API_URL\n",
    "openai.api_version = \"2024-03-01\"\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"youcannottellanyone\"\n",
    "# openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_key = 'null'\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    engine=\"<foo-bar-deployment>\",\n",
    "    temperature=0,\n",
    "    openai_api_version=openai.api_version,\n",
    "    model_kwargs={\n",
    "        \"api_key\": openai.api_key,\n",
    "        \"api_base\": openai.api_base,\n",
    "        \"api_type\": openai.api_type,\n",
    "        \"api_version\": openai.api_version,\n",
    "    },\n",
    ")\n",
    "\n",
    "# You need to deploy your own embedding model as well as your own chat completion model\n",
    "embedding_llm = LangchainEmbedding(\n",
    "    OpenAIEmbeddings(\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        deployment=\"<foo-bar-deployment>\",\n",
    "        openai_api_key=openai.api_key,\n",
    "        openai_api_base=openai.api_base,\n",
    "        openai_api_type=openai.api_type,\n",
    "        openai_api_version=openai.api_version,\n",
    "    ),\n",
    "    embed_batch_size=1,\n",
    ")\n",
    "\n",
    "# service_context = ServiceContext.from_defaults(\n",
    "#     llm=llm,\n",
    "#     embed_model=embedding_llm,\n",
    "# )\n",
    "\n",
    "# set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dbbe82",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embedding_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafd15f5",
   "metadata": {},
   "source": [
    "## 1.2. Prepare for NebulaGraph as Graph Store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5561b495",
   "metadata": {},
   "source": [
    "❗Access NebulaGraph Console to **create space** and **graph schema**\n",
    "\n",
    "```sql\n",
    "CREATE SPACE guardians(vid_type=FIXED_STRING(256), partition_num=1, replica_factor=1);\n",
    ":sleep 10;\n",
    "USE guardians;\n",
    "CREATE TAG entity(name string);\n",
    "CREATE EDGE relationship(relationship string);\n",
    ":sleep 10;\n",
    "CREATE TAG INDEX entity_index ON entity(name(256));\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa40d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install nebula3-python ipython-ngql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801f88ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['NEBULA_USER'] = \"root\"\n",
    "os.environ['NEBULA_PASSWORD'] = \"nebula\" # default password\n",
    "os.environ['NEBULA_ADDRESS'] = \"192.168.30.158:9669\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cba664",
   "metadata": {},
   "source": [
    "## 2. Build the Knowledge Graph and Persist(Optional)\n",
    "\n",
    "**you can skip this step and move to $4 if you have downloaded and unzipped 'index.zip'**   \n",
    "\n",
    "In my work, the Knowledge Graph was created with LLM.\n",
    "\n",
    "We simply do so leveragint the `KnowledgeGraphIndex` from LlamaIndex, when creating it, Triplets will be extracted with LLM and evantually persisted into `NebulaGraphStore`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0af465",
   "metadata": {},
   "source": [
    "### 2.1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dd6ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# change path to where you save the teaching resources\n",
    "document_path = 'books/'\n",
    "file_name_ls = ['微观经济学.pdf']\n",
    "file_name_ls = [document_path + file_name for file_name in file_name_ls]\n",
    "\n",
    "reader = SimpleDirectoryReader(input_files=file_name_ls)\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28861e17",
   "metadata": {},
   "source": [
    "### 2.2 Split Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757f5733",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "\n",
    "chunk_size_ls = [256, 512, 1024]\n",
    "chunk_overlap_pct_ls = [1/8, 1/4]\n",
    "split_document_dc = {}\n",
    "for chunk_size in chunk_size_ls:\n",
    "    for chunk_overlap_pct in chunk_overlap_pct_ls:\n",
    "        chunk_overlap = int(chunk_size * chunk_overlap_pct)\n",
    "        nodes_group = f'size_{chunk_size}_overlap_{chunk_overlap}'\n",
    "        splitter = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        split_document = splitter.get_nodes_from_documents(documents=documents)\n",
    "        split_document_dc[nodes_group] = split_document\n",
    "        print(f'chunk_size: {chunk_size}; chunk_overlap: {chunk_overlap} len_chunks: {len(split_document)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98ab097",
   "metadata": {},
   "source": [
    "### 2.3 Extract Triplets and Save to NebulaGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1ad1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_extract_template = \"\"\"\n",
    "    下面提供了一些文本。根据文本，提取最多 {max_knowledge_triplets} 个三元组的知识，形式为(实体,关系,实体)，具体可以是(主语,谓语,宾语)或者其他类型，注意避开停用词。\n",
    "    请忽略page_label和file_path\n",
    "    ---------------------\n",
    "    示例：\n",
    "    文本：小红是小明的母亲.\n",
    "    三元组：\n",
    "    (小红,是母亲,小明)\n",
    "    文本:瑞幸是2017年在厦门创立的咖啡店。\n",
    "    三元组：\n",
    "    (瑞幸,是,咖啡店)\n",
    "    (瑞幸,创立于,厦门)\n",
    "    (瑞幸,创立于,2017)\n",
    "    文本:在长期中，物价总水平会调整到使货币需求等于货币供给的水平。\n",
    "    三元组：\n",
    "    (物价总水平,长期调整使等于,货币需求等于货币供给的水平)\n",
    "    ---------------------\n",
    "    文本：{text}\n",
    "    三元组：\"\"\"\n",
    "kg_extract_template = PromptTemplate(kg_extract_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115938c7",
   "metadata": {},
   "source": [
    "This cell will take some time, it'll extract entities and relationships and store them into NebulaGraph, but once you have run it before, you can skip this step and load storage_context from disk in $4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e98679",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "kg_index_ls = []\n",
    "\n",
    "for nodes_group, nodes in split_document_dc.items():\n",
    "    start = time.time()\n",
    "    print(f'\\n\\nstart extract {nodes_group} nodes...\\n\\n')\n",
    "    space_name = f\"books_content_{nodes_group}\"\n",
    "    edge_types, rel_prop_names = [\"relationship\"], [\"relationship\"] # default, could be omit if create from an empty kg\n",
    "    tags = [\"entity\"] # default, could be omit if create from an empty kg\n",
    "\n",
    "    graph_store = NebulaGraphStore(\n",
    "        space_name=space_name,\n",
    "        edge_types=edge_types,\n",
    "        rel_prop_names=rel_prop_names,\n",
    "        tags=tags,\n",
    "    )\n",
    "    storage_context = StorageContext.from_defaults(graph_store=graph_store)\n",
    "    kg_index = KnowledgeGraphIndex(\n",
    "        nodes=nodes,\n",
    "        storage_context=storage_context,\n",
    "        max_triplets_per_chunk=10,\n",
    "        space_name=space_name,\n",
    "        edge_types=edge_types,\n",
    "        rel_prop_names=rel_prop_names,\n",
    "        tags=tags,\n",
    "        include_embeddings=True,\n",
    "        kg_triple_extract_template=kg_extract_template\n",
    "    )\n",
    "    end = time.time()\n",
    "    print(f'{nodes_group} takes {(end-start)//60} min')\n",
    "    kg_index_ls.append(kg_index)\n",
    "\n",
    "    # store index\n",
    "    kg_index.storage_context.persist(persist_dir=f'../storage/storage_graph/{nodes_group}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db87d406",
   "metadata": {},
   "source": [
    "#### Multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16870a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile create_kg_index.py\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "os.environ['NEBULA_USER'] = \"root\"\n",
    "os.environ['NEBULA_PASSWORD'] = \"nebula\" # default password\n",
    "os.environ['NEBULA_ADDRESS'] = \"192.168.30.158:9669\" \n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout, level=logging.INFO\n",
    ")  # logging.DEBUG for more verbose output\n",
    "\n",
    "from llama_index.core import (\n",
    "    KnowledgeGraphIndex,\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    PromptTemplate)\n",
    "from llama_index.graph_stores.nebula import NebulaGraphStore\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from finllmqa.api.core import LLM_API_URL\n",
    "from llama_index.core import Settings\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", api_base=LLM_API_URL, api_key='null')\n",
    "embed_model = OpenAIEmbedding(api_base=LLM_API_URL, api_key='null')\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "# change path to where you save the teaching resources\n",
    "document_path = 'books/'\n",
    "file_name_ls = ['微观经济学.pdf']\n",
    "file_name_ls = [document_path + file_name for file_name in file_name_ls]\n",
    "\n",
    "reader = SimpleDirectoryReader(input_files=file_name_ls)\n",
    "documents = reader.load_data()\n",
    "\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "\n",
    "chunk_size_ls = [256, 512, 1024]\n",
    "chunk_overlap_pct_ls = [1/8, 1/4]\n",
    "split_document_dc = {}\n",
    "for chunk_size in chunk_size_ls:\n",
    "    for chunk_overlap_pct in chunk_overlap_pct_ls:\n",
    "        chunk_overlap = int(chunk_size * chunk_overlap_pct)\n",
    "        nodes_group = f'size_{chunk_size}_overlap_{chunk_overlap}'\n",
    "        if os.path.exists(f'../storage/storage_graph/{nodes_group}'):\n",
    "            continue\n",
    "        splitter = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        split_document = splitter.get_nodes_from_documents(documents=documents)\n",
    "        split_document_dc[nodes_group] = split_document\n",
    "        print(f'chunk_size: {chunk_size}; chunk_overlap: {chunk_overlap} len_chunks: {len(split_document)}')\n",
    "print(split_document_dc.keys())\n",
    "from threading import Thread\n",
    "\n",
    "def create_and_store_kg_index(nodes_group, nodes):\n",
    "    kg_extract_template = \"\"\"\n",
    "    下面提供了一些文本。根据文本，提取最多 {max_knowledge_triplets} 个三元组的知识，形式为(实体,关系,实体)，具体可以是(主语,谓语,宾语)或者其他类型，注意避开停用词。\n",
    "    请忽略page_label和file_path\n",
    "    ---------------------\n",
    "    示例：\n",
    "    文本：小红是小明的母亲.\n",
    "    三元组：\n",
    "    (小红,是母亲,小明)\n",
    "    文本:瑞幸是2017年在厦门创立的咖啡店。\n",
    "    三元组：\n",
    "    (瑞幸,是,咖啡店)\n",
    "    (瑞幸,创立于,厦门)\n",
    "    (瑞幸,创立于,2017)\n",
    "    文本:在长期中，物价总水平会调整到使货币需求等于货币供给的水平。\n",
    "    三元组：\n",
    "    (物价总水平,长期调整使等于,货币需求等于货币供给的水平)\n",
    "    ---------------------\n",
    "    文本：{text}\n",
    "    三元组：\"\"\"\n",
    "    kg_extract_template = PromptTemplate(kg_extract_template)\n",
    "\n",
    "    print(f'\\n\\nstart extract {nodes_group} nodes...\\n\\n')\n",
    "    space_name = f\"books_content_{nodes_group}\"\n",
    "    edge_types, rel_prop_names = [\"relationship\"], [\"relationship\"] # default, could be omit if create from an empty kg\n",
    "    tags = [\"entity\"] # default, could be omit if create from an empty kg\n",
    "\n",
    "    graph_store = NebulaGraphStore(\n",
    "        space_name=space_name,\n",
    "        edge_types=edge_types,\n",
    "        rel_prop_names=rel_prop_names,\n",
    "        tags=tags,\n",
    "    )\n",
    "    storage_context = StorageContext.from_defaults(graph_store=graph_store)\n",
    "    kg_index = KnowledgeGraphIndex(\n",
    "        nodes=nodes,\n",
    "        storage_context=storage_context,\n",
    "        max_triplets_per_chunk=10,\n",
    "        space_name=space_name,\n",
    "        edge_types=edge_types,\n",
    "        rel_prop_names=rel_prop_names,\n",
    "        tags=tags,\n",
    "        include_embeddings=True,\n",
    "        kg_triple_extract_template=kg_extract_template\n",
    "    )\n",
    "\n",
    "    # store index\n",
    "    kg_index.storage_context.persist(persist_dir=f'../storage/storage_graph/{nodes_group}')\n",
    "\n",
    "for nodes_group, nodes in split_document_dc.items():\n",
    "    thread = Thread(target=create_and_store_kg_index, args=(nodes_group, nodes))\n",
    "    thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab07ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run create_kg_index.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb691d6",
   "metadata": {},
   "source": [
    "## 3 Create VectorStoreIndex for RAG and Persist(Optional)\n",
    "\n",
    "**you can skip this step and move to $4 if you have downloaded and unzipped 'index.zip'**   \n",
    "\n",
    "To compare with/work together with VectorDB based RAG, let's also create a `VectorStoreIndex`.\n",
    "\n",
    "During the creation, same data source will be split into chunks and embedding of them will be created, during the RAG query time, the top-k related embeddings will be vector-searched with the embedding of the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204f3ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index_ls = []\n",
    "\n",
    "for nodes_group, nodes in split_document_dc.items():\n",
    "    print(f'\\n\\nstart extract {nodes_group} nodes...\\n\\n')\n",
    "    vector_index = VectorStoreIndex(nodes=nodes)\n",
    "    vector_index_ls.append(vector_index)\n",
    "\n",
    "    # store index\n",
    "    vector_index.storage_context.persist(persist_dir=f'../storage/storage_vector/{nodes_group}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ede3c10",
   "metadata": {},
   "source": [
    "## 4. Load from disk Llama Indexes\n",
    "\n",
    "**You have to run cells in $2 and $3 or download index.zip first**\n",
    "\n",
    "Both the `KnowledgeGraphIndex` and `VectorStoreIndex` will be created only once, afterwards, we could persist their in-memory context to enable their reuse from disk anytime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bc1c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import load_index_from_storage\n",
    "\n",
    "assert os.path.exists(os.path.join(os.path.abspath(os.path.join('..')), 'storage/storage_graph')), 'Do not have graph storage_context in disk'\n",
    "assert os.path.exists(os.path.join(os.path.abspath(os.path.join('..')), 'storage/storage_vector')), 'Do not have vector storage_context in disk'\n",
    "\n",
    "entries = os.listdir()\n",
    "folders = [entry for entry in entries if os.path.isdir(os.path.join(entry))]\n",
    "\n",
    "kg_index_ls = []\n",
    "vector_index_ls = []\n",
    "for nodes_group in folders:\n",
    "    space_name = f\"books_content_{nodes_group}\"\n",
    "    edge_types, rel_prop_names = [\"relationship\"], [\"relationship\"] # default, could be omit if create from an empty kg\n",
    "    tags = [\"entity\"] # default, could be omit if create from an empty kg\n",
    "\n",
    "    graph_store = NebulaGraphStore(\n",
    "        space_name=space_name,\n",
    "        edge_types=edge_types,\n",
    "        rel_prop_names=rel_prop_names,\n",
    "        tags=tags,\n",
    "    )\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=f'../storage/storage_graph/{nodes_group}', graph_store=graph_store)\n",
    "    kg_index = load_index_from_storage(\n",
    "        storage_context=storage_context,\n",
    "        space_name=space_name,\n",
    "        edge_types=edge_types,\n",
    "        rel_prop_names=rel_prop_names,\n",
    "        tags=tags,\n",
    "        include_embeddings=True,\n",
    "    )\n",
    "    kg_index_ls.append(kg_index)\n",
    "\n",
    "    storage_context_vector = StorageContext.from_defaults(persist_dir=f'../storage_vector/{nodes_group}')\n",
    "    vector_index = load_index_from_storage(\n",
    "    #     service_context=service_context,\n",
    "        storage_context=storage_context_vector\n",
    "    )\n",
    "    vector_index_ls.append(vector_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare for different query approaches\n",
    "\n",
    "We will do 4 types of query approaches with LLM, KG, VectorDB:\n",
    "\n",
    "| QueryEngine | Knowledge Graph query engine                                 | Graph RAG query engine                                       | Vector RAG query engine                                      | Graph Vector RAG query engine                                |\n",
    "| ----------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n",
    "| Mechanism   | 1. **Text-to-GraphQuery** based on KG<br />2. Query KG with the result<br />3. Answer synthesis based on query result | 1. Get related entities of the question<br />2. Get n-depth **SubGraphs** of related entities from KG<br />3. Answer synthesis based on related SubGraphs | 1. Create embedding of question<br />2. Semantic search **top-k related doc chunks**<br />3. Answer synthesis based on related doc chunks | 1. Do retrieval as Vector and Graph RAG <br />2. Answer synthesis based on **both related chunks and SubGraphs** |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 text-to-NebulaGraphCypher\n",
    "\n",
    "Text-to-NebulaGraphCypher approach Translate task/question into a Graph Cypher Query, and answer based on its query result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import KnowledgeGraphQueryEngine\n",
    "\n",
    "from llama_index.core import load_index_from_storage\n",
    "\n",
    "assert os.path.exists(os.path.join(os.path.abspath(os.path.join('..')), 'storage/storage_graph')), 'Do not have graph storage_context in disk'\n",
    "assert os.path.exists(os.path.join(os.path.abspath(os.path.join('..')), 'storage/storage_vector')), 'Do not have vector storage_context in disk'\n",
    "\n",
    "entries = os.listdir()\n",
    "folders = [entry for entry in entries if os.path.isdir(os.path.join(entry))]\n",
    "\n",
    "nl2kg_qg_ls = []\n",
    "for nodes_group in folders:\n",
    "    space_name = f\"books_content_{nodes_group}\"\n",
    "    edge_types, rel_prop_names = [\"relationship\"], [\"relationship\"] # default, could be omit if create from an empty kg\n",
    "    tags = [\"entity\"] # default, could be omit if create from an empty kg\n",
    "\n",
    "    graph_store = NebulaGraphStore(\n",
    "        space_name=space_name,\n",
    "        edge_types=edge_types,\n",
    "        rel_prop_names=rel_prop_names,\n",
    "        tags=tags,\n",
    "    )\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=f'../storage/storage_graph/{nodes_group}', graph_store=graph_store)\n",
    "\n",
    "    nl2kg_query_engine = KnowledgeGraphQueryEngine(\n",
    "        storage_context=storage_context,\n",
    "    #     service_context=service_context,\n",
    "        verbose=True\n",
    "    )\n",
    "    nl2kg_qg_ls.append(nl2kg_query_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl2kg_query_engine.get_prompts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Graph RAG query engine\n",
    "\n",
    "Graph RAG takes SubGraphs related to entities of the task/question as Context.\n",
    "\n",
    "```\n",
    "           Graph RAG with Llama Index\n",
    "                  ┌────┬────┬────┬────┐                  \n",
    "                  │ 1  │ 2  │ 3  │ 4  │                  \n",
    "                  ├────┴────┴────┴────┤                  \n",
    "                  │  Docs/Knowledge   │                  \n",
    "┌───────┐         │        ...        │       ┌─────────┐\n",
    "│       │         ├────┬────┬────┬────┤       │         │\n",
    "│       │         │ 95 │ 96 │    │    │       │         │\n",
    "│       │         └────┴────┴────┴────┘       │         │\n",
    "│ User  │─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─▶   LLM   │\n",
    "│       │                                     │         │\n",
    "│       │                                     │         │\n",
    "└───────┘    ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  └─────────┘\n",
    "    │          ┌──────────────────────────┐        ▲     \n",
    "    └────────┼▶│  Tell me about x, please │├───────┘     \n",
    "               └──────────────────────────┘              \n",
    "             │ Below are knowledge about x │             \n",
    "               x->y<-z,x->h->i, m<-n,...                            \n",
    "             │ Please answer based on them │             \n",
    "              ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_rag_qg_ls = []\n",
    "for kg_index in kg_index_ls:\n",
    "    kg_rag_query_engine = kg_index.as_query_engine(\n",
    "        include_text=False,\n",
    "        retriever_mode=\"hybrid\",\n",
    "        response_mode=\"tree_summarize\",\n",
    "    )\n",
    "    kg_rag_qg_ls.append(kg_rag_query_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Vector RAG query engine\n",
    "\n",
    "Vector RAG is the common approach to find topK semantic related doc chunks as context to synthesize the answer.\n",
    "\n",
    "```\n",
    "                  RAG with Llama Index\n",
    "                  ┌────┬────┬────┬────┐                  \n",
    "                  │ 1  │ 2  │ 3  │ 4  │                  \n",
    "                  ├────┴────┴────┴────┤                  \n",
    "                  │  Docs/Knowledge   │                  \n",
    "┌───────┐         │        ...        │       ┌─────────┐\n",
    "│       │         ├────┬────┬────┬────┤       │         │\n",
    "│       │         │ 95 │ 96 │    │    │       │         │\n",
    "│       │         └────┴────┴────┴────┘       │         │\n",
    "│ User  │─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─▶   LLM   │\n",
    "│       │                                     │         │\n",
    "│       │                                     │         │\n",
    "└───────┘    ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  └─────────┘\n",
    "    │          ┌──────────────────────────┐        ▲     \n",
    "    └────────┼▶│  Tell me ....., please   │├───────┘     \n",
    "               └──────────────────────────┘              \n",
    "             │ ┌────┐ ┌────┐               │             \n",
    "               │ 3  │ │ 96 │                             \n",
    "             │ └────┘ └────┘               │             \n",
    "              ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_rag_qg_ls = []\n",
    "for vetor_index in vector_index_ls:\n",
    "    vector_rag_query_engine = vector_index.as_query_engine()\n",
    "    vector_rag_qg_ls.append(vector_rag_query_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Graph+Vector RAG query engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a combined Graph+Vector Based RAG, where we will retrieve both VectorDB and KG SubGraphs as the context, for synthesis of the answer.\n",
    "\n",
    "```\n",
    "           Graph + Vector RAG with Llama Index\n",
    "                  ┌────┬────┬────┬────┐                  \n",
    "                  │ 1  │ 2  │ 3  │ 4  │                  \n",
    "                  ├────┴────┴────┴────┤                  \n",
    "                  │  Docs/Knowledge   │                  \n",
    "┌───────┐         │        ...        │       ┌─────────┐\n",
    "│       │         ├────┬────┬────┬────┤       │         │\n",
    "│       │         │ 95 │ 96 │    │    │       │         │\n",
    "│       │         └────┴────┴────┴────┘       │         │\n",
    "│ User  │─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─▶   LLM   │\n",
    "│       │                                     │         │\n",
    "│       │                                     │         │\n",
    "└───────┘    ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  └─────────┘\n",
    "    │          ┌──────────────────────────┐        ▲     \n",
    "    └────────┼▶│  Tell me ....., please   │├───────┘     \n",
    "               └──────────────────────────┘              \n",
    "             │ ┌────┐┌────┐               │             \n",
    "               │ 3  ││ 96 │ x->y<-z,x->h...                            \n",
    "             │ └────┘└────┘               │             \n",
    "              ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ \n",
    "```\n",
    "\n",
    "To implement that in Llama Index, we create a `CustomRetriever` to comebine the two: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import QueryBundle\n",
    "from llama_index.core import QueryBundle\n",
    "\n",
    "# import NodeWithScore\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "\n",
    "# Retrievers\n",
    "from llama_index.core.retrievers import BaseRetriever, VectorIndexRetriever, KGTableRetriever\n",
    "\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class CustomRetriever(BaseRetriever):\n",
    "    \"\"\"Custom retriever that performs both Vector search and Knowledge Graph search\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_retriever: VectorIndexRetriever,\n",
    "        kg_retriever: KGTableRetriever,\n",
    "        mode: str = \"OR\",\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "\n",
    "        self._vector_retriever = vector_retriever\n",
    "        self._kg_retriever = kg_retriever\n",
    "        if mode not in (\"AND\", \"OR\"):\n",
    "            raise ValueError(\"Invalid mode.\")\n",
    "        self._mode = mode\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query.\"\"\"\n",
    "\n",
    "        vector_nodes = self._vector_retriever.retrieve(query_bundle)\n",
    "        kg_nodes = self._kg_retriever.retrieve(query_bundle)\n",
    "\n",
    "        vector_ids = {n.node.node_id for n in vector_nodes}\n",
    "        kg_ids = {n.node.node_id for n in kg_nodes}\n",
    "\n",
    "        combined_dict = {n.node.node_id: n for n in vector_nodes}\n",
    "        combined_dict.update({n.node.node_id: n for n in kg_nodes})\n",
    "\n",
    "        if self._mode == \"AND\":\n",
    "            retrieve_ids = vector_ids.intersection(kg_ids)\n",
    "        else:\n",
    "            retrieve_ids = vector_ids.union(kg_ids)\n",
    "\n",
    "        retrieve_nodes = [combined_dict[rid] for rid in retrieve_ids]\n",
    "        return retrieve_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create instances of the Vector and KG retrievers, which will be used in the instantiation of the Custom Retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "kg_vec_rag_qg_ls = []\n",
    "for kg_index, vector_index in zip(kg_index_ls, vector_index_ls):\n",
    "    # create custom retriever\n",
    "    vector_retriever = VectorIndexRetriever(index=vector_index)\n",
    "    kg_retriever = KGTableRetriever(\n",
    "        index=kg_index, retriever_mode=\"keyword\", include_text=False\n",
    "    )\n",
    "    custom_retriever = CustomRetriever(vector_retriever, kg_retriever)\n",
    "\n",
    "    # create response synthesizer\n",
    "    response_synthesizer = get_response_synthesizer(\n",
    "    #     service_context=service_context,\n",
    "        response_mode=\"tree_summarize\",\n",
    "    )\n",
    "    kg_vector_rag_query_engine = RetrieverQueryEngine(\n",
    "    retriever=custom_retriever,\n",
    "    response_synthesizer=response_synthesizer\n",
    "    )\n",
    "    kg_vec_rag_qg_ls.append(kg_vector_rag_query_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ac5564",
   "metadata": {},
   "source": [
    "### 5.5 General load index from disk and get query engine function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e463125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_query_engine_from_cache_index(kg_index_folder_path, vector_index_folder_path, nodes_group: str|List[str]):\n",
    "    if isinstance(nodes_group, str):\n",
    "        nodes_group_ls = [nodes_group]\n",
    "    else:\n",
    "        nodes_group_ls = nodes_group\n",
    "    query_engine_dc = {\n",
    "        'nl2kg': [],\n",
    "        'kg_rag': [],\n",
    "        'vec_rag': [],\n",
    "        'kg_vec_rag': []\n",
    "    }\n",
    "    for nodes_group in nodes_group_ls:\n",
    "        space_name = f\"books_content_{nodes_group}\"\n",
    "        edge_types, rel_prop_names = [\"relationship\"], [\"relationship\"] # default, could be omit if create from an empty kg\n",
    "        tags = [\"entity\"] # default, could be omit if create from an empty kg\n",
    "\n",
    "        graph_store = NebulaGraphStore(\n",
    "            space_name=space_name,\n",
    "            edge_types=edge_types,\n",
    "            rel_prop_names=rel_prop_names,\n",
    "            tags=tags,\n",
    "        )\n",
    "        storage_context_kg = StorageContext.from_defaults(persist_dir=kg_index_folder_path + f'/{nodes_group}', graph_store=graph_store)\n",
    "        kg_index = load_index_from_storage(\n",
    "            storage_context=storage_context,\n",
    "            space_name=space_name,\n",
    "            edge_types=edge_types,\n",
    "            rel_prop_names=rel_prop_names,\n",
    "            tags=tags,\n",
    "            include_embeddings=True,\n",
    "        )\n",
    "\n",
    "        storage_context_vector = StorageContext.from_defaults(persist_dir=vector_index_folder_path + f'/{nodes_group}')\n",
    "        vector_index = load_index_from_storage(\n",
    "            storage_context=storage_context_vector\n",
    "        )\n",
    "\n",
    "        # text2cypher query engine\n",
    "        nl2kg_query_engine = KnowledgeGraphQueryEngine(\n",
    "            storage_context=storage_context_kg,\n",
    "            verbose=True\n",
    "        )\n",
    "        query_engine_dc['nl2kg'].append(nl2kg_query_engine)\n",
    "        # kg_rag query engine\n",
    "        kg_rag_query_engine = kg_index.as_query_engine(\n",
    "            include_text=False,\n",
    "            response_mode=\"tree_summarize\"\n",
    "        )\n",
    "        query_engine_dc['kg_rag'].append(kg_rag_query_engine)\n",
    "        # vec_rag query engine\n",
    "        vec_rag_query_engine = vector_index.as_query_engine(response_mode=\"tree_summarize\")\n",
    "        query_engine_dc['vec_rag'].append(vec_rag_query_engine)\n",
    "        # kg_vec_rag query engine\n",
    "        vector_retriever = VectorIndexRetriever(index=vector_index)\n",
    "        kg_retriever = KGTableRetriever(\n",
    "            index=kg_index, retriever_mode=\"keyword\", include_text=False\n",
    "        )\n",
    "        custom_retriever = CustomRetriever(vector_retriever, kg_retriever)\n",
    "\n",
    "        response_synthesizer = get_response_synthesizer(response_mode=\"tree_summarize\")\n",
    "        kg_vector_rag_query_engine = RetrieverQueryEngine(\n",
    "            retriever=custom_retriever,\n",
    "            response_synthesizer=response_synthesizer\n",
    "        )\n",
    "        query_engine_dc['kg_vec_rag'].append(kg_vector_rag_query_engine)\n",
    "    return query_engine_dc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Base Query with all the Engines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Text-to-GraphQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_nl2kg = nl2kg_query_engine.query(\"什么是经济学十大原理.\")\n",
    "\n",
    "\n",
    "display(Markdown(f\"<b>{response_nl2kg}</b>\"))\n",
    "\n",
    "# Cypher:\n",
    "\n",
    "print(\"Cypher Query:\")\n",
    "\n",
    "graph_query = nl2kg_query_engine.generate_query(\n",
    "    \"什么是经济学十大原理\",\n",
    ")\n",
    "graph_query = graph_query.replace(\"WHERE\", \"\\n  WHERE\").replace(\"RETURN\", \"\\nRETURN\")\n",
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        f\"\"\"\n",
    "```cypher\n",
    "{graph_query}\n",
    "```\n",
    "\"\"\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Graph RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_graph_rag = kg_rag_query_engine.query(\"什么是经济学十大原理\")\n",
    "\n",
    "display(Markdown(f\"<b>{response_graph_rag}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Vector RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_vector_rag = vector_rag_query_engine.query(\"什么是经济学十大原理\")\n",
    "\n",
    "display(Markdown(f\"<b>{response_vector_rag}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    Markdown(\n",
    "        llm.complete(f\"\"\"\n",
    "Compare the two QA result on \"什么是经济学十大原理\", list the differences between them, to help evalute them. Output in markdown table.\n",
    "\n",
    "Result from Graph: {response_graph_rag}\n",
    "---\n",
    "Result from Vector: {response_vector_rag}\n",
    "\n",
    "\"\"\"\n",
    "           ).text\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Graph + Vector RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_graph_vector_rag = graph_vector_rag_query_engine.query(\"什么是经济学十大原理\")\n",
    "\n",
    "display(Markdown(f\"<b>{response_graph_vector_rag}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Overall Comparision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the results of them.\n",
    "\n",
    "First check the information that were coverred by different approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    Markdown(\n",
    "        llm(f\"\"\"\n",
    "Compare the QA results on \"Tell me about Peter Quill.\", list the knowledge facts between them, to help evalute them. Output in markdown table.\n",
    "\n",
    "Result text2GraphQuery: {response_nl2kg}\n",
    "---\n",
    "Result Graph: {response_graph_rag}\n",
    "---\n",
    "Result Vector: {response_vector_rag}\n",
    "---\n",
    "Result Graph+Vector: {response_graph_vector_rag}\n",
    "---\n",
    "\n",
    "\"\"\"\n",
    "           )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "- The pure **KG**(both text2GraphQuery and Graph RAG) comes with **concise** results, and much **lower cost**(for cost comparision see our previous result [here](https://gpt-index.readthedocs.io/en/latest/examples/index_structs/knowledge_graph/KnowledgeGraphIndex_vs_VectorStoreIndex_vs_CustomIndex_combined.html#comparison-of-results) )\n",
    "- The **Graph+Vector** RAG could be more **comprehensive** in case the question envolves knowledge that's fine-grained **spread** across more chunks than top-K searching.\n",
    "\n",
    "\n",
    "| QueryEngine | Knowledge Graph query engine                                 | Graph RAG query engine                                       | Vector RAG query engine                                      | Graph Vector RAG query engine                                |\n",
    "| ----------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n",
    "| Mechanism   | 1. **Text-to-GraphQuery** based on KG<br />2. Query KG with the result<br />3. Answer synthesis based on query result | 1. Get related entities of the question<br />2. Get n-depth **SubGraphs** of related entities from KG<br />3. Answer synthesis based on related SubGraphs | 1. Create embedding of question<br />2. Semantic search **top-k related doc chunks**<br />3. Answer synthesis based on related doc chunks | 1. Do retrieval as Vector and Graph RAG <br />2. Answer synthesis based on **both related chunks and SubGraphs** |\n",
    "| Performance | Concise                                                      | Concise                                                      | Fruitful                                                     | Fruitful, could be more comprehensive                        |\n",
    "| Cost        | Low                                                          | Low                                                          | High                                                         | High                                                         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "For those tasks:\n",
    "\n",
    "- Potentially cares more relationed knowledge\n",
    "- Schema of the KG is sophisticated to be hard for text2cypher to express the task\n",
    "- KG quality isn't good enough\n",
    "- Multiple \"starting entities\" are involved\n",
    "\n",
    "Graph RAG could be a better approach to start with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b21d109",
   "metadata": {},
   "source": [
    "## 7. Financial Evaluation on four types of engines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7d3cd5",
   "metadata": {},
   "source": [
    "### 7.1 FinEval on query engines base on nodes of different chunk sizes and chunk overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590bcfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size_ls = [256, 512, 1024]\n",
    "chunk_overlap_pct_ls = [1/8, 1/4]\n",
    "nodes_group_ls = []\n",
    "for chunk_size in chunk_size_ls:\n",
    "    for chunk_overlap_pct in chunk_overlap_pct_ls:\n",
    "        chunk_overlap = int(chunk_size * chunk_overlap_pct)\n",
    "        nodes_group = f'size_{chunk_size}_overlap_{chunk_overlap}'\n",
    "        nodes_group_ls.append(nodes_group)\n",
    "query_engine_dc = get_all_query_engine_from_cache_index(kg_index_folder_path='../storage/storage_graph',\n",
    "                                                        vector_index_folder_path='../storage/storage_graph',\n",
    "                                                        nodes_group=nodes_group_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2911c5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from ..FinancialEvaluation.evaluators.query_engine import QueryEngineEvaluator\n",
    "\n",
    "choices = [\"A\", \"B\", \"C\", \"D\"]\n",
    "eval_path = '../FinancialEvaluation/'\n",
    "\n",
    "def fineval(args, evaluator, take):\n",
    "    assert os.path.exists(eval_path + \"subject_mapping.json\"), \"subject_mapping.json not found!\"\n",
    "    with open(eval_path+ \"subject_mapping.json\") as f:\n",
    "        subject_mapping = json.load(f)\n",
    "    filenames = os.listdir(eval_path + \"data/val\")\n",
    "    subject_list = [val_file.replace(\"_val.csv\", \"\") for val_file in filenames]\n",
    "    accuracy, summary = {}, {}\n",
    "\n",
    "    run_date = time.strftime('%Y-%m-%d_%H-%M-%S', time.localtime(time.time()))\n",
    "    output_dir = args.output_dir\n",
    "    save_result_dir = os.path.join(output_dir, f\"take{take}\")\n",
    "    if not os.path.exists(save_result_dir):\n",
    "        os.makedirs(save_result_dir, exist_ok=True)\n",
    "\n",
    "    print(f'############# nodes group: {args.nodes_group} ###############')\n",
    "\n",
    "    all_answers = {}\n",
    "    for index, subject_name in enumerate(subject_list):\n",
    "        print(\n",
    "            f\"{index / len(subject_list)} Inference starts at {run_date} on {args.model_name} with subject of {subject_name}!\")\n",
    "        val_file_path = os.path.join('data/val', f'{subject_name}_val.csv')\n",
    "        dev_file_path = os.path.join('data/dev', f'{subject_name}_dev.csv')\n",
    "        test_file_path = os.path.join('data/test', f'{subject_name}_test.csv')\n",
    "\n",
    "        val_df = pd.read_csv(val_file_path) if args.do_test is False else pd.read_csv(test_file_path)\n",
    "        dev_df = pd.read_csv(dev_file_path) if args.few_shot else None\n",
    "\n",
    "        correct_ratio, answers = evaluator.eval_subject(subject_name, val_df, dev_df,\n",
    "                                                        save_result_dir=save_result_dir if args.do_save_csv else None,\n",
    "                                                        few_shot=args.few_shot,\n",
    "                                                        cot=args.cot,\n",
    "                                                        )\n",
    "        print(f\"Subject: {subject_name}\")\n",
    "        print(f\"Acc: {correct_ratio}\")\n",
    "        accuracy[subject_name] = correct_ratio\n",
    "        summary[subject_name] = {\"score\": correct_ratio,\n",
    "                                 \"num\": len(val_df),\n",
    "                                 \"correct\": correct_ratio * len(val_df) / 100}\n",
    "        all_answers[subject_name] = answers\n",
    "\n",
    "    json.dump(all_answers, open(save_result_dir + '/submission.json', 'w'), ensure_ascii=False, indent=4)\n",
    "    print(\"Accuracy:\")\n",
    "    for k, v in accuracy.items():\n",
    "        print(k, \": \", v)\n",
    "\n",
    "    total_num = 0\n",
    "    total_correct = 0\n",
    "    summary['grouped'] = {\n",
    "        \"Accounting\": {\"correct\": 0.0, \"num\": 0},\n",
    "        \"Finance\": {\"correct\": 0.0, \"num\": 0},\n",
    "        \"Economy\": {\"correct\": 0.0, \"num\": 0},\n",
    "        \"Certificate\": {\"correct\": 0.0, \"num\": 0}\n",
    "    }\n",
    "    for subj, info in subject_mapping.items():\n",
    "        group = info[2]\n",
    "        summary['grouped'][group][\"num\"] += summary[subj]['num']\n",
    "        summary['grouped'][group][\"correct\"] += summary[subj]['correct']\n",
    "    for group, info in summary['grouped'].items():\n",
    "        info['score'] = info[\"correct\"] / info[\"num\"]\n",
    "        total_num += info[\"num\"]\n",
    "        total_correct += info[\"correct\"]\n",
    "    summary['All'] = {\"score\": total_correct / total_num, \"num\": total_num, \"correct\": total_correct}\n",
    "\n",
    "    print('-' * 80)\n",
    "    print(\"Accuracy_subject:\")\n",
    "    for k, v in accuracy.items():\n",
    "        print(k, \": \", v)\n",
    "    print('-' * 80)\n",
    "    print(\"Accuracy_grouped:\")\n",
    "    for k, v in summary['grouped'].items():\n",
    "        print(k, \": \", v['score'])\n",
    "\n",
    "    print(\"Avg: \")\n",
    "    print(summary['All']['score'])\n",
    "\n",
    "    json.dump(summary, open(save_result_dir + '/summary.json', 'w'), ensure_ascii=False, indent=2)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19908354",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--openai_key OPENAI_KEY]\n",
      "                             [--cot {False,True}] [--few_shot {False,True}]\n",
      "                             [--ntrain NTRAIN] [--n_times N_TIMES]\n",
      "                             [--do_save_csv {False,True}]\n",
      "                             [--output_dir OUTPUT_DIR]\n",
      "                             [--model_name MODEL_NAME]\n",
      "                             [--do_test {False,True}]\n",
      "ipykernel_launcher.py: error: argument --few_shot: invalid choice: 'c:\\\\Users\\\\intern-pm3\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-v2-17096Psv6nNu2GWxA.json' (choose from 'False', 'True')\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'tb_frame'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32md:\\Anaconda3\\envs\\chatglm3\\Lib\\argparse.py:1902\u001b[0m, in \u001b[0;36mArgumentParser.parse_known_args\u001b[1;34m(self, args, namespace)\u001b[0m\n\u001b[0;32m   1901\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1902\u001b[0m     namespace, args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_known_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1903\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ArgumentError \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\chatglm3\\Lib\\argparse.py:2114\u001b[0m, in \u001b[0;36mArgumentParser._parse_known_args\u001b[1;34m(self, arg_strings, namespace)\u001b[0m\n\u001b[0;32m   2113\u001b[0m     \u001b[38;5;66;03m# consume the next optional and any arguments for it\u001b[39;00m\n\u001b[1;32m-> 2114\u001b[0m     start_index \u001b[38;5;241m=\u001b[39m \u001b[43mconsume_optional\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2116\u001b[0m \u001b[38;5;66;03m# consume any positionals following the last Optional\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\chatglm3\\Lib\\argparse.py:2054\u001b[0m, in \u001b[0;36mArgumentParser._parse_known_args.<locals>.consume_optional\u001b[1;34m(start_index)\u001b[0m\n\u001b[0;32m   2053\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action, args, option_string \u001b[38;5;129;01min\u001b[39;00m action_tuples:\n\u001b[1;32m-> 2054\u001b[0m     \u001b[43mtake_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moption_string\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2055\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stop\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\chatglm3\\Lib\\argparse.py:1962\u001b[0m, in \u001b[0;36mArgumentParser._parse_known_args.<locals>.take_action\u001b[1;34m(action, argument_strings, option_string)\u001b[0m\n\u001b[0;32m   1961\u001b[0m seen_actions\u001b[38;5;241m.\u001b[39madd(action)\n\u001b[1;32m-> 1962\u001b[0m argument_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margument_strings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1964\u001b[0m \u001b[38;5;66;03m# error if this argument is not allowed with other previously\u001b[39;00m\n\u001b[0;32m   1965\u001b[0m \u001b[38;5;66;03m# seen arguments, assuming that actions that use the default\u001b[39;00m\n\u001b[0;32m   1966\u001b[0m \u001b[38;5;66;03m# value don't really count as \"present\"\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\chatglm3\\Lib\\argparse.py:2497\u001b[0m, in \u001b[0;36mArgumentParser._get_values\u001b[1;34m(self, action, arg_strings)\u001b[0m\n\u001b[0;32m   2496\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_value(action, arg_string)\n\u001b[1;32m-> 2497\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2499\u001b[0m \u001b[38;5;66;03m# REMAINDER arguments convert all values, checking none\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\chatglm3\\Lib\\argparse.py:2553\u001b[0m, in \u001b[0;36mArgumentParser._check_value\u001b[1;34m(self, action, value)\u001b[0m\n\u001b[0;32m   2552\u001b[0m msg \u001b[38;5;241m=\u001b[39m _(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minvalid choice: \u001b[39m\u001b[38;5;132;01m%(value)r\u001b[39;00m\u001b[38;5;124m (choose from \u001b[39m\u001b[38;5;132;01m%(choices)s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 2553\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ArgumentError(action, msg \u001b[38;5;241m%\u001b[39m args)\n",
      "\u001b[1;31mArgumentError\u001b[0m: argument --few_shot: invalid choice: 'c:\\\\Users\\\\intern-pm3\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-v2-17096Psv6nNu2GWxA.json' (choose from 'False', 'True')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[3], line 13\u001b[0m\n\u001b[0;32m     12\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--do_test\u001b[39m\u001b[38;5;124m\"\u001b[39m, choices\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFalse\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrue\u001b[39m\u001b[38;5;124m\"\u001b[39m], default\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFalse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m args\u001b[38;5;241m.\u001b[39mcot \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mcot \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\chatglm3\\Lib\\argparse.py:1869\u001b[0m, in \u001b[0;36mArgumentParser.parse_args\u001b[1;34m(self, args, namespace)\u001b[0m\n\u001b[0;32m   1868\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_args\u001b[39m(\u001b[38;5;28mself\u001b[39m, args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, namespace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m-> 1869\u001b[0m     args, argv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_known_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1870\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m argv:\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\chatglm3\\Lib\\argparse.py:1904\u001b[0m, in \u001b[0;36mArgumentParser.parse_known_args\u001b[1;34m(self, args, namespace)\u001b[0m\n\u001b[0;32m   1903\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ArgumentError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 1904\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1905\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\chatglm3\\Lib\\argparse.py:2630\u001b[0m, in \u001b[0;36mArgumentParser.error\u001b[1;34m(self, message)\u001b[0m\n\u001b[0;32m   2629\u001b[0m args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprog\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprog, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m: message}\n\u001b[1;32m-> 2630\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m%(prog)s\u001b[39;49;00m\u001b[38;5;124;43m: error: \u001b[39;49m\u001b[38;5;132;43;01m%(message)s\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\chatglm3\\Lib\\argparse.py:2617\u001b[0m, in \u001b[0;36mArgumentParser.exit\u001b[1;34m(self, status, message)\u001b[0m\n\u001b[0;32m   2616\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_print_message(message, _sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[1;32m-> 2617\u001b[0m \u001b[43m_sys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatus\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mSystemExit\u001b[0m: 2",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\chatglm3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:2145\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_only:\n\u001b[0;32m   2143\u001b[0m     stb \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAn exception has occurred, use \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mtb to see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2144\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe full traceback.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m-> 2145\u001b[0m     stb\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInteractiveTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_exception_only\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2146\u001b[0m \u001b[43m                                                     \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   2147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2149\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontains_exceptiongroup\u001b[39m(val):\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\chatglm3\\Lib\\site-packages\\IPython\\core\\ultratb.py:710\u001b[0m, in \u001b[0;36mListTB.get_exception_only\u001b[1;34m(self, etype, value)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_exception_only\u001b[39m(\u001b[38;5;28mself\u001b[39m, etype, value):\n\u001b[0;32m    703\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Only print the exception type and message, without a traceback.\u001b[39;00m\n\u001b[0;32m    704\u001b[0m \n\u001b[0;32m    705\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    708\u001b[0m \u001b[38;5;124;03m    value : exception value\u001b[39;00m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mListTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\chatglm3\\Lib\\site-packages\\IPython\\core\\ultratb.py:568\u001b[0m, in \u001b[0;36mListTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[0;32m    565\u001b[0m     chained_exc_ids\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mid\u001b[39m(exception[\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m    566\u001b[0m     chained_exceptions_tb_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    567\u001b[0m     out_list \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 568\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    569\u001b[0m \u001b[43m            \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    570\u001b[0m \u001b[43m            \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchained_exc_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[0;32m    572\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchained_exceptions_tb_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    573\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    575\u001b[0m         \u001b[38;5;241m+\u001b[39m chained_exception_message\n\u001b[0;32m    576\u001b[0m         \u001b[38;5;241m+\u001b[39m out_list)\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out_list\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\chatglm3\\Lib\\site-packages\\IPython\\core\\ultratb.py:1454\u001b[0m, in \u001b[0;36mAutoFormattedTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1452\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1453\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtb \u001b[38;5;241m=\u001b[39m etb\n\u001b[1;32m-> 1454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFormattedTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1455\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\n\u001b[0;32m   1456\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\chatglm3\\Lib\\site-packages\\IPython\\core\\ultratb.py:1345\u001b[0m, in \u001b[0;36mFormattedTB.structured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1342\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\n\u001b[0;32m   1343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose_modes:\n\u001b[0;32m   1344\u001b[0m     \u001b[38;5;66;03m# Verbose modes need a full traceback\u001b[39;00m\n\u001b[1;32m-> 1345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVerboseTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1346\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\n\u001b[0;32m   1347\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1348\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMinimal\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m   1349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ListTB\u001b[38;5;241m.\u001b[39mget_exception_only(\u001b[38;5;28mself\u001b[39m, etype, value)\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\chatglm3\\Lib\\site-packages\\IPython\\core\\ultratb.py:1192\u001b[0m, in \u001b[0;36mVerboseTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstructured_traceback\u001b[39m(\n\u001b[0;32m   1184\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1185\u001b[0m     etype: \u001b[38;5;28mtype\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1189\u001b[0m     number_of_lines_of_context: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m   1190\u001b[0m ):\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1192\u001b[0m     formatted_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_exception_as_a_whole\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1193\u001b[0m \u001b[43m                                                           \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1195\u001b[0m     colors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mColors  \u001b[38;5;66;03m# just a shorthand + quicker name lookup\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m     colorsnormal \u001b[38;5;241m=\u001b[39m colors\u001b[38;5;241m.\u001b[39mNormal  \u001b[38;5;66;03m# used a lot\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\chatglm3\\Lib\\site-packages\\IPython\\core\\ultratb.py:1082\u001b[0m, in \u001b[0;36mVerboseTB.format_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1079\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tb_offset, \u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m   1080\u001b[0m head \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_header(\u001b[38;5;28mstr\u001b[39m(etype), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlong_header)\n\u001b[0;32m   1081\u001b[0m records \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1082\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_records\u001b[49m\u001b[43m(\u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m etb \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m   1083\u001b[0m )\n\u001b[0;32m   1085\u001b[0m frames \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1086\u001b[0m skipped \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\chatglm3\\Lib\\site-packages\\IPython\\core\\ultratb.py:1150\u001b[0m, in \u001b[0;36mVerboseTB.get_records\u001b[1;34m(self, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1148\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1149\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1150\u001b[0m         mod \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mgetmodule(\u001b[43mcf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtb_frame\u001b[49m)\n\u001b[0;32m   1151\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mod \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1152\u001b[0m             mod_name \u001b[38;5;241m=\u001b[39m mod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'tb_frame'"
     ]
    }
   ],
   "source": [
    "cot = False\n",
    "few_shot = False\n",
    "ntrain = 5\n",
    "n_times = 1\n",
    "do_save_csv = False\n",
    "output_dir = eval_path + 'output'\n",
    "model_name = 'chatglm'\n",
    "do_test = False\n",
    "args = dict(\n",
    "    cot=cot,\n",
    "    few_shot = few_shot,\n",
    "    ntrain = ntrain,\n",
    "    n_times = n_times,\n",
    "    do_save_csv = do_save_csv,\n",
    "    output_dir = output_dir,\n",
    "    model_name = model_name,\n",
    "    do_test = do_test\n",
    ")\n",
    "\n",
    "tree_summary_template = \\\n",
    "    \"从不同来源获取的参考信息如下:\\n\" \\\n",
    "    \"---------------------\\n\" \\\n",
    "    \"{context_str}\\n\" \\\n",
    "    \"---------------------\\n\" \\\n",
    "    \"题目:{query_str}\" \n",
    "\n",
    "graph_query_synthesis_prompt = \\\n",
    "    \"\"\"\n",
    "    你是一个专业的人工智能助手\n",
    "    现在有一个基于Nebula搭建的财经百科知识图谱，给定这个图谱框架结构，请你根据框架结构将问题文本转化成Nebula Cypher查询语句\n",
    "    保证查询语可以直接在Nebula终端中运行\n",
    "\n",
    "    知识图谱框架:{schema}\n",
    "    问题: {query_str}\n",
    "    \"\"\"\n",
    "\n",
    "graph_response_answer_prompt = \\\n",
    "    \"\"\"\n",
    "    原问题被转化成了查询语句，查询语句和查询结果将作为参考信息，如下:\n",
    "\n",
    "    查询语句: {kg_query_str}\n",
    "    查询结果: {kg_response_str}\n",
    "    题目: {query_str}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4a6953",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = QueryEngineEvaluator(\n",
    "    choices=choices,\n",
    "    k=args.ntrain,\n",
    "    model_name=args.model_name\n",
    ")\n",
    "for i in range(args.n_times):\n",
    "    fineval(args, evaluator=evaluator, take=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063d5cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatglm3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
