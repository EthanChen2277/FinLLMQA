{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b713e1a",
   "metadata": {},
   "source": [
    "# 1. Preparation\n",
    "\n",
    "## 1.1 Prepare for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99a3dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install llama-index-llms-azure-openai\n",
    "# %pip install llama-index-graph-stores-nebula\n",
    "# %pip install llama-index-llms-openai\n",
    "# %pip install llama-index-embeddings-azure-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a002bd52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\chatglm3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# For OpenAI\n",
    "\n",
    "import os\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"INSERT YOUR KEY\"\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout, level=logging.INFO\n",
    ")  # logging.DEBUG for more verbose output\n",
    "\n",
    "from llama_index.core import (\n",
    "    KnowledgeGraphIndex,\n",
    "    VectorStoreIndex,\n",
    "    ServiceContext,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    PromptTemplate\n",
    ")\n",
    "from llama_index.graph_stores.nebula import NebulaGraphStore\n",
    "\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from finllmqa.api.core import LLM_API_URL\n",
    "\n",
    "from llama_index.core import Settings\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", api_base=LLM_API_URL, api_key='null')\n",
    "embed_model = OpenAIEmbedding(api_base=LLM_API_URL, api_key='null')\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2f2a17",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# For Azure OpenAI\n",
    "\n",
    "import os\n",
    "import json\n",
    "import openai\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    KnowledgeGraphIndex,\n",
    "    ServiceContext\n",
    ")\n",
    "from llama_index.core import set_global_service_context\n",
    "\n",
    "from llama_index.core.storage.storage_context import StorageContext\n",
    "from llama_index.graph_stores.nebula import NebulaGraphStore\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from finllmqa.api.core import LLM_API_URL\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout, level=logging.INFO\n",
    ")  # logging.DEBUG for more verbose output\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = LLM_API_URL\n",
    "openai.api_version = \"2024-03-01\"\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"youcannottellanyone\"\n",
    "# openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_key = 'null'\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    engine=\"<foo-bar-deployment>\",\n",
    "    temperature=0,\n",
    "    openai_api_version=openai.api_version,\n",
    "    model_kwargs={\n",
    "        \"api_key\": openai.api_key,\n",
    "        \"api_base\": openai.api_base,\n",
    "        \"api_type\": openai.api_type,\n",
    "        \"api_version\": openai.api_version,\n",
    "    },\n",
    ")\n",
    "\n",
    "# You need to deploy your own embedding model as well as your own chat completion model\n",
    "embedding_llm = LangchainEmbedding(\n",
    "    OpenAIEmbeddings(\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        deployment=\"<foo-bar-deployment>\",\n",
    "        openai_api_key=openai.api_key,\n",
    "        openai_api_base=openai.api_base,\n",
    "        openai_api_type=openai.api_type,\n",
    "        openai_api_version=openai.api_version,\n",
    "    ),\n",
    "    embed_batch_size=1,\n",
    ")\n",
    "\n",
    "# service_context = ServiceContext.from_defaults(\n",
    "#     llm=llm,\n",
    "#     embed_model=embedding_llm,\n",
    "# )\n",
    "\n",
    "# set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dbbe82",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embedding_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafd15f5",
   "metadata": {},
   "source": [
    "## 1.2. Prepare for NebulaGraph as Graph Store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5561b495",
   "metadata": {},
   "source": [
    "❗Access NebulaGraph Console to **create space** and **graph schema**\n",
    "\n",
    "```sql\n",
    "CREATE SPACE guardians(vid_type=FIXED_STRING(256), partition_num=1, replica_factor=1);\n",
    ":sleep 10;\n",
    "USE guardians;\n",
    "CREATE TAG entity(name string);\n",
    "CREATE EDGE relationship(relationship string);\n",
    ":sleep 10;\n",
    "CREATE TAG INDEX entity_index ON entity(name(256));\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa40d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install nebula3-python ipython-ngql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "801f88ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['NEBULA_USER'] = \"root\"\n",
    "os.environ['NEBULA_PASSWORD'] = \"nebula\" # default password\n",
    "os.environ['NEBULA_ADDRESS'] = \"192.168.30.158:9669\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cba664",
   "metadata": {},
   "source": [
    "## 2. Build the Knowledge Graph(Optional)\n",
    "\n",
    "**you can skip this step and move to $4 if you have downloaded and unzipped 'index.zip'**   \n",
    "\n",
    "In my work, the Knowledge Graph was created with LLM.\n",
    "\n",
    "We simply do so leveragint the `KnowledgeGraphIndex` from LlamaIndex, when creating it, Triplets will be extracted with LLM and evantually persisted into `NebulaGraphStore`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0af465",
   "metadata": {},
   "source": [
    "### 2.1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04dd6ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# change path to where you save the teaching resources\n",
    "document_path = 'books/'\n",
    "file_name_ls = ['微观经济学.pdf']\n",
    "file_name_ls = [document_path + file_name for file_name in file_name_ls]\n",
    "\n",
    "reader = SimpleDirectoryReader(input_files=file_name_ls)\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28861e17",
   "metadata": {},
   "source": [
    "### 2.2 Split Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "757f5733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size: 256; chunk_overlap: 32 len_chunks: 2941\n",
      "chunk_size: 256; chunk_overlap: 64 len_chunks: 3277\n",
      "chunk_size: 512; chunk_overlap: 64 len_chunks: 1508\n",
      "chunk_size: 512; chunk_overlap: 128 len_chunks: 1588\n",
      "chunk_size: 1024; chunk_overlap: 128 len_chunks: 871\n",
      "chunk_size: 1024; chunk_overlap: 256 len_chunks: 874\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "\n",
    "chunk_size_ls = [256, 512, 1024]\n",
    "chunk_overlap_pct_ls = [1/8, 1/4]\n",
    "split_document_dc = {}\n",
    "for chunk_size in chunk_size_ls:\n",
    "    for chunk_overlap_pct in chunk_overlap_pct_ls:\n",
    "        chunk_overlap = int(chunk_size * chunk_overlap_pct)\n",
    "        splitter = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        split_document = splitter.get_nodes_from_documents(documents=documents)\n",
    "        split_document_dc[f'size_{chunk_size}_overlap_{chunk_overlap}'] = split_document\n",
    "        print(f'chunk_size: {chunk_size}; chunk_overlap: {chunk_overlap} len_chunks: {len(split_document)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98ab097",
   "metadata": {},
   "source": [
    "### 2.3 Extract Triplets and Save to NebulaGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f1ad1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_extract_template = \"\"\"\n",
    "    下面提供了一些文本。根据文本，提取最多 {max_knowledge_triplets} 个三元组的知识，形式为(实体,关系,实体)，具体可以是(主语,谓语,宾语)或者其他类型，注意避开停用词。\n",
    "    请忽略page_label和file_path\n",
    "    ---------------------\n",
    "    示例：\n",
    "    文本：小红是小明的母亲.\n",
    "    三元组：\n",
    "    (小红,是母亲,小明)\n",
    "    文本:瑞幸是2017年在厦门创立的咖啡店。\n",
    "    三元组：\n",
    "    (瑞幸,是,咖啡店)\n",
    "    (瑞幸,创立于,厦门)\n",
    "    (瑞幸,创立于,2017)\n",
    "    文本:在长期中，物价总水平会调整到使货币需求等于货币供给的水平。\n",
    "    三元组：\n",
    "    (物价总水平,长期调整使等于,货币需求等于货币供给的水平)\n",
    "    ---------------------\n",
    "    文本：{text}\n",
    "    三元组：\"\"\"\n",
    "kg_extract_template = PromptTemplate(kg_extract_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115938c7",
   "metadata": {},
   "source": [
    "This cell will take some time, it'll extract entities and relationships and store them into NebulaGraph, but once you have run it before, you can skip this step and load storage_context from disk in $4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e98679",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "kg_index_ls = []\n",
    "\n",
    "for nodes_group, nodes in split_document_dc.items():\n",
    "    start = time.time()\n",
    "    print(f'\\n\\nstart extract {nodes_group} nodes...\\n\\n')\n",
    "    space_name = f\"books_content_{nodes_group}\"\n",
    "    edge_types, rel_prop_names = [\"relationship\"], [\"relationship\"] # default, could be omit if create from an empty kg\n",
    "    tags = [\"entity\"] # default, could be omit if create from an empty kg\n",
    "\n",
    "    graph_store = NebulaGraphStore(\n",
    "        space_name=space_name,\n",
    "        edge_types=edge_types,\n",
    "        rel_prop_names=rel_prop_names,\n",
    "        tags=tags,\n",
    "    )\n",
    "    storage_context = StorageContext.from_defaults(graph_store=graph_store)\n",
    "    kg_index = KnowledgeGraphIndex(\n",
    "        nodes=nodes,\n",
    "        storage_context=storage_context,\n",
    "        max_triplets_per_chunk=10,\n",
    "        space_name=space_name,\n",
    "        edge_types=edge_types,\n",
    "        rel_prop_names=rel_prop_names,\n",
    "        tags=tags,\n",
    "        include_embeddings=True,\n",
    "        kg_triple_extract_template=kg_extract_template\n",
    "    )\n",
    "    end = time.time()\n",
    "    print(f'{nodes_group} takes {(end-start)//60} min')\n",
    "    kg_index_ls.append(kg_index)\n",
    "\n",
    "    # store index\n",
    "    kg_index.storage_context.persist(persist_dir=f'../index/storage_graph/{nodes_group}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db87d406",
   "metadata": {},
   "source": [
    "#### Multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16870a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.pool import ThreadPool\n",
    "def create_and_store_kg_index(nodes_group, nodes):\n",
    "    print(f'\\n\\nstart extract {nodes_group} nodes...\\n\\n')\n",
    "    space_name = f\"books_content_{nodes_group}\"\n",
    "    edge_types, rel_prop_names = [\"relationship\"], [\"relationship\"] # default, could be omit if create from an empty kg\n",
    "    tags = [\"entity\"] # default, could be omit if create from an empty kg\n",
    "\n",
    "    graph_store = NebulaGraphStore(\n",
    "        space_name=space_name,\n",
    "        edge_types=edge_types,\n",
    "        rel_prop_names=rel_prop_names,\n",
    "        tags=tags,\n",
    "    )\n",
    "    storage_context = StorageContext.from_defaults(graph_store=graph_store)\n",
    "    kg_index = KnowledgeGraphIndex(\n",
    "        nodes=nodes,\n",
    "        storage_context=storage_context,\n",
    "        max_triplets_per_chunk=10,\n",
    "        space_name=space_name,\n",
    "        edge_types=edge_types,\n",
    "        rel_prop_names=rel_prop_names,\n",
    "        tags=tags,\n",
    "        include_embeddings=True,\n",
    "        kg_triple_extract_template=kg_extract_template\n",
    "    )\n",
    "    kg_index_ls.append(kg_index)\n",
    "\n",
    "    # store index\n",
    "    kg_index.storage_context.persist(persist_dir=f'../index/storage_graph/{nodes_group}')\n",
    "\n",
    "pool = ThreadPool(len(split_document_dc))\n",
    "pool.map(create_and_store_kg_index, list(split_document_dc.items()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb691d6",
   "metadata": {},
   "source": [
    "## 3 Create VectorStoreIndex for RAG(Optional)\n",
    "\n",
    "**you can skip this step and move to $4 if you have downloaded and unzipped 'index.zip'**   \n",
    "\n",
    "To compare with/work together with VectorDB based RAG, let's also create a `VectorStoreIndex`.\n",
    "\n",
    "During the creation, same data source will be split into chunks and embedding of them will be created, during the RAG query time, the top-k related embeddings will be vector-searched with the embedding of the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204f3ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index_ls = []\n",
    "\n",
    "for nodes_group, nodes in split_document_dc.items():\n",
    "    print(f'\\n\\nstart extract {nodes_group} nodes...\\n\\n')\n",
    "    vector_index = VectorStoreIndex(nodes=nodes)\n",
    "    vector_index_ls.append(vector_index)\n",
    "\n",
    "    # store index\n",
    "    vector_index.storage_context.persist(persist_dir=f'../index/storage_vector/{nodes_group}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ede3c10",
   "metadata": {},
   "source": [
    "## 4. Persist and Load from disk Llama Indexes\n",
    "\n",
    "**You have to run cells in $2 and $3 or download index.zip first**\n",
    "\n",
    "Both the `KnowledgeGraphIndex` and `VectorStoreIndex` will be created only once, afterwards, we could persist their in-memory context to enable their reuse from disk anytime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bc1c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import load_index_from_storage\n",
    "\n",
    "assert os.path.exists(os.path.join(os.path.abspath(os.path.join('..')), 'index/storage_graph')), 'Do not have graph storage_context in disk'\n",
    "assert os.path.exists(os.path.join(os.path.abspath(os.path.join('..')), 'index/storage_vector')), 'Do not have vector storage_context in disk'\n",
    "\n",
    "entries = os.listdir()\n",
    "folders = [entry for entry in entries if os.path.isdir(os.path.join(entry))]\n",
    "\n",
    "kg_index_ls = []\n",
    "vector_index_ls = []\n",
    "for nodes_group in folders:\n",
    "    space_name = f\"books_content_{nodes_group}\"\n",
    "    edge_types, rel_prop_names = [\"relationship\"], [\"relationship\"] # default, could be omit if create from an empty kg\n",
    "    tags = [\"entity\"] # default, could be omit if create from an empty kg\n",
    "\n",
    "    graph_store = NebulaGraphStore(\n",
    "        space_name=space_name,\n",
    "        edge_types=edge_types,\n",
    "        rel_prop_names=rel_prop_names,\n",
    "        tags=tags,\n",
    "    )\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=f'../index/storage_graph/{nodes_group}', graph_store=graph_store)\n",
    "    kg_index = load_index_from_storage(\n",
    "        storage_context=storage_context,\n",
    "        space_name=space_name,\n",
    "        edge_types=edge_types,\n",
    "        rel_prop_names=rel_prop_names,\n",
    "        tags=tags,\n",
    "        include_embeddings=True,\n",
    "    )\n",
    "    kg_index_ls.append(kg_index)\n",
    "\n",
    "    storage_context_vector = StorageContext.from_defaults(persist_dir=f'../storage_vector/{nodes_group}')\n",
    "    vector_index = load_index_from_storage(\n",
    "    #     service_context=service_context,\n",
    "        storage_context=storage_context_vector\n",
    "    )\n",
    "    vector_index_ls.append(vector_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare for different query approaches\n",
    "\n",
    "We will do 4 types of query approaches with LLM, KG, VectorDB:\n",
    "\n",
    "| QueryEngine | Knowledge Graph query engine                                 | Graph RAG query engine                                       | Vector RAG query engine                                      | Graph Vector RAG query engine                                |\n",
    "| ----------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n",
    "| Mechanism   | 1. **Text-to-GraphQuery** based on KG<br />2. Query KG with the result<br />3. Answer synthesis based on query result | 1. Get related entities of the question<br />2. Get n-depth **SubGraphs** of related entities from KG<br />3. Answer synthesis based on related SubGraphs | 1. Create embedding of question<br />2. Semantic search **top-k related doc chunks**<br />3. Answer synthesis based on related doc chunks | 1. Do retrieval as Vector and Graph RAG <br />2. Answer synthesis based on **both related chunks and SubGraphs** |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 text-to-NebulaGraphCypher\n",
    "\n",
    "Text-to-NebulaGraphCypher approach Translate task/question into a Graph Cypher Query, and answer based on its query result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import KnowledgeGraphQueryEngine\n",
    "\n",
    "from llama_index.core import load_index_from_storage\n",
    "\n",
    "assert os.path.exists(os.path.join(os.path.abspath(os.path.join('..')), 'index/storage_graph')), 'Do not have graph storage_context in disk'\n",
    "assert os.path.exists(os.path.join(os.path.abspath(os.path.join('..')), 'index/storage_vector')), 'Do not have vector storage_context in disk'\n",
    "\n",
    "entries = os.listdir()\n",
    "folders = [entry for entry in entries if os.path.isdir(os.path.join(entry))]\n",
    "\n",
    "nl2kg_qg_ls = []\n",
    "for nodes_group in folders:\n",
    "    space_name = f\"books_content_{nodes_group}\"\n",
    "    edge_types, rel_prop_names = [\"relationship\"], [\"relationship\"] # default, could be omit if create from an empty kg\n",
    "    tags = [\"entity\"] # default, could be omit if create from an empty kg\n",
    "\n",
    "    graph_store = NebulaGraphStore(\n",
    "        space_name=space_name,\n",
    "        edge_types=edge_types,\n",
    "        rel_prop_names=rel_prop_names,\n",
    "        tags=tags,\n",
    "    )\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=f'../index/storage_graph/{nodes_group}', graph_store=graph_store)\n",
    "\n",
    "    nl2kg_query_engine = KnowledgeGraphQueryEngine(\n",
    "        storage_context=storage_context,\n",
    "    #     service_context=service_context,\n",
    "        verbose=True\n",
    "    )\n",
    "    nl2kg_qg_ls.append(nl2kg_query_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl2kg_query_engine.get_prompts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Graph RAG query engine\n",
    "\n",
    "Graph RAG takes SubGraphs related to entities of the task/question as Context.\n",
    "\n",
    "```\n",
    "           Graph RAG with Llama Index\n",
    "                  ┌────┬────┬────┬────┐                  \n",
    "                  │ 1  │ 2  │ 3  │ 4  │                  \n",
    "                  ├────┴────┴────┴────┤                  \n",
    "                  │  Docs/Knowledge   │                  \n",
    "┌───────┐         │        ...        │       ┌─────────┐\n",
    "│       │         ├────┬────┬────┬────┤       │         │\n",
    "│       │         │ 95 │ 96 │    │    │       │         │\n",
    "│       │         └────┴────┴────┴────┘       │         │\n",
    "│ User  │─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─▶   LLM   │\n",
    "│       │                                     │         │\n",
    "│       │                                     │         │\n",
    "└───────┘    ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  └─────────┘\n",
    "    │          ┌──────────────────────────┐        ▲     \n",
    "    └────────┼▶│  Tell me about x, please │├───────┘     \n",
    "               └──────────────────────────┘              \n",
    "             │ Below are knowledge about x │             \n",
    "               x->y<-z,x->h->i, m<-n,...                            \n",
    "             │ Please answer based on them │             \n",
    "              ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_rag_qg_ls = []\n",
    "for kg_index in kg_index_ls:\n",
    "    kg_rag_query_engine = kg_index.as_query_engine(\n",
    "        include_text=False,\n",
    "        retriever_mode=\"keyword\",\n",
    "        response_mode=\"tree_summarize\",\n",
    "    )\n",
    "    kg_rag_qg_ls.append(kg_rag_query_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Vector RAG query engine\n",
    "\n",
    "Vector RAG is the common approach to find topK semantic related doc chunks as context to synthesize the answer.\n",
    "\n",
    "```\n",
    "                  RAG with Llama Index\n",
    "                  ┌────┬────┬────┬────┐                  \n",
    "                  │ 1  │ 2  │ 3  │ 4  │                  \n",
    "                  ├────┴────┴────┴────┤                  \n",
    "                  │  Docs/Knowledge   │                  \n",
    "┌───────┐         │        ...        │       ┌─────────┐\n",
    "│       │         ├────┬────┬────┬────┤       │         │\n",
    "│       │         │ 95 │ 96 │    │    │       │         │\n",
    "│       │         └────┴────┴────┴────┘       │         │\n",
    "│ User  │─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─▶   LLM   │\n",
    "│       │                                     │         │\n",
    "│       │                                     │         │\n",
    "└───────┘    ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  └─────────┘\n",
    "    │          ┌──────────────────────────┐        ▲     \n",
    "    └────────┼▶│  Tell me ....., please   │├───────┘     \n",
    "               └──────────────────────────┘              \n",
    "             │ ┌────┐ ┌────┐               │             \n",
    "               │ 3  │ │ 96 │                             \n",
    "             │ └────┘ └────┘               │             \n",
    "              ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_rag_qg_ls = []\n",
    "for vetor_index in vector_index_ls:\n",
    "    vector_rag_query_engine = vector_index.as_query_engine()\n",
    "    vector_rag_qg_ls.append(vector_rag_query_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Graph+Vector RAG query engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a combined Graph+Vector Based RAG, where we will retrieve both VectorDB and KG SubGraphs as the context, for synthesis of the answer.\n",
    "\n",
    "```\n",
    "           Graph + Vector RAG with Llama Index\n",
    "                  ┌────┬────┬────┬────┐                  \n",
    "                  │ 1  │ 2  │ 3  │ 4  │                  \n",
    "                  ├────┴────┴────┴────┤                  \n",
    "                  │  Docs/Knowledge   │                  \n",
    "┌───────┐         │        ...        │       ┌─────────┐\n",
    "│       │         ├────┬────┬────┬────┤       │         │\n",
    "│       │         │ 95 │ 96 │    │    │       │         │\n",
    "│       │         └────┴────┴────┴────┘       │         │\n",
    "│ User  │─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─▶   LLM   │\n",
    "│       │                                     │         │\n",
    "│       │                                     │         │\n",
    "└───────┘    ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  └─────────┘\n",
    "    │          ┌──────────────────────────┐        ▲     \n",
    "    └────────┼▶│  Tell me ....., please   │├───────┘     \n",
    "               └──────────────────────────┘              \n",
    "             │ ┌────┐┌────┐               │             \n",
    "               │ 3  ││ 96 │ x->y<-z,x->h...                            \n",
    "             │ └────┘└────┘               │             \n",
    "              ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ \n",
    "```\n",
    "\n",
    "To implement that in Llama Index, we create a `CustomRetriever` to comebine the two: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import QueryBundle\n",
    "from llama_index.core import QueryBundle\n",
    "\n",
    "# import NodeWithScore\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "\n",
    "# Retrievers\n",
    "from llama_index.core.retrievers import BaseRetriever, VectorIndexRetriever, KGTableRetriever\n",
    "\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class CustomRetriever(BaseRetriever):\n",
    "    \"\"\"Custom retriever that performs both Vector search and Knowledge Graph search\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_retriever: VectorIndexRetriever,\n",
    "        kg_retriever: KGTableRetriever,\n",
    "        mode: str = \"OR\",\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "\n",
    "        self._vector_retriever = vector_retriever\n",
    "        self._kg_retriever = kg_retriever\n",
    "        if mode not in (\"AND\", \"OR\"):\n",
    "            raise ValueError(\"Invalid mode.\")\n",
    "        self._mode = mode\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query.\"\"\"\n",
    "\n",
    "        vector_nodes = self._vector_retriever.retrieve(query_bundle)\n",
    "        kg_nodes = self._kg_retriever.retrieve(query_bundle)\n",
    "\n",
    "        vector_ids = {n.node.node_id for n in vector_nodes}\n",
    "        kg_ids = {n.node.node_id for n in kg_nodes}\n",
    "\n",
    "        combined_dict = {n.node.node_id: n for n in vector_nodes}\n",
    "        combined_dict.update({n.node.node_id: n for n in kg_nodes})\n",
    "\n",
    "        if self._mode == \"AND\":\n",
    "            retrieve_ids = vector_ids.intersection(kg_ids)\n",
    "        else:\n",
    "            retrieve_ids = vector_ids.union(kg_ids)\n",
    "\n",
    "        retrieve_nodes = [combined_dict[rid] for rid in retrieve_ids]\n",
    "        return retrieve_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create instances of the Vector and KG retrievers, which will be used in the instantiation of the Custom Retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "kg_vec_rag_qg_ls = []\n",
    "for kg_index, vector_index in zip(kg_index_ls, vector_index_ls):\n",
    "    # create custom retriever\n",
    "    vector_retriever = VectorIndexRetriever(index=vector_index)\n",
    "    kg_retriever = KGTableRetriever(\n",
    "        index=kg_index, retriever_mode=\"keyword\", include_text=False\n",
    "    )\n",
    "    custom_retriever = CustomRetriever(vector_retriever, kg_retriever)\n",
    "\n",
    "    # create response synthesizer\n",
    "    response_synthesizer = get_response_synthesizer(\n",
    "    #     service_context=service_context,\n",
    "        response_mode=\"tree_summarize\",\n",
    "    )\n",
    "    kg_vector_rag_query_engine = RetrieverQueryEngine(\n",
    "    retriever=custom_retriever,\n",
    "    response_synthesizer=response_synthesizer\n",
    "    )\n",
    "    kg_vec_rag_qg_ls.append(kg_vector_rag_query_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ac5564",
   "metadata": {},
   "source": [
    "### 5.5 General load index from disk and get query engine function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e463125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_query_engine_from_cache_index(kg_index_folder_path, vector_index_folder_path, nodes_group: str|List[str]):\n",
    "    if isinstance(nodes_group, str):\n",
    "        nodes_group_ls = [nodes_group]\n",
    "    else:\n",
    "        nodes_group_ls = nodes_group\n",
    "    query_engine_dc = {\n",
    "        'nl2kg': [],\n",
    "        'kg_rag': [],\n",
    "        'vec_rag': [],\n",
    "        'kg_vec_rag': []\n",
    "    }\n",
    "    for nodes_group in nodes_group_ls:\n",
    "        space_name = f\"books_content_{nodes_group}\"\n",
    "        edge_types, rel_prop_names = [\"relationship\"], [\"relationship\"] # default, could be omit if create from an empty kg\n",
    "        tags = [\"entity\"] # default, could be omit if create from an empty kg\n",
    "\n",
    "        graph_store = NebulaGraphStore(\n",
    "            space_name=space_name,\n",
    "            edge_types=edge_types,\n",
    "            rel_prop_names=rel_prop_names,\n",
    "            tags=tags,\n",
    "        )\n",
    "        storage_context_kg = StorageContext.from_defaults(persist_dir=kg_index_folder_path + f'/{nodes_group}', graph_store=graph_store)\n",
    "        kg_index = load_index_from_storage(\n",
    "            storage_context=storage_context,\n",
    "            space_name=space_name,\n",
    "            edge_types=edge_types,\n",
    "            rel_prop_names=rel_prop_names,\n",
    "            tags=tags,\n",
    "            include_embeddings=True,\n",
    "        )\n",
    "\n",
    "        storage_context_vector = StorageContext.from_defaults(persist_dir=vector_index_folder_path + f'{nodes_group}')\n",
    "        vector_index = load_index_from_storage(\n",
    "            storage_context=storage_context_vector\n",
    "        )\n",
    "\n",
    "        # text2cypher query engine\n",
    "        nl2kg_query_engine = KnowledgeGraphQueryEngine(\n",
    "        storage_context=storage_context_kg,\n",
    "        verbose=True\n",
    "        )\n",
    "\n",
    "        #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Base Query with all the Engines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Text-to-GraphQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_nl2kg = nl2kg_query_engine.query(\"什么是经济学十大原理.\")\n",
    "\n",
    "\n",
    "display(Markdown(f\"<b>{response_nl2kg}</b>\"))\n",
    "\n",
    "# Cypher:\n",
    "\n",
    "print(\"Cypher Query:\")\n",
    "\n",
    "graph_query = nl2kg_query_engine.generate_query(\n",
    "    \"什么是经济学十大原理\",\n",
    ")\n",
    "graph_query = graph_query.replace(\"WHERE\", \"\\n  WHERE\").replace(\"RETURN\", \"\\nRETURN\")\n",
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        f\"\"\"\n",
    "```cypher\n",
    "{graph_query}\n",
    "```\n",
    "\"\"\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Graph RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_graph_rag = kg_rag_query_engine.query(\"什么是经济学十大原理\")\n",
    "\n",
    "display(Markdown(f\"<b>{response_graph_rag}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Vector RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_vector_rag = vector_rag_query_engine.query(\"什么是经济学十大原理\")\n",
    "\n",
    "display(Markdown(f\"<b>{response_vector_rag}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    Markdown(\n",
    "        llm.complete(f\"\"\"\n",
    "Compare the two QA result on \"什么是经济学十大原理\", list the differences between them, to help evalute them. Output in markdown table.\n",
    "\n",
    "Result from Graph: {response_graph_rag}\n",
    "---\n",
    "Result from Vector: {response_vector_rag}\n",
    "\n",
    "\"\"\"\n",
    "           ).text\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Graph + Vector RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_graph_vector_rag = graph_vector_rag_query_engine.query(\"什么是经济学十大原理\")\n",
    "\n",
    "display(Markdown(f\"<b>{response_graph_vector_rag}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Overall Comparision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the results of them.\n",
    "\n",
    "First check the information that were coverred by different approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    Markdown(\n",
    "        llm(f\"\"\"\n",
    "Compare the QA results on \"Tell me about Peter Quill.\", list the knowledge facts between them, to help evalute them. Output in markdown table.\n",
    "\n",
    "Result text2GraphQuery: {response_nl2kg}\n",
    "---\n",
    "Result Graph: {response_graph_rag}\n",
    "---\n",
    "Result Vector: {response_vector_rag}\n",
    "---\n",
    "Result Graph+Vector: {response_graph_vector_rag}\n",
    "---\n",
    "\n",
    "\"\"\"\n",
    "           )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "- The pure **KG**(both text2GraphQuery and Graph RAG) comes with **concise** results, and much **lower cost**(for cost comparision see our previous result [here](https://gpt-index.readthedocs.io/en/latest/examples/index_structs/knowledge_graph/KnowledgeGraphIndex_vs_VectorStoreIndex_vs_CustomIndex_combined.html#comparison-of-results) )\n",
    "- The **Graph+Vector** RAG could be more **comprehensive** in case the question envolves knowledge that's fine-grained **spread** across more chunks than top-K searching.\n",
    "\n",
    "\n",
    "| QueryEngine | Knowledge Graph query engine                                 | Graph RAG query engine                                       | Vector RAG query engine                                      | Graph Vector RAG query engine                                |\n",
    "| ----------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n",
    "| Mechanism   | 1. **Text-to-GraphQuery** based on KG<br />2. Query KG with the result<br />3. Answer synthesis based on query result | 1. Get related entities of the question<br />2. Get n-depth **SubGraphs** of related entities from KG<br />3. Answer synthesis based on related SubGraphs | 1. Create embedding of question<br />2. Semantic search **top-k related doc chunks**<br />3. Answer synthesis based on related doc chunks | 1. Do retrieval as Vector and Graph RAG <br />2. Answer synthesis based on **both related chunks and SubGraphs** |\n",
    "| Performance | Concise                                                      | Concise                                                      | Fruitful                                                     | Fruitful, could be more comprehensive                        |\n",
    "| Cost        | Low                                                          | Low                                                          | High                                                         | High                                                         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "For those tasks:\n",
    "\n",
    "- Potentially cares more relationed knowledge\n",
    "- Schema of the KG is sophisticated to be hard for text2cypher to express the task\n",
    "- KG quality isn't good enough\n",
    "- Multiple \"starting entities\" are involved\n",
    "\n",
    "Graph RAG could be a better approach to start with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b21d109",
   "metadata": {},
   "source": [
    "## 7. Financial Evaluation on four types of engines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7d3cd5",
   "metadata": {},
   "source": [
    "### 7.1 FinEval on query engines base on nodes of different chunk sizes and chunk overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590bcfff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatglm3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
