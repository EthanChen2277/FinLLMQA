{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b713e1a",
   "metadata": {},
   "source": [
    "# 1. Preparation\n",
    "\n",
    "## 1.1 Prepare for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99a3dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install llama-index-llms-azure-openai\n",
    "# %pip install llama-index-graph-stores-nebula\n",
    "# %pip install llama-index-llms-openai\n",
    "# %pip install llama-index-embeddings-azure-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a002bd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For OpenAI\n",
    "\n",
    "import os\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"INSERT YOUR KEY\"\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout, level=logging.INFO\n",
    ")  # logging.DEBUG for more verbose output\n",
    "\n",
    "from llama_index.core import (\n",
    "    KnowledgeGraphIndex,\n",
    "    VectorStoreIndex,\n",
    "    ServiceContext,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    PromptTemplate,\n",
    "    QueryBundle\n",
    ")\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "from llama_index.core.retrievers import BaseRetriever, VectorIndexRetriever, KGTableRetriever\n",
    "\n",
    "from llama_index.graph_stores.nebula import NebulaGraphStore\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from finllmqa.api.core import LLM_API_URL\n",
    "\n",
    "from llama_index.core import Settings\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", api_base=LLM_API_URL, api_key='null')\n",
    "embed_model = OpenAIEmbedding(api_base=LLM_API_URL, api_key='null')\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2f2a17",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# For Azure OpenAI\n",
    "\n",
    "import os\n",
    "import json\n",
    "import openai\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    KnowledgeGraphIndex\n",
    ")\n",
    "\n",
    "from llama_index.core.storage.storage_context import StorageContext\n",
    "from llama_index.graph_stores.nebula import NebulaGraphStore\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from finllmqa.api.core import LLM_API_URL\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout, level=logging.INFO\n",
    ")  # logging.DEBUG for more verbose output\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = LLM_API_URL\n",
    "openai.api_version = \"2024-03-01\"\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"youcannottellanyone\"\n",
    "# openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_key = 'null'\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    engine=\"<foo-bar-deployment>\",\n",
    "    temperature=0,\n",
    "    openai_api_version=openai.api_version,\n",
    "    model_kwargs={\n",
    "        \"api_key\": openai.api_key,\n",
    "        \"api_base\": openai.api_base,\n",
    "        \"api_type\": openai.api_type,\n",
    "        \"api_version\": openai.api_version,\n",
    "    },\n",
    ")\n",
    "\n",
    "# You need to deploy your own embedding model as well as your own chat completion model\n",
    "embedding_llm = LangchainEmbedding(\n",
    "    OpenAIEmbeddings(\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        deployment=\"<foo-bar-deployment>\",\n",
    "        openai_api_key=openai.api_key,\n",
    "        openai_api_base=openai.api_base,\n",
    "        openai_api_type=openai.api_type,\n",
    "        openai_api_version=openai.api_version,\n",
    "    ),\n",
    "    embed_batch_size=1,\n",
    ")\n",
    "\n",
    "# service_context = ServiceContext.from_defaults(\n",
    "#     llm=llm,\n",
    "#     embed_model=embedding_llm,\n",
    "# )\n",
    "\n",
    "# set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dbbe82",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embedding_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafd15f5",
   "metadata": {},
   "source": [
    "## 1.2. Prepare for NebulaGraph as Graph Store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5561b495",
   "metadata": {},
   "source": [
    "❗Access NebulaGraph Console to **create space** and **graph schema**\n",
    "\n",
    "```sql\n",
    "CREATE SPACE guardians(vid_type=FIXED_STRING(256), partition_num=1, replica_factor=1);\n",
    ":sleep 10;\n",
    "USE guardians;\n",
    "CREATE TAG entity(name string);\n",
    "CREATE EDGE relationship(relationship string);\n",
    ":sleep 10;\n",
    "CREATE TAG INDEX entity_index ON entity(name(256));\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa40d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install nebula3-python ipython-ngql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801f88ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['NEBULA_USER'] = \"root\"\n",
    "os.environ['NEBULA_PASSWORD'] = \"nebula\" # default password\n",
    "os.environ['NEBULA_ADDRESS'] = \"192.168.30.158:9669\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cba664",
   "metadata": {},
   "source": [
    "## 2. Build the Knowledge Graph and Persist\n",
    "\n",
    "In my work, the Knowledge Graph was created with LLM.\n",
    "\n",
    "We simply do so leveragint the `KnowledgeGraphIndex` from LlamaIndex, when creating it, Triplets will be extracted with LLM and evantually persisted into `NebulaGraphStore`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0af465",
   "metadata": {},
   "source": [
    "### 2.1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dd6ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# change path to where you save the teaching resources\n",
    "document_path = 'books/'\n",
    "file_name_ls = ['微观经济学.pdf']\n",
    "file_name_ls = [document_path + file_name for file_name in file_name_ls]\n",
    "\n",
    "reader = SimpleDirectoryReader(input_files=file_name_ls)\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28861e17",
   "metadata": {},
   "source": [
    "### 2.2 Split Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757f5733",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "\n",
    "chunk_size_ls = [256, 512, 1024]\n",
    "chunk_overlap_pct_ls = [1/8, 1/4]\n",
    "split_document_dc = {}\n",
    "for chunk_size in chunk_size_ls:\n",
    "    for chunk_overlap_pct in chunk_overlap_pct_ls:\n",
    "        chunk_overlap = int(chunk_size * chunk_overlap_pct)\n",
    "        nodes_group = f'size_{chunk_size}_overlap_{chunk_overlap}'\n",
    "        splitter = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        split_document = splitter.get_nodes_from_documents(documents=documents)\n",
    "        split_document_dc[nodes_group] = split_document\n",
    "        print(f'chunk_size: {chunk_size}; chunk_overlap: {chunk_overlap} len_chunks: {len(split_document)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98ab097",
   "metadata": {},
   "source": [
    "### 2.3 Extract Triplets and Save to NebulaGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1ad1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_extract_template = \"\"\"\n",
    "    下面提供了一些文本。根据文本，提取最多 {max_knowledge_triplets} 个三元组的知识，形式为(实体,关系,实体)，具体可以是(主语,谓语,宾语)或者其他类型，注意避开停用词。\n",
    "    请忽略page_label和file_path\n",
    "    ---------------------\n",
    "    示例：\n",
    "    文本：小红是小明的母亲.\n",
    "    三元组：\n",
    "    (小红,是母亲,小明)\n",
    "    文本:瑞幸是2017年在厦门创立的咖啡店。\n",
    "    三元组：\n",
    "    (瑞幸,是,咖啡店)\n",
    "    (瑞幸,创立于,厦门)\n",
    "    (瑞幸,创立于,2017)\n",
    "    文本:在长期中，物价总水平会调整到使货币需求等于货币供给的水平。\n",
    "    三元组：\n",
    "    (物价总水平,长期调整使等于,货币需求等于货币供给的水平)\n",
    "    ---------------------\n",
    "    文本：{text}\n",
    "    三元组：\"\"\"\n",
    "kg_extract_template = PromptTemplate(kg_extract_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115938c7",
   "metadata": {},
   "source": [
    "This cell will take some time, it'll extract entities and relationships and store them into NebulaGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e98679",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "kg_index_ls = []\n",
    "\n",
    "for nodes_group, nodes in split_document_dc.items():\n",
    "    start = time.time()\n",
    "    print(f'\\n\\nstart extract {nodes_group} nodes...\\n\\n')\n",
    "    space_name = f\"books_content_{nodes_group}\"\n",
    "    edge_types, rel_prop_names = [\"relationship\"], [\"relationship\"] # default, could be omit if create from an empty kg\n",
    "    tags = [\"entity\"] # default, could be omit if create from an empty kg\n",
    "\n",
    "    graph_store = NebulaGraphStore(\n",
    "        space_name=space_name,\n",
    "        edge_types=edge_types,\n",
    "        rel_prop_names=rel_prop_names,\n",
    "        tags=tags,\n",
    "    )\n",
    "    storage_context = StorageContext.from_defaults(graph_store=graph_store)\n",
    "    kg_index = KnowledgeGraphIndex(\n",
    "        nodes=nodes,\n",
    "        storage_context=storage_context,\n",
    "        max_triplets_per_chunk=10,\n",
    "        space_name=space_name,\n",
    "        edge_types=edge_types,\n",
    "        rel_prop_names=rel_prop_names,\n",
    "        tags=tags,\n",
    "        include_embeddings=True,\n",
    "        kg_triple_extract_template=kg_extract_template\n",
    "    )\n",
    "    end = time.time()\n",
    "    print(f'{nodes_group} takes {(end-start)//60} min')\n",
    "    kg_index_ls.append(kg_index)\n",
    "\n",
    "    # store index\n",
    "    kg_index.storage_context.persist(persist_dir=f'../storage/storage_graph/{nodes_group}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db87d406",
   "metadata": {},
   "source": [
    "#### Multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16870a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile create_kg_index.py\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "os.environ['NEBULA_USER'] = \"root\"\n",
    "os.environ['NEBULA_PASSWORD'] = \"nebula\" # default password\n",
    "os.environ['NEBULA_ADDRESS'] = \"192.168.30.158:9669\" \n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout, level=logging.INFO\n",
    ")  # logging.DEBUG for more verbose output\n",
    "\n",
    "from llama_index.core import (\n",
    "    KnowledgeGraphIndex,\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    PromptTemplate)\n",
    "from llama_index.graph_stores.nebula import NebulaGraphStore\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from finllmqa.api.core import LLM_API_URL\n",
    "from llama_index.core import Settings\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", api_base=LLM_API_URL, api_key='null')\n",
    "embed_model = OpenAIEmbedding(api_base=LLM_API_URL, api_key='null')\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "# change path to where you save the teaching resources\n",
    "document_path = 'books/'\n",
    "file_name_ls = ['微观经济学.pdf']\n",
    "file_name_ls = [document_path + file_name for file_name in file_name_ls]\n",
    "\n",
    "reader = SimpleDirectoryReader(input_files=file_name_ls)\n",
    "documents = reader.load_data()\n",
    "\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "\n",
    "chunk_size_ls = [256, 512, 1024]\n",
    "chunk_overlap_pct_ls = [1/8, 1/4]\n",
    "split_document_dc = {}\n",
    "for chunk_size in chunk_size_ls:\n",
    "    for chunk_overlap_pct in chunk_overlap_pct_ls:\n",
    "        chunk_overlap = int(chunk_size * chunk_overlap_pct)\n",
    "        nodes_group = f'size_{chunk_size}_overlap_{chunk_overlap}'\n",
    "        if os.path.exists(f'../storage/storage_graph/{nodes_group}'):\n",
    "            continue\n",
    "        splitter = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        split_document = splitter.get_nodes_from_documents(documents=documents)\n",
    "        split_document_dc[nodes_group] = split_document\n",
    "        print(f'chunk_size: {chunk_size}; chunk_overlap: {chunk_overlap} len_chunks: {len(split_document)}')\n",
    "print(split_document_dc.keys())\n",
    "from threading import Thread\n",
    "\n",
    "def create_and_store_kg_index(nodes_group, nodes):\n",
    "    kg_extract_template = \"\"\"\n",
    "    下面提供了一些文本。根据文本，提取最多 {max_knowledge_triplets} 个三元组的知识，形式为(实体,关系,实体)，具体可以是(主语,谓语,宾语)或者其他类型，注意避开停用词。\n",
    "    请忽略page_label和file_path\n",
    "    ---------------------\n",
    "    示例：\n",
    "    文本：小红是小明的母亲.\n",
    "    三元组：\n",
    "    (小红,是母亲,小明)\n",
    "    文本:瑞幸是2017年在厦门创立的咖啡店。\n",
    "    三元组：\n",
    "    (瑞幸,是,咖啡店)\n",
    "    (瑞幸,创立于,厦门)\n",
    "    (瑞幸,创立于,2017)\n",
    "    文本:在长期中，物价总水平会调整到使货币需求等于货币供给的水平。\n",
    "    三元组：\n",
    "    (物价总水平,长期调整使等于,货币需求等于货币供给的水平)\n",
    "    ---------------------\n",
    "    文本：{text}\n",
    "    三元组：\"\"\"\n",
    "    kg_extract_template = PromptTemplate(kg_extract_template)\n",
    "\n",
    "    print(f'\\n\\nstart extract {nodes_group} nodes...\\n\\n')\n",
    "    space_name = f\"books_content_{nodes_group}\"\n",
    "    edge_types, rel_prop_names = [\"relationship\"], [\"relationship\"] # default, could be omit if create from an empty kg\n",
    "    tags = [\"entity\"] # default, could be omit if create from an empty kg\n",
    "\n",
    "    graph_store = NebulaGraphStore(\n",
    "        space_name=space_name,\n",
    "        edge_types=edge_types,\n",
    "        rel_prop_names=rel_prop_names,\n",
    "        tags=tags,\n",
    "    )\n",
    "    storage_context = StorageContext.from_defaults(graph_store=graph_store)\n",
    "    kg_index = KnowledgeGraphIndex(\n",
    "        nodes=nodes,\n",
    "        storage_context=storage_context,\n",
    "        max_triplets_per_chunk=10,\n",
    "        space_name=space_name,\n",
    "        edge_types=edge_types,\n",
    "        rel_prop_names=rel_prop_names,\n",
    "        tags=tags,\n",
    "        include_embeddings=True,\n",
    "        kg_triple_extract_template=kg_extract_template\n",
    "    )\n",
    "\n",
    "    # store index\n",
    "    kg_index.storage_context.persist(persist_dir=f'../storage/storage_graph/{nodes_group}')\n",
    "\n",
    "for nodes_group, nodes in split_document_dc.items():\n",
    "    thread = Thread(target=create_and_store_kg_index, args=(nodes_group, nodes))\n",
    "    thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab07ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run create_kg_index.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb691d6",
   "metadata": {},
   "source": [
    "## 3 Create VectorStoreIndex for RAG and Persist\n",
    "\n",
    "To compare with/work together with VectorDB based RAG, let's also create a `VectorStoreIndex`.\n",
    "\n",
    "During the creation, same data source will be split into chunks and embedding of them will be created, during the RAG query time, the top-k related embeddings will be vector-searched with the embedding of the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204f3ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index_ls = []\n",
    "\n",
    "for nodes_group, nodes in split_document_dc.items():\n",
    "    print(f'\\n\\nstart extract {nodes_group} nodes...\\n\\n')\n",
    "    vector_index = VectorStoreIndex(nodes=nodes)\n",
    "    vector_index_ls.append(vector_index)\n",
    "\n",
    "    # store index\n",
    "    vector_index.storage_context.persist(persist_dir=f'../storage/storage_vector/{nodes_group}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3aa4c2e",
   "metadata": {},
   "source": [
    "### 6.5 Overall Comparision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the results of them.\n",
    "\n",
    "First check the information that were coverred by different approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    Markdown(\n",
    "        llm(f\"\"\"\n",
    "Compare the QA results on \"Tell me about Peter Quill.\", list the knowledge facts between them, to help evalute them. Output in markdown table.\n",
    "\n",
    "Result text2GraphQuery: {response_nl2kg}\n",
    "---\n",
    "Result Graph: {response_graph_rag}\n",
    "---\n",
    "Result Vector: {response_vector_rag}\n",
    "---\n",
    "Result Graph+Vector: {response_graph_vector_rag}\n",
    "---\n",
    "\n",
    "\"\"\"\n",
    "           )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "- The pure **KG**(both text2GraphQuery and Graph RAG) comes with **concise** results, and much **lower cost**(for cost comparision see our previous result [here](https://gpt-index.readthedocs.io/en/latest/examples/index_structs/knowledge_graph/KnowledgeGraphIndex_vs_VectorStoreIndex_vs_CustomIndex_combined.html#comparison-of-results) )\n",
    "- The **Graph+Vector** RAG could be more **comprehensive** in case the question envolves knowledge that's fine-grained **spread** across more chunks than top-K searching.\n",
    "\n",
    "\n",
    "| QueryEngine | Knowledge Graph query engine                                 | Graph RAG query engine                                       | Vector RAG query engine                                      | Graph Vector RAG query engine                                |\n",
    "| ----------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n",
    "| Mechanism   | 1. **Text-to-GraphQuery** based on KG<br />2. Query KG with the result<br />3. Answer synthesis based on query result | 1. Get related entities of the question<br />2. Get n-depth **SubGraphs** of related entities from KG<br />3. Answer synthesis based on related SubGraphs | 1. Create embedding of question<br />2. Semantic search **top-k related doc chunks**<br />3. Answer synthesis based on related doc chunks | 1. Do retrieval as Vector and Graph RAG <br />2. Answer synthesis based on **both related chunks and SubGraphs** |\n",
    "| Performance | Concise                                                      | Concise                                                      | Fruitful                                                     | Fruitful, could be more comprehensive                        |\n",
    "| Cost        | Low                                                          | Low                                                          | High                                                         | High                                                         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "For those tasks:\n",
    "\n",
    "- Potentially cares more relationed knowledge\n",
    "- Schema of the KG is sophisticated to be hard for text2cypher to express the task\n",
    "- KG quality isn't good enough\n",
    "- Multiple \"starting entities\" are involved\n",
    "\n",
    "Graph RAG could be a better approach to start with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b21d109",
   "metadata": {},
   "source": [
    "## 7. Financial Evaluation on four types of engines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7d3cd5",
   "metadata": {},
   "source": [
    "### 7.1 FinEval on query engines base on nodes of different chunk sizes and chunk overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590bcfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size_ls = [256, 512, 1024]\n",
    "chunk_overlap_pct_ls = [1/8, 1/4]\n",
    "nodes_group_ls = []\n",
    "for chunk_size in chunk_size_ls:\n",
    "    for chunk_overlap_pct in chunk_overlap_pct_ls:\n",
    "        chunk_overlap = int(chunk_size * chunk_overlap_pct)\n",
    "        nodes_group = f'size_{chunk_size}_overlap_{chunk_overlap}'\n",
    "        nodes_group_ls.append(nodes_group)\n",
    "query_engine_dc = get_all_query_engine_from_cache_index(kg_index_folder_path='../storage/storage_graph',\n",
    "                                                        vector_index_folder_path='../storage/storage_vector',\n",
    "                                                        nodes_group=nodes_group_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2911c5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "\n",
    "choices = [\"A\", \"B\", \"C\", \"D\"]\n",
    "eval_path = 'financial_eval'\n",
    "\n",
    "def fineval(args, evaluator, take):\n",
    "    assert os.path.exists(eval_path + \"subject_mapping.json\"), \"subject_mapping.json not found!\"\n",
    "    with open(eval_path+ \"subject_mapping.json\") as f:\n",
    "        subject_mapping = json.load(f)\n",
    "    filenames = os.listdir(eval_path + \"data/val\")\n",
    "    subject_list = [val_file.replace(\"_val.csv\", \"\") for val_file in filenames]\n",
    "    accuracy, summary = {}, {}\n",
    "\n",
    "    run_date = time.strftime('%Y-%m-%d_%H-%M-%S', time.localtime(time.time()))\n",
    "    output_dir = args.output_dir\n",
    "    save_result_dir = os.path.join(output_dir, f\"take{take}\")\n",
    "    if not os.path.exists(save_result_dir):\n",
    "        os.makedirs(save_result_dir, exist_ok=True)\n",
    "\n",
    "    print(f'############# nodes group: {args.nodes_group} ###############')\n",
    "\n",
    "    all_answers = {}\n",
    "    for index, subject_name in enumerate(subject_list):\n",
    "        print(\n",
    "            f\"{index / len(subject_list)} Inference starts at {run_date} on {args.model_name} with subject of {subject_name}!\")\n",
    "        val_file_path = os.path.join('data/val', f'{subject_name}_val.csv')\n",
    "        dev_file_path = os.path.join('data/dev', f'{subject_name}_dev.csv')\n",
    "        test_file_path = os.path.join('data/test', f'{subject_name}_test.csv')\n",
    "\n",
    "        val_df = pd.read_csv(val_file_path) if args.do_test is False else pd.read_csv(test_file_path)\n",
    "        dev_df = pd.read_csv(dev_file_path) if args.few_shot else None\n",
    "\n",
    "        correct_ratio, answers = evaluator.eval_subject(subject_name, val_df, dev_df,\n",
    "                                                        save_result_dir=save_result_dir if args.do_save_csv else None,\n",
    "                                                        few_shot=args.few_shot,\n",
    "                                                        cot=args.cot,\n",
    "                                                        )\n",
    "        print(f\"Subject: {subject_name}\")\n",
    "        print(f\"Acc: {correct_ratio}\")\n",
    "        accuracy[subject_name] = correct_ratio\n",
    "        summary[subject_name] = {\"score\": correct_ratio,\n",
    "                                 \"num\": len(val_df),\n",
    "                                 \"correct\": correct_ratio * len(val_df) / 100}\n",
    "        all_answers[subject_name] = answers\n",
    "\n",
    "    json.dump(all_answers, open(save_result_dir + '/submission.json', 'w'), ensure_ascii=False, indent=4)\n",
    "    print(\"Accuracy:\")\n",
    "    for k, v in accuracy.items():\n",
    "        print(k, \": \", v)\n",
    "\n",
    "    total_num = 0\n",
    "    total_correct = 0\n",
    "    summary['grouped'] = {\n",
    "        \"Accounting\": {\"correct\": 0.0, \"num\": 0},\n",
    "        \"Finance\": {\"correct\": 0.0, \"num\": 0},\n",
    "        \"Economy\": {\"correct\": 0.0, \"num\": 0},\n",
    "        \"Certificate\": {\"correct\": 0.0, \"num\": 0}\n",
    "    }\n",
    "    for subj, info in subject_mapping.items():\n",
    "        group = info[2]\n",
    "        summary['grouped'][group][\"num\"] += summary[subj]['num']\n",
    "        summary['grouped'][group][\"correct\"] += summary[subj]['correct']\n",
    "    for group, info in summary['grouped'].items():\n",
    "        info['score'] = info[\"correct\"] / info[\"num\"]\n",
    "        total_num += info[\"num\"]\n",
    "        total_correct += info[\"correct\"]\n",
    "    summary['All'] = {\"score\": total_correct / total_num, \"num\": total_num, \"correct\": total_correct}\n",
    "\n",
    "    print('-' * 80)\n",
    "    print(\"Accuracy_subject:\")\n",
    "    for k, v in accuracy.items():\n",
    "        print(k, \": \", v)\n",
    "    print('-' * 80)\n",
    "    print(\"Accuracy_grouped:\")\n",
    "    for k, v in summary['grouped'].items():\n",
    "        print(k, \": \", v['score'])\n",
    "\n",
    "    print(\"Avg: \")\n",
    "    print(summary['All']['score'])\n",
    "\n",
    "    json.dump(summary, open(save_result_dir + '/summary.json', 'w'), ensure_ascii=False, indent=2)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19908354",
   "metadata": {},
   "outputs": [],
   "source": [
    "cot = False\n",
    "few_shot = False\n",
    "ntrain = 5\n",
    "n_times = 1\n",
    "do_save_csv = False\n",
    "output_dir = eval_path + 'output'\n",
    "model_name = 'chatglm'\n",
    "do_test = False\n",
    "args = dict(\n",
    "    cot=cot,\n",
    "    few_shot = few_shot,\n",
    "    ntrain = ntrain,\n",
    "    n_times = n_times,\n",
    "    do_save_csv = do_save_csv,\n",
    "    output_dir = output_dir,\n",
    "    model_name = model_name,\n",
    "    do_test = do_test\n",
    ")\n",
    "\n",
    "tree_summary_template = \\\n",
    "    \"从不同来源获取的参考信息如下:\\n\" \\\n",
    "    \"---------------------\\n\" \\\n",
    "    \"{context_str}\\n\" \\\n",
    "    \"---------------------\\n\" \\\n",
    "    \"题目:{query_str}\" \n",
    "\n",
    "graph_query_synthesis_prompt = \\\n",
    "    \"\"\"Task:Generate nGQL statement to query a Nebula graph database.\n",
    "    Instructions:\n",
    "    Use only the provided relationship types and properties in the schema.\n",
    "    Do not use any other relationship types or properties that are not provided.\n",
    "    Schema:\n",
    "    {schema}\n",
    "    Note: Do not include any explanations or apologies in your responses.\n",
    "    Do not respond to any questions that might ask anything else than for you to construct a nGQL statement.\n",
    "    Do not include any text except the generated nGQL statement.\n",
    "    Examples: Here are a few examples of generated nGQL statements for particular questions:\n",
    "    # Tell me about Peter Quill?\n",
    "    MATCH (m:entity {name:\"Peter Quill\"})<-[:relationship]-()\n",
    "    RETURN count(*) AS numberOfActors\n",
    "\n",
    "    The question is:\n",
    "    {query_str}\"\"\"\n",
    "\n",
    "graph_response_answer_prompt = \\\n",
    "    \"\"\"\n",
    "    原问题被转化成了查询语句，查询语句和查询结果将作为参考信息，如下:\n",
    "\n",
    "    查询语句: {kg_query_str}\n",
    "    查询结果: {kg_response_str}\n",
    "    题目: {query_str}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4a6953",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = QueryEngineEvaluator(\n",
    "    choices=choices,\n",
    "    k=args.ntrain,\n",
    "    model_name=args.model_name\n",
    ")\n",
    "for i in range(args.n_times):\n",
    "    fineval(args, evaluator=evaluator, take=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063d5cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatglm3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
