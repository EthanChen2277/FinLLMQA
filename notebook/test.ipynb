{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.chatglm3 import ChatGLM3\n",
    "from finllmqa.api.core import LLM_API_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGLM3(\n",
    "        endpoint_url=CHAT_API_URL,\n",
    "        max_tokens=8096,\n",
    "        top_p=0.9,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = KGRetrieveTool(llm=llm, reference_llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-31 23:04:08,489 - INFO - HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error raised during decoding response from inference endpoint: Expecting value: line 1 column 1 (char 0).\nResponse: data: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"\\n 我是一个\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897448}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"名为\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897448}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\" Chat\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897448}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"GL\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897448}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"M\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897448}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"3\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897448}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"-\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897448}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"6\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897448}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"B\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897448}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\" \",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897448}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"的人工\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897448}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"智能\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897448}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"助手\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897448}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"，\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897448}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"是基于\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897448}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"清华大学\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897448}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\" KE\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897448}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"G\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897448}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\" \",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897448}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"实验室\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897448}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"和\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897448}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"智\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"谱\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\" AI\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\" 公司\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"于\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\" \",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"2\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"0\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"2\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"3\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\" 年\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"共同\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"训练\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"的语言\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"模型\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"开发的\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"。\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"我的\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"任务\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"是\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"针对\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"用户\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"的问题\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"和要求\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"提供\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"适当的\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"答复\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"和支持\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"。\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"\",\"function_call\":null},\"finish_reason\":\"stop\",\"index\":0}],\"created\":1711897449}\r\n\r\ndata: [DONE]\r\n\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32me:\\anaconda\\app\\envs\\chatglm3\\lib\\site-packages\\langchain_community\\llms\\chatglm3.py:129\u001b[0m, in \u001b[0;36mChatGLM3._call\u001b[1;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 129\u001b[0m     parsed_response \u001b[38;5;241m=\u001b[39m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(parsed_response, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[1;32me:\\anaconda\\app\\envs\\chatglm3\\lib\\site-packages\\httpx\\_models.py:764\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    763\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjson\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: typing\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mAny:\n\u001b[1;32m--> 764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jsonlib\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\anaconda\\app\\envs\\chatglm3\\lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32me:\\anaconda\\app\\envs\\chatglm3\\lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[1;32me:\\anaconda\\app\\envs\\chatglm3\\lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m llm_chain \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;241m|\u001b[39m llm\n\u001b[0;32m     22\u001b[0m ret \u001b[38;5;241m=\u001b[39m llm_chain\u001b[38;5;241m.\u001b[39mstream({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m你是谁？\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m ret:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(token\u001b[38;5;241m.\u001b[39mcontent,end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[1;32me:\\anaconda\\app\\envs\\chatglm3\\lib\\site-packages\\langchain_core\\runnables\\base.py:2822\u001b[0m, in \u001b[0;36mRunnableSequence.stream\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   2816\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstream\u001b[39m(\n\u001b[0;32m   2817\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2818\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[0;32m   2819\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2820\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   2821\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[1;32m-> 2822\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28miter\u001b[39m([\u001b[38;5;28minput\u001b[39m]), config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\anaconda\\app\\envs\\chatglm3\\lib\\site-packages\\langchain_core\\runnables\\base.py:2809\u001b[0m, in \u001b[0;36mRunnableSequence.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   2803\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\n\u001b[0;32m   2804\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2805\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Input],\n\u001b[0;32m   2806\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2807\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   2808\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[1;32m-> 2809\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_stream_with_config(\n\u001b[0;32m   2810\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   2811\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform,\n\u001b[0;32m   2812\u001b[0m         patch_config(config, run_name\u001b[38;5;241m=\u001b[39m(config \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m   2813\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2814\u001b[0m     )\n",
      "File \u001b[1;32me:\\anaconda\\app\\envs\\chatglm3\\lib\\site-packages\\langchain_core\\runnables\\base.py:1880\u001b[0m, in \u001b[0;36mRunnable._transform_stream_with_config\u001b[1;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m   1878\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1879\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1880\u001b[0m         chunk: Output \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   1881\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[0;32m   1882\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_output_supported:\n",
      "File \u001b[1;32me:\\anaconda\\app\\envs\\chatglm3\\lib\\site-packages\\langchain_core\\runnables\\base.py:2773\u001b[0m, in \u001b[0;36mRunnableSequence._transform\u001b[1;34m(self, input, run_manager, config)\u001b[0m\n\u001b[0;32m   2764\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m steps:\n\u001b[0;32m   2765\u001b[0m     final_pipeline \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[0;32m   2766\u001b[0m         final_pipeline,\n\u001b[0;32m   2767\u001b[0m         patch_config(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2770\u001b[0m         ),\n\u001b[0;32m   2771\u001b[0m     )\n\u001b[1;32m-> 2773\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m final_pipeline:\n\u001b[0;32m   2774\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m output\n",
      "File \u001b[1;32me:\\anaconda\\app\\envs\\chatglm3\\lib\\site-packages\\langchain_core\\runnables\\base.py:1300\u001b[0m, in \u001b[0;36mRunnable.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   1293\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   1294\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed while trying to add together \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1295\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(final)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(chunk)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1296\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThese types should be addable for transform to work.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1297\u001b[0m             )\n\u001b[0;32m   1299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m got_first_val:\n\u001b[1;32m-> 1300\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(final, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\anaconda\\app\\envs\\chatglm3\\lib\\site-packages\\langchain_core\\language_models\\llms.py:386\u001b[0m, in \u001b[0;36mBaseLLM.stream\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstream\u001b[39m(\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_stream \u001b[38;5;241m==\u001b[39m BaseLLM\u001b[38;5;241m.\u001b[39m_stream:\n\u001b[0;32m    385\u001b[0m         \u001b[38;5;66;03m# model doesn't implement streaming, so use default implementation\u001b[39;00m\n\u001b[1;32m--> 386\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config\u001b[38;5;241m=\u001b[39mconfig, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m         prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)\u001b[38;5;241m.\u001b[39mto_string()\n",
      "File \u001b[1;32me:\\anaconda\\app\\envs\\chatglm3\\lib\\site-packages\\langchain_core\\language_models\\llms.py:248\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    245\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    246\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 248\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    249\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    250\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    251\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    252\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    253\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    254\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    255\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    256\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    257\u001b[0m         )\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    260\u001b[0m     )\n",
      "File \u001b[1;32me:\\anaconda\\app\\envs\\chatglm3\\lib\\site-packages\\langchain_core\\language_models\\llms.py:569\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    562\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    563\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    566\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    567\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    568\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 569\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\anaconda\\app\\envs\\chatglm3\\lib\\site-packages\\langchain_core\\language_models\\llms.py:748\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    731\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    732\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    733\u001b[0m         )\n\u001b[0;32m    734\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    735\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    736\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    746\u001b[0m         )\n\u001b[0;32m    747\u001b[0m     ]\n\u001b[1;32m--> 748\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    749\u001b[0m         prompts, stop, run_managers, \u001b[38;5;28mbool\u001b[39m(new_arg_supported), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    750\u001b[0m     )\n\u001b[0;32m    751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32me:\\anaconda\\app\\envs\\chatglm3\\lib\\site-packages\\langchain_core\\language_models\\llms.py:606\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    604\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    605\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 606\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    607\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32me:\\anaconda\\app\\envs\\chatglm3\\lib\\site-packages\\langchain_core\\language_models\\llms.py:593\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    584\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    585\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    589\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    590\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    591\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    592\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 593\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    594\u001b[0m                 prompts,\n\u001b[0;32m    595\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    596\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    597\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    598\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    599\u001b[0m             )\n\u001b[0;32m    600\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    601\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    602\u001b[0m         )\n\u001b[0;32m    603\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    604\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32me:\\anaconda\\app\\envs\\chatglm3\\lib\\site-packages\\langchain_core\\language_models\\llms.py:1209\u001b[0m, in \u001b[0;36mLLM._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1206\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m   1208\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1209\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1210\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m   1211\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1212\u001b[0m     )\n\u001b[0;32m   1213\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32me:\\anaconda\\app\\envs\\chatglm3\\lib\\site-packages\\langchain_community\\llms\\chatglm3.py:143\u001b[0m, in \u001b[0;36mChatGLM3._call\u001b[1;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected response type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparsed_response\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m json\u001b[38;5;241m.\u001b[39mJSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError raised during decoding response from inference endpoint: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mResponse: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    146\u001b[0m     )\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    149\u001b[0m     text \u001b[38;5;241m=\u001b[39m enforce_stop_tokens(text, stop)\n",
      "\u001b[1;31mValueError\u001b[0m: Error raised during decoding response from inference endpoint: Expecting value: line 1 column 1 (char 0).\nResponse: data: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"\\n 我是一个\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897448}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"名为\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897448}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\" Chat\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897448}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"GL\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897448}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"M\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897448}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"3\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897448}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"-\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897448}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"6\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897448}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"B\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897448}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\" \",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897448}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"的人工\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897448}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"智能\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897448}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"助手\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897448}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"，\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897448}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"是基于\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897448}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"清华大学\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897448}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\" KE\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897448}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"G\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897448}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\" \",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897448}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"实验室\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897448}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"和\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897448}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"智\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"谱\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\" AI\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\" 公司\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"于\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\" \",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"2\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"0\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"2\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"3\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\" 年\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"共同\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"训练\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"的语言\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"模型\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"开发的\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"。\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"我的\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"任务\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"是\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"针对\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"用户\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"的问题\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"和要求\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"提供\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"适当的\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"答复\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"和支持\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"。\",\"function_call\":null},\"finish_reason\":null,\"index\":0}],\"created\":1711897449}\r\n\r\ndata: {\"model\":\"chatglm3-6b\",\"id\":\"\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"role\":\"assistant\",\"content\":\"\",\"function_call\":null},\"finish_reason\":\"stop\",\"index\":0}],\"created\":1711897449}\r\n\r\ndata: [DONE]\r\n\r\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"chatglm3\",streaming=True,max_tokens=2048, base_url=LLM_API_URL, api_key='null')\n",
    "llm = ChatGLM3(\n",
    "    endpoint_url=CHAT_API_URL,\n",
    "    max_tokens=8096,\n",
    "    top_p=0.9,\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"{query}\"\n",
    ")\n",
    "\n",
    "# llm_chain = prompt | llm.bind(model=\"chatglm3\")  # bin的用法\n",
    "llm_chain = prompt | llm\n",
    "ret = llm_chain.stream({\"query\": \"你是谁？\"})\n",
    "for token in ret:\n",
    "    print(token.content,end=\"\",flush=True)\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core import SummaryIndex\n",
    "from llama_index.core.schema import IndexNode\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from finllmqa.api.core import LLM_API_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", api_base=LLM_API_URL, api_key='null')\n",
    "embed_model = OpenAIEmbedding(api_base=LLM_API_URL, api_key='null')\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = SimpleDirectoryReader(input_files=[f\"books/经济学原理.txt\"]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-07 22:45:10,619 - INFO - HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-04-07 22:45:16,043 - INFO - HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-04-07 22:45:21,510 - INFO - HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-04-07 22:45:26,969 - INFO - HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-04-07 22:45:28,352 - INFO - HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import ChatPromptTemplate\n",
    "\n",
    "qa_prompt_str = (\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, \"\n",
    "    \"answer the question: {query_str}\\n\"\n",
    ")\n",
    "\n",
    "refine_prompt_str = (\n",
    "    \"We have the opportunity to refine the original answer \"\n",
    "    \"(only if needed) with some more context below.\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{context_msg}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"Given the new context, refine the original answer to better \"\n",
    "    \"answer the question: {query_str}. \"\n",
    "    \"If the context isn't useful, output the original answer again.\\n\"\n",
    "    \"Original Answer: {existing_answer}\"\n",
    ")\n",
    "\n",
    "# Text QA Prompt\n",
    "chat_text_qa_msgs = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"Always answer the question, even if the context isn't helpful.\",\n",
    "    ),\n",
    "    (\"user\", qa_prompt_str),\n",
    "]\n",
    "text_qa_template = ChatPromptTemplate.from_messages(chat_text_qa_msgs)\n",
    "\n",
    "# Refine Prompt\n",
    "chat_refine_msgs = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"Always answer the question, even if the context isn't helpful.\",\n",
    "    ),\n",
    "    (\"user\", refine_prompt_str),\n",
    "]\n",
    "refine_template = ChatPromptTemplate.from_messages(chat_refine_msgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(streaming=True, \n",
    "                                     similarity_top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_qa_template': SelectorPromptTemplate(metadata={'prompt_type': <PromptType.QUESTION_ANSWER: 'text_qa'>}, template_vars=['context_str', 'query_str'], kwargs={}, output_parser=None, template_var_mappings={}, function_mappings={}, default_template=PromptTemplate(metadata={'prompt_type': <PromptType.QUESTION_ANSWER: 'text_qa'>}, template_vars=['context_str', 'query_str'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template='Context information is below.\\n---------------------\\n{context_str}\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: {query_str}\\nAnswer: '), conditionals=[(<function is_chat_model at 0x0000022D87326EF0>, ChatPromptTemplate(metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>}, template_vars=['context_str', 'query_str'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, message_templates=[ChatMessage(role=<MessageRole.SYSTEM: 'system'>, content=\"You are an expert Q&A system that is trusted around the world.\\nAlways answer the query using the provided context information, and not prior knowledge.\\nSome rules to follow:\\n1. Never directly reference the given context in your answer.\\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\", additional_kwargs={}), ChatMessage(role=<MessageRole.USER: 'user'>, content='Context information is below.\\n---------------------\\n{context_str}\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: {query_str}\\nAnswer: ', additional_kwargs={})]))]),\n",
       " 'refine_template': SelectorPromptTemplate(metadata={'prompt_type': <PromptType.REFINE: 'refine'>}, template_vars=['query_str', 'existing_answer', 'context_msg'], kwargs={}, output_parser=None, template_var_mappings={}, function_mappings={}, default_template=PromptTemplate(metadata={'prompt_type': <PromptType.REFINE: 'refine'>}, template_vars=['query_str', 'existing_answer', 'context_msg'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template=\"The original query is as follows: {query_str}\\nWe have provided an existing answer: {existing_answer}\\nWe have the opportunity to refine the existing answer (only if needed) with some more context below.\\n------------\\n{context_msg}\\n------------\\nGiven the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\\nRefined Answer: \"), conditionals=[(<function is_chat_model at 0x0000022D87326EF0>, ChatPromptTemplate(metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>}, template_vars=['context_msg', 'query_str', 'existing_answer'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, message_templates=[ChatMessage(role=<MessageRole.USER: 'user'>, content=\"You are an expert Q&A system that strictly operates in two modes when refining existing answers:\\n1. **Rewrite** an original answer using the new context.\\n2. **Repeat** the original answer if the new context isn't useful.\\nNever reference the original answer or context directly in your answer.\\nWhen in doubt, just repeat the original answer.\\nNew Context: {context_msg}\\nQuery: {query_str}\\nOriginal Answer: {existing_answer}\\nNew Answer: \", additional_kwargs={})]))])}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine._response_synthesizer.get_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-08 00:08:22,680 - INFO - HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-04-08 00:08:23,712 - INFO - HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-04-08 00:08:25,415 - INFO - HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-04-08 00:08:27,001 - INFO - HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 需求价格弹性的计算公式是：需求价格弹性 = (变化后的数量 - 变化前的数量) / (变化后的价格 - 变化前的价格)。"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"根据上下文，告诉我需求价格弹性的计算公式是什么\"\n",
    ")\n",
    "response.print_response_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'时间的长短 物品往往随着时间变长而需求更富有弹性；当汽油价格上升时，在最初的几个月中汽油的需求量只略有减少。但是，随着时间推移，人们购买更省油的汽车，转向公共交通，或迁移到离工作地方近的地点。在几年之内，汽油的需求量会大幅度减少。\\r\\n计算需求价格弹性\\r\\n我们已经在一般意义上讨论了需求价格弹性，现在我们更精确地讨论它的计量。经济学家用需求量变动的百分比除以价格变动的百分比来计算需求价格弹性。这就是：\\r\\n需求价格弹性=需求量变动的百分比/价格变动的百分比\\r\\n例如，假定冰激凌蛋卷的价格从2美元上升到2.2美元使你购买的冰激凌从每月10个减少为8个。我们计算出价格变动百分比为：\\r\\n价格变动百分比＝(2.20－2.00)÷2.00×100＝10%\\r\\n同样，我们计算出需求量变动百分比为：\\r\\n需求量变动百分比＝(10－8)÷10×100＝20%\\r\\n在这种情况下，你的需求弹性是：需求价格弹性＝20%/10%＝2\\r\\n在这个例于中，弹性是2，反映了需求量变动的比例是价格变动比例的两倍。\\r\\n由于一种物品的需求量与其价格负相关，所以，数量变动的百分比与价格变动百分比总是相反的符号。在这个例子中，价格变动的百分比是正的10%（反映了上升），而需求量变动的百分比是负的20%（反映了减少）。由于这个原因，需求价格弹性有时称为负数。在本书中我们遵循一般做法，去掉负号，把所有价格弹性作为正数。（数学上称这种数为绝对值。）根据这个习惯，需求价格弹性越大，意味着需求量对价格越敏感。\\r\\n参考资料：用中点法计算弹性\\r\\n如果你想计算一条需求曲线上两点之间的需求价格弹性，你将很快会注意到一个令人烦恼的问题：从A点到B点的弹性似乎不同于从B点到A点的弹性。例如，考虑这些数字：\\r\\nA点：价格=4美元，数量=120\\r\\nB点：价格=6美元，数量=80\\r\\n从A点到B点，价格上升了50%，数量减少了33%，表明需求的价格弹性是33/50，或者0.66。与此相比，从B点到A点，价格下降了33%，而数量增加了50%，表明需求的价格弹性是50/33，或1.5。\\r\\n避免这个问题的一种方法是用中点法计算弹性；中点法不是用标准的方法（变动量除以原先的水平）计算变动的白分比，而是用变动量除以原先水平与最后水平的中点来计算变动的百分比。例如，4美元和6美元的中点是5美元；。'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.source_nodes[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from llama_index.agent.openai import OpenAIAgent\n",
    "\n",
    "# build vector index\n",
    "vector_index = VectorStoreIndex.from_documents(\n",
    "    docs,\n",
    ")\n",
    "# build summary index\n",
    "summary_index = SummaryIndex.from_documents(\n",
    "    docs,\n",
    ")\n",
    "# define query engines\n",
    "vector_query_engine = vector_index.as_query_engine()\n",
    "list_query_engine = summary_index.as_query_engine()\n",
    "\n",
    "# define tools\n",
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=vector_query_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"vector_tool\",\n",
    "            description=(\n",
    "                f\"Useful for retrieving specific context from 经济学原理\"\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "    QueryEngineTool(\n",
    "        query_engine=list_query_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"summary_tool\",\n",
    "            description=(\n",
    "                \"Useful for summarization questions related to 经济学原理\"\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "# build agent\n",
    "function_llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo\", api_base=LLM_API_URL, api_key='null')\n",
    "agent = OpenAIAgent.from_tools(\n",
    "    query_engine_tools,\n",
    "    llm=function_llm,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define top-level nodes\n",
    "book_title = '经济学原理(微观经济学)'\n",
    "book_summary = (\n",
    "    f\"This content contains content about {book_title}. Use\"\n",
    "    \" this index if you need to lookup specific facts about\"\n",
    "    f\" {book_title}.\\n\"\n",
    ")\n",
    "node = IndexNode(\n",
    "    text=book_summary, index_id=book_title, obj=agent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# define top-level retriever\n",
    "vector_index = VectorStoreIndex(\n",
    "    objects=[node],\n",
    ")\n",
    "query_engine = vector_index.as_query_engine(similarity_top_k=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "\u001b[1;3;38;2;11;159;203mRetrieval entering 经济学原理(微观经济学): OpenAIAgent\n",
      "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object OpenAIAgent with query 什么是经济学十大原理\n",
      "\u001b[0mAdded user message to memory: 什么是经济学十大原理\n",
      "INFO:httpx:HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# should use Boston agent -> vector tool\n",
    "response = query_engine.query(\"什么是经济学十大原理\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "经济学十大原理是经济学的基本理论，它们是对经济现象进行科学解释的基础。这些原理包括劳动分工、价值规律、货币、资本、生产、消费、投资、分配、交换和信息。这些原理对于理解经济发展、市场运作以及国家政策制定都具有重要的指导意义。\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:chatglm3]",
   "language": "python",
   "name": "conda-env-chatglm3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
