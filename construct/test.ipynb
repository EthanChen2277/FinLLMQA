{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core import SummaryIndex\n",
    "from llama_index.core.schema import IndexNode\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from finllmqa.api.core import LLM_API_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", api_base=LLM_API_URL, api_key='null')\n",
    "embed_model = OpenAIEmbedding(api_base=LLM_API_URL, api_key='null')\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import ChatPromptTemplate\n",
    "\n",
    "qa_prompt_str = (\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, \"\n",
    "    \"answer the question: {query_str}\\n\"\n",
    ")\n",
    "\n",
    "refine_prompt_str = (\n",
    "    \"We have the opportunity to refine the original answer \"\n",
    "    \"(only if needed) with some more context below.\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{context_msg}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"Given the new context, refine the original answer to better \"\n",
    "    \"answer the question: {query_str}. \"\n",
    "    \"If the context isn't useful, output the original answer again.\\n\"\n",
    "    \"Original Answer: {existing_answer}\"\n",
    ")\n",
    "\n",
    "# Text QA Prompt\n",
    "chat_text_qa_msgs = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"Always answer the question, even if the context isn't helpful.\",\n",
    "    ),\n",
    "    (\"user\", qa_prompt_str),\n",
    "]\n",
    "text_qa_template = ChatPromptTemplate.from_messages(chat_text_qa_msgs)\n",
    "\n",
    "# Refine Prompt\n",
    "chat_refine_msgs = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"Always answer the question, even if the context isn't helpful.\",\n",
    "    ),\n",
    "    (\"user\", refine_prompt_str),\n",
    "]\n",
    "refine_template = ChatPromptTemplate.from_messages(chat_refine_msgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(streaming=True, \n",
    "                                     similarity_top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_qa_template': SelectorPromptTemplate(metadata={'prompt_type': <PromptType.QUESTION_ANSWER: 'text_qa'>}, template_vars=['context_str', 'query_str'], kwargs={}, output_parser=None, template_var_mappings={}, function_mappings={}, default_template=PromptTemplate(metadata={'prompt_type': <PromptType.QUESTION_ANSWER: 'text_qa'>}, template_vars=['context_str', 'query_str'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template='Context information is below.\\n---------------------\\n{context_str}\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: {query_str}\\nAnswer: '), conditionals=[(<function is_chat_model at 0x0000022D87326EF0>, ChatPromptTemplate(metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>}, template_vars=['context_str', 'query_str'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, message_templates=[ChatMessage(role=<MessageRole.SYSTEM: 'system'>, content=\"You are an expert Q&A system that is trusted around the world.\\nAlways answer the query using the provided context information, and not prior knowledge.\\nSome rules to follow:\\n1. Never directly reference the given context in your answer.\\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\", additional_kwargs={}), ChatMessage(role=<MessageRole.USER: 'user'>, content='Context information is below.\\n---------------------\\n{context_str}\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: {query_str}\\nAnswer: ', additional_kwargs={})]))]),\n",
       " 'refine_template': SelectorPromptTemplate(metadata={'prompt_type': <PromptType.REFINE: 'refine'>}, template_vars=['query_str', 'existing_answer', 'context_msg'], kwargs={}, output_parser=None, template_var_mappings={}, function_mappings={}, default_template=PromptTemplate(metadata={'prompt_type': <PromptType.REFINE: 'refine'>}, template_vars=['query_str', 'existing_answer', 'context_msg'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template=\"The original query is as follows: {query_str}\\nWe have provided an existing answer: {existing_answer}\\nWe have the opportunity to refine the existing answer (only if needed) with some more context below.\\n------------\\n{context_msg}\\n------------\\nGiven the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\\nRefined Answer: \"), conditionals=[(<function is_chat_model at 0x0000022D87326EF0>, ChatPromptTemplate(metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>}, template_vars=['context_msg', 'query_str', 'existing_answer'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, message_templates=[ChatMessage(role=<MessageRole.USER: 'user'>, content=\"You are an expert Q&A system that strictly operates in two modes when refining existing answers:\\n1. **Rewrite** an original answer using the new context.\\n2. **Repeat** the original answer if the new context isn't useful.\\nNever reference the original answer or context directly in your answer.\\nWhen in doubt, just repeat the original answer.\\nNew Context: {context_msg}\\nQuery: {query_str}\\nOriginal Answer: {existing_answer}\\nNew Answer: \", additional_kwargs={})]))])}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine._response_synthesizer.get_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-08 00:08:22,680 - INFO - HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-04-08 00:08:23,712 - INFO - HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-04-08 00:08:25,415 - INFO - HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-04-08 00:08:27,001 - INFO - HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 需求价格弹性的计算公式是：需求价格弹性 = (变化后的数量 - 变化前的数量) / (变化后的价格 - 变化前的价格)。"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"根据上下文，告诉我需求价格弹性的计算公式是什么\"\n",
    ")\n",
    "response.print_response_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'时间的长短 物品往往随着时间变长而需求更富有弹性；当汽油价格上升时，在最初的几个月中汽油的需求量只略有减少。但是，随着时间推移，人们购买更省油的汽车，转向公共交通，或迁移到离工作地方近的地点。在几年之内，汽油的需求量会大幅度减少。\\r\\n计算需求价格弹性\\r\\n我们已经在一般意义上讨论了需求价格弹性，现在我们更精确地讨论它的计量。经济学家用需求量变动的百分比除以价格变动的百分比来计算需求价格弹性。这就是：\\r\\n需求价格弹性=需求量变动的百分比/价格变动的百分比\\r\\n例如，假定冰激凌蛋卷的价格从2美元上升到2.2美元使你购买的冰激凌从每月10个减少为8个。我们计算出价格变动百分比为：\\r\\n价格变动百分比＝(2.20－2.00)÷2.00×100＝10%\\r\\n同样，我们计算出需求量变动百分比为：\\r\\n需求量变动百分比＝(10－8)÷10×100＝20%\\r\\n在这种情况下，你的需求弹性是：需求价格弹性＝20%/10%＝2\\r\\n在这个例于中，弹性是2，反映了需求量变动的比例是价格变动比例的两倍。\\r\\n由于一种物品的需求量与其价格负相关，所以，数量变动的百分比与价格变动百分比总是相反的符号。在这个例子中，价格变动的百分比是正的10%（反映了上升），而需求量变动的百分比是负的20%（反映了减少）。由于这个原因，需求价格弹性有时称为负数。在本书中我们遵循一般做法，去掉负号，把所有价格弹性作为正数。（数学上称这种数为绝对值。）根据这个习惯，需求价格弹性越大，意味着需求量对价格越敏感。\\r\\n参考资料：用中点法计算弹性\\r\\n如果你想计算一条需求曲线上两点之间的需求价格弹性，你将很快会注意到一个令人烦恼的问题：从A点到B点的弹性似乎不同于从B点到A点的弹性。例如，考虑这些数字：\\r\\nA点：价格=4美元，数量=120\\r\\nB点：价格=6美元，数量=80\\r\\n从A点到B点，价格上升了50%，数量减少了33%，表明需求的价格弹性是33/50，或者0.66。与此相比，从B点到A点，价格下降了33%，而数量增加了50%，表明需求的价格弹性是50/33，或1.5。\\r\\n避免这个问题的一种方法是用中点法计算弹性；中点法不是用标准的方法（变动量除以原先的水平）计算变动的白分比，而是用变动量除以原先水平与最后水平的中点来计算变动的百分比。例如，4美元和6美元的中点是5美元；。'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.source_nodes[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from llama_index.agent.openai import OpenAIAgent\n",
    "\n",
    "# build vector index\n",
    "vector_index = VectorStoreIndex.from_documents(\n",
    "    docs,\n",
    ")\n",
    "# build summary index\n",
    "summary_index = SummaryIndex.from_documents(\n",
    "    docs,\n",
    ")\n",
    "# define query engines\n",
    "vector_query_engine = vector_index.as_query_engine()\n",
    "list_query_engine = summary_index.as_query_engine()\n",
    "\n",
    "# define tools\n",
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=vector_query_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"vector_tool\",\n",
    "            description=(\n",
    "                f\"Useful for retrieving specific context from 经济学原理\"\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "    QueryEngineTool(\n",
    "        query_engine=list_query_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"summary_tool\",\n",
    "            description=(\n",
    "                \"Useful for summarization questions related to 经济学原理\"\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "# build agent\n",
    "function_llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo\", api_base=LLM_API_URL, api_key='null')\n",
    "agent = OpenAIAgent.from_tools(\n",
    "    query_engine_tools,\n",
    "    llm=function_llm,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define top-level nodes\n",
    "book_title = '经济学原理(微观经济学)'\n",
    "book_summary = (\n",
    "    f\"This content contains content about {book_title}. Use\"\n",
    "    \" this index if you need to lookup specific facts about\"\n",
    "    f\" {book_title}.\\n\"\n",
    ")\n",
    "node = IndexNode(\n",
    "    text=book_summary, index_id=book_title, obj=agent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# define top-level retriever\n",
    "vector_index = VectorStoreIndex(\n",
    "    objects=[node],\n",
    ")\n",
    "query_engine = vector_index.as_query_engine(similarity_top_k=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "\u001b[1;3;38;2;11;159;203mRetrieval entering 经济学原理(微观经济学): OpenAIAgent\n",
      "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object OpenAIAgent with query 什么是经济学十大原理\n",
      "\u001b[0mAdded user message to memory: 什么是经济学十大原理\n",
      "INFO:httpx:HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://gemini2.sufe.edu.cn:27282/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# should use Boston agent -> vector tool\n",
    "response = query_engine.query(\"什么是经济学十大原理\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finllmqa.agent.langchain_tools import KGRetrieverTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = KGRetrieverTool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'*贵州茅台2024-06-30的市净率信息如下\\n存在市净率如下表所示\\n|属性|值|\\n|name|2024-06-30|\\n|报告期|2024-06-30|\\n|股票代码|600519|\\n\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool.graph_searcher.search_main({'主体': {'股票': ['贵州茅台']}, '时间': ['2024年'], '意图': ['市净率']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.graph_stores.nebula import NebulaGraphStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../FinEval/subject_mapping.json', 'r') as f:\n",
    "    subject_mapping = json.load(f)\n",
    "subjects = subject_mapping.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['insurance', 'international_finance', 'investments', 'monetary_finance']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(subjects)[30:35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['banking_practitioner_qualification_certificate', 'fund_qualification_certificate', 'futures_practitioner_qualification_certificate',\n",
    " 'securities_practitioner_qualification_certificate','statistics', 'financial_engineering', 'investments', 'monetary_finance']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatglm3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
