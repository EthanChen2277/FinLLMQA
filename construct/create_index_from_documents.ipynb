{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b713e1a",
   "metadata": {},
   "source": [
    "# 1. Preparation\n",
    "\n",
    "## 1.1 Prepare for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99a3dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install llama-index-llms-azure-openai\n",
    "# %pip install llama-index-graph-stores-nebula\n",
    "# %pip install llama-index-llms-openai\n",
    "# %pip install llama-index-embeddings-azure-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a002bd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For OpenAI\n",
    "\n",
    "import os\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"INSERT YOUR KEY\"\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout, level=logging.INFO\n",
    ")  # logging.DEBUG for more verbose output\n",
    "\n",
    "from llama_index.core import (\n",
    "    KnowledgeGraphIndex,\n",
    "    VectorStoreIndex,\n",
    "    ServiceContext,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    PromptTemplate,\n",
    "    QueryBundle\n",
    ")\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "from llama_index.core.retrievers import BaseRetriever, VectorIndexRetriever, KGTableRetriever\n",
    "\n",
    "from llama_index.graph_stores.nebula import NebulaGraphStore\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from finllmqa.api.core import LLM_API_URL\n",
    "\n",
    "from llama_index.core import Settings\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", api_base=LLM_API_URL, api_key='null')\n",
    "embed_model = OpenAIEmbedding(api_base=LLM_API_URL, api_key='null')\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2f2a17",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# For Azure OpenAI\n",
    "\n",
    "import os\n",
    "import json\n",
    "import openai\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    KnowledgeGraphIndex\n",
    ")\n",
    "\n",
    "from llama_index.core.storage.storage_context import StorageContext\n",
    "from llama_index.graph_stores.nebula import NebulaGraphStore\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from finllmqa.api.core import LLM_API_URL\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout, level=logging.INFO\n",
    ")  # logging.DEBUG for more verbose output\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = LLM_API_URL\n",
    "openai.api_version = \"2024-03-01\"\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"youcannottellanyone\"\n",
    "# openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_key = 'null'\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    engine=\"<foo-bar-deployment>\",\n",
    "    temperature=0,\n",
    "    openai_api_version=openai.api_version,\n",
    "    model_kwargs={\n",
    "        \"api_key\": openai.api_key,\n",
    "        \"api_base\": openai.api_base,\n",
    "        \"api_type\": openai.api_type,\n",
    "        \"api_version\": openai.api_version,\n",
    "    },\n",
    ")\n",
    "\n",
    "# You need to deploy your own embedding model as well as your own chat completion model\n",
    "embedding_llm = LangchainEmbedding(\n",
    "    OpenAIEmbeddings(\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        deployment=\"<foo-bar-deployment>\",\n",
    "        openai_api_key=openai.api_key,\n",
    "        openai_api_base=openai.api_base,\n",
    "        openai_api_type=openai.api_type,\n",
    "        openai_api_version=openai.api_version,\n",
    "    ),\n",
    "    embed_batch_size=1,\n",
    ")\n",
    "\n",
    "# service_context = ServiceContext.from_defaults(\n",
    "#     llm=llm,\n",
    "#     embed_model=embedding_llm,\n",
    "# )\n",
    "\n",
    "# set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dbbe82",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embedding_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafd15f5",
   "metadata": {},
   "source": [
    "## 1.2. Prepare for NebulaGraph as Graph Store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa40d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install nebula3-python ipython-ngql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "801f88ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['NEBULA_USER'] = \"root\"\n",
    "os.environ['NEBULA_PASSWORD'] = \"nebula\" # default password\n",
    "os.environ['NEBULA_ADDRESS'] = \"127.0.0.1:9669\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cba664",
   "metadata": {},
   "source": [
    "## 2. Build the Knowledge Graph and Persist\n",
    "\n",
    "In my work, the Knowledge Graph was created with LLM.\n",
    "\n",
    "We simply do so leveragint the `KnowledgeGraphIndex` from LlamaIndex, when creating it, Triplets will be extracted with LLM and evantually persisted into `NebulaGraphStore`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0af465",
   "metadata": {},
   "source": [
    "### 2.1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04dd6ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function NebulaGraphStore.__del__ at 0x00000278ECF41AB0>\n",
      "Traceback (most recent call last):\n",
      "  File \"e:\\anaconda\\app\\envs\\chatglm3\\lib\\site-packages\\llama_index\\graph_stores\\nebula\\base.py\", line 242, in __del__\n",
      "    self._session_pool.close()\n",
      "AttributeError: 'NoneType' object has no attribute 'close'\n",
      "Exception ignored in: <function NebulaGraphStore.__del__ at 0x00000278ECF41AB0>\n",
      "Traceback (most recent call last):\n",
      "  File \"e:\\anaconda\\app\\envs\\chatglm3\\lib\\site-packages\\llama_index\\graph_stores\\nebula\\base.py\", line 242, in __del__\n",
      "    self._session_pool.close()\n",
      "AttributeError: 'NoneType' object has no attribute 'close'\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# change path to where you save the teaching resources\n",
    "document_path = 'books/'\n",
    "file_name_ls = ['微观经济学.pdf']\n",
    "file_name_ls = [document_path + file_name for file_name in file_name_ls]\n",
    "\n",
    "reader = SimpleDirectoryReader(input_files=file_name_ls)\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28861e17",
   "metadata": {},
   "source": [
    "### 2.2 Split Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "757f5733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size: 256; chunk_overlap: 32 len_chunks: 2941\n",
      "chunk_size: 256; chunk_overlap: 64 len_chunks: 3277\n",
      "chunk_size: 512; chunk_overlap: 64 len_chunks: 1508\n",
      "chunk_size: 512; chunk_overlap: 128 len_chunks: 1588\n",
      "chunk_size: 1024; chunk_overlap: 128 len_chunks: 871\n",
      "chunk_size: 1024; chunk_overlap: 256 len_chunks: 874\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "\n",
    "chunk_size_ls = [256, 512, 1024]\n",
    "chunk_overlap_pct_ls = [1/8, 1/4]\n",
    "split_document_dc = {}\n",
    "for chunk_size in chunk_size_ls:\n",
    "    for chunk_overlap_pct in chunk_overlap_pct_ls:\n",
    "        chunk_overlap = int(chunk_size * chunk_overlap_pct)\n",
    "        nodes_group = f'size_{chunk_size}_overlap_{chunk_overlap}'\n",
    "        splitter = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        split_document = splitter.get_nodes_from_documents(documents=documents)\n",
    "        split_document_dc[nodes_group] = split_document\n",
    "        print(f'chunk_size: {chunk_size}; chunk_overlap: {chunk_overlap} len_chunks: {len(split_document)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98ab097",
   "metadata": {},
   "source": [
    "### 2.3 Extract Triplets and Save to NebulaGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1ad1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_extract_template = \"\"\"\n",
    "    下面提供了一些文本。根据文本，提取最多 {max_knowledge_triplets} 个三元组的知识，形式为(实体,关系,实体)，具体可以是(主语,谓语,宾语)或者其他类型，注意避开停用词。\n",
    "    请忽略page_label和file_path\n",
    "    ---------------------\n",
    "    示例：\n",
    "    文本：小红是小明的母亲.\n",
    "    三元组：\n",
    "    (小红,是母亲,小明)\n",
    "    文本:瑞幸是2017年在厦门创立的咖啡店。\n",
    "    三元组：\n",
    "    (瑞幸,是,咖啡店)\n",
    "    (瑞幸,创立于,厦门)\n",
    "    (瑞幸,创立于,2017)\n",
    "    文本:在长期中，物价总水平会调整到使货币需求等于货币供给的水平。\n",
    "    三元组：\n",
    "    (物价总水平,长期调整使等于,货币需求等于货币供给的水平)\n",
    "    ---------------------\n",
    "    文本：{text}\n",
    "    三元组：\"\"\"\n",
    "kg_extract_template = PromptTemplate(kg_extract_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115938c7",
   "metadata": {},
   "source": [
    "This cell will take some time, it'll extract entities and relationships and store them into NebulaGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e98679",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "kg_index_ls = []\n",
    "\n",
    "for nodes_group, nodes in split_document_dc.items():\n",
    "    start = time.time()\n",
    "    print(f'\\n\\nstart extract {nodes_group} nodes...\\n\\n')\n",
    "    space_name = f\"book_微观经济学_{nodes_group}\"\n",
    "    edge_types, rel_prop_names = [\"关系\"], [\"关系\"] # default, could be omit if create from an empty kg\n",
    "    tags = [\"实体\"] # default, could be omit if create from an empty kg\n",
    "\n",
    "    graph_store = NebulaGraphStore(\n",
    "        space_name=space_name,\n",
    "        edge_types=edge_types,\n",
    "        rel_prop_names=rel_prop_names,\n",
    "        tags=tags,\n",
    "    )\n",
    "    storage_context = StorageContext.from_defaults(graph_store=graph_store)\n",
    "    kg_index = KnowledgeGraphIndex(\n",
    "        nodes=nodes,\n",
    "        storage_context=storage_context,\n",
    "        max_triplets_per_chunk=10,\n",
    "        space_name=space_name,\n",
    "        edge_types=edge_types,\n",
    "        rel_prop_names=rel_prop_names,\n",
    "        tags=tags,\n",
    "        include_embeddings=True,\n",
    "        kg_triple_extract_template=kg_extract_template\n",
    "    )\n",
    "    end = time.time()\n",
    "    print(f'{nodes_group} takes {(end-start)//60} min')\n",
    "    kg_index_ls.append(kg_index)\n",
    "\n",
    "    # store index\n",
    "    kg_index.storage_context.persist(persist_dir=f'../storage/storage_graph/{nodes_group}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb691d6",
   "metadata": {},
   "source": [
    "## 3 Create VectorStoreIndex for RAG and Persist\n",
    "\n",
    "To compare with/work together with VectorDB based RAG, let's also create a `VectorStoreIndex`.\n",
    "\n",
    "During the creation, same data source will be split into chunks and embedding of them will be created, during the RAG query time, the top-k related embeddings will be vector-searched with the embedding of the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204f3ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index_ls = []\n",
    "\n",
    "for nodes_group, nodes in split_document_dc.items():\n",
    "    print(f'\\n\\nstart extract {nodes_group} nodes...\\n\\n')\n",
    "    vector_index = VectorStoreIndex(nodes=nodes)\n",
    "    vector_index_ls.append(vector_index)\n",
    "\n",
    "    # store index\n",
    "    vector_index.storage_context.persist(persist_dir=f'../storage/storage_vector/{nodes_group}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db87d406",
   "metadata": {},
   "source": [
    "## 4.Multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16870a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile create_diff_chunk_index.py\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "os.environ['NEBULA_USER'] = \"root\"\n",
    "os.environ['NEBULA_PASSWORD'] = \"nebula\" # default password\n",
    "os.environ['NEBULA_ADDRESS'] = \"192.168.30.158:9669\" \n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout, level=logging.INFO\n",
    ")  # logging.DEBUG for more verbose output\n",
    "\n",
    "from llama_index.core import (\n",
    "    KnowledgeGraphIndex,\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    PromptTemplate)\n",
    "from llama_index.graph_stores.nebula import NebulaGraphStore\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from finllmqa.api.core import LLM_API_URL\n",
    "from llama_index.core import Settings\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", api_base=LLM_API_URL, api_key='null')\n",
    "embed_model = OpenAIEmbedding(api_base=LLM_API_URL, api_key='null')\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "# change path to where you save the teaching resources\n",
    "document_path = 'books/'\n",
    "file_name_ls = ['微观经济学.pdf']\n",
    "file_name_ls = [document_path + file_name for file_name in file_name_ls]\n",
    "\n",
    "reader = SimpleDirectoryReader(input_files=file_name_ls)\n",
    "documents = reader.load_data()\n",
    "\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "\n",
    "chunk_size_ls = [256, 512, 1024]\n",
    "chunk_overlap_pct_ls = [1/8, 1/4]\n",
    "split_document_dc = {}\n",
    "for chunk_size in chunk_size_ls:\n",
    "    for chunk_overlap_pct in chunk_overlap_pct_ls:\n",
    "        chunk_overlap = int(chunk_size * chunk_overlap_pct)\n",
    "        nodes_group = f'size_{chunk_size}_overlap_{chunk_overlap}'\n",
    "        if os.path.exists(f'../storage/storage_graph/{nodes_group}'):\n",
    "            continue\n",
    "        splitter = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        split_document = splitter.get_nodes_from_documents(documents=documents)\n",
    "        split_document_dc[nodes_group] = split_document\n",
    "        print(f'chunk_size: {chunk_size}; chunk_overlap: {chunk_overlap} len_chunks: {len(split_document)}')\n",
    "print(split_document_dc.keys())\n",
    "from threading import Thread\n",
    "\n",
    "def create_and_store_kg_index(nodes_group, nodes):\n",
    "    kg_extract_template = \"\"\"\n",
    "    下面提供了一些文本。根据文本，提取最多 {max_knowledge_triplets} 个三元组的知识，形式为(实体,关系,实体)，具体可以是(主语,谓语,宾语)或者其他类型，注意避开停用词。\n",
    "    请忽略page_label和file_path\n",
    "    ---------------------\n",
    "    示例：\n",
    "    文本：小红是小明的母亲.\n",
    "    三元组：\n",
    "    (小红,是母亲,小明)\n",
    "    文本:瑞幸是2017年在厦门创立的咖啡店。\n",
    "    三元组：\n",
    "    (瑞幸,是,咖啡店)\n",
    "    (瑞幸,创立于,厦门)\n",
    "    (瑞幸,创立于,2017)\n",
    "    文本:在长期中，物价总水平会调整到使货币需求等于货币供给的水平。\n",
    "    三元组：\n",
    "    (物价总水平,长期调整使等于,货币需求等于货币供给的水平)\n",
    "    ---------------------\n",
    "    文本：{text}\n",
    "    三元组：\"\"\"\n",
    "    kg_extract_template = PromptTemplate(kg_extract_template)\n",
    "\n",
    "    print(f'\\n\\nstart extract {nodes_group} nodes...\\n\\n')\n",
    "    space_name = f\"books_content_{nodes_group}\"\n",
    "    edge_types, rel_prop_names = [\"relationship\"], [\"relationship\"] # default, could be omit if create from an empty kg\n",
    "    tags = [\"entity\"] # default, could be omit if create from an empty kg\n",
    "\n",
    "    graph_store = NebulaGraphStore(\n",
    "        space_name=space_name,\n",
    "        edge_types=edge_types,\n",
    "        rel_prop_names=rel_prop_names,\n",
    "        tags=tags,\n",
    "    )\n",
    "    storage_context = StorageContext.from_defaults(graph_store=graph_store)\n",
    "    kg_index = KnowledgeGraphIndex(\n",
    "        nodes=nodes,\n",
    "        storage_context=storage_context,\n",
    "        max_triplets_per_chunk=10,\n",
    "        space_name=space_name,\n",
    "        edge_types=edge_types,\n",
    "        rel_prop_names=rel_prop_names,\n",
    "        tags=tags,\n",
    "        include_embeddings=True,\n",
    "        kg_triple_extract_template=kg_extract_template\n",
    "    )\n",
    "\n",
    "    vector_index = VectorStoreIndex(nodes=nodes)\n",
    "    \n",
    "    # store index\n",
    "    kg_index.storage_context.persist(persist_dir=f'../storage/storage_graph/{nodes_group}')\n",
    "    vector_index.storage_context.persist(persist_dir=f'../storage/storage_vector/{nodes_group}')\n",
    "\n",
    "for nodes_group, nodes in split_document_dc.items():\n",
    "    thread = Thread(target=create_and_store_kg_index, args=(nodes_group, nodes))\n",
    "    thread.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatglm3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
