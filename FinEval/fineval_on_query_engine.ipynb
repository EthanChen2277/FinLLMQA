{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b713e1a",
   "metadata": {},
   "source": [
    "# 1. Preparation\n",
    "\n",
    "## 1.1 Prepare for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99a3dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install llama-index-llms-azure-openai\n",
    "# %pip install llama-index-graph-stores-nebula\n",
    "# %pip install llama-index-llms-openai\n",
    "# %pip install llama-index-embeddings-azure-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a002bd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For OpenAI\n",
    "\n",
    "import os\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"INSERT YOUR KEY\"\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout, level=logging.INFO\n",
    ")  # logging.DEBUG for more verbose output\n",
    "\n",
    "from llama_index.core import (\n",
    "    KnowledgeGraphIndex,\n",
    "    VectorStoreIndex,\n",
    "    ServiceContext,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    PromptTemplate,\n",
    "    load_index_from_storage\n",
    ")\n",
    "from llama_index.core.query_engine import KnowledgeGraphQueryEngine\n",
    "\n",
    "from llama_index.graph_stores.nebula import NebulaGraphStore\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from finllmqa.api.core import LLM_API_URL\n",
    "\n",
    "from llama_index.core import Settings\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", api_base=LLM_API_URL, api_key='null')\n",
    "embed_model = OpenAIEmbedding(api_base=LLM_API_URL, api_key='null')\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2f2a17",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# For Azure OpenAI\n",
    "\n",
    "import os\n",
    "import json\n",
    "import openai\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    KnowledgeGraphIndex,\n",
    "    ServiceContext\n",
    ")\n",
    "from llama_index.core import set_global_service_context\n",
    "\n",
    "from llama_index.core.storage.storage_context import StorageContext\n",
    "from llama_index.graph_stores.nebula import NebulaGraphStore\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from finllmqa.api.core import LLM_API_URL\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout, level=logging.INFO\n",
    ")  # logging.DEBUG for more verbose output\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = LLM_API_URL\n",
    "openai.api_version = \"2024-03-01\"\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"youcannottellanyone\"\n",
    "# openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_key = 'null'\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    engine=\"<foo-bar-deployment>\",\n",
    "    temperature=0,\n",
    "    openai_api_version=openai.api_version,\n",
    "    model_kwargs={\n",
    "        \"api_key\": openai.api_key,\n",
    "        \"api_base\": openai.api_base,\n",
    "        \"api_type\": openai.api_type,\n",
    "        \"api_version\": openai.api_version,\n",
    "    },\n",
    ")\n",
    "\n",
    "# You need to deploy your own embedding model as well as your own chat completion model\n",
    "embedding_llm = LangchainEmbedding(\n",
    "    OpenAIEmbeddings(\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        deployment=\"<foo-bar-deployment>\",\n",
    "        openai_api_key=openai.api_key,\n",
    "        openai_api_base=openai.api_base,\n",
    "        openai_api_type=openai.api_type,\n",
    "        openai_api_version=openai.api_version,\n",
    "    ),\n",
    "    embed_batch_size=1,\n",
    ")\n",
    "\n",
    "# service_context = ServiceContext.from_defaults(\n",
    "#     llm=llm,\n",
    "#     embed_model=embedding_llm,\n",
    "# )\n",
    "\n",
    "# set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dbbe82",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embedding_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafd15f5",
   "metadata": {},
   "source": [
    "## 1.2. Prepare for NebulaGraph as Graph Store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5561b495",
   "metadata": {},
   "source": [
    "‚ùóAccess NebulaGraph Console to **create space** and **graph schema**\n",
    "\n",
    "```sql\n",
    "CREATE SPACE guardians(vid_type=FIXED_STRING(256), partition_num=1, replica_factor=1);\n",
    ":sleep 10;\n",
    "USE guardians;\n",
    "CREATE TAG entity(name string);\n",
    "CREATE EDGE relationship(relationship string);\n",
    ":sleep 10;\n",
    "CREATE TAG INDEX entity_index ON entity(name(256));\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa40d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install nebula3-python ipython-ngql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801f88ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['NEBULA_USER'] = \"root\"\n",
    "os.environ['NEBULA_PASSWORD'] = \"nebula\" # default password\n",
    "os.environ['NEBULA_ADDRESS'] = \"192.168.30.158:9669\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ede3c10",
   "metadata": {},
   "source": [
    "## 2. Load from disk Llama Indexes\n",
    "\n",
    "**You have to run cells in $2 and $3 or download index.zip first**\n",
    "\n",
    "Both the `KnowledgeGraphIndex` and `VectorStoreIndex` will be created only once, afterwards, we could persist their in-memory context to enable their reuse from disk anytime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bc1c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import load_index_from_storage\n",
    "\n",
    "assert os.path.exists(os.path.join(os.path.abspath(os.path.join('..')), 'storage/storage_graph')), 'Do not have graph storage_context in disk'\n",
    "assert os.path.exists(os.path.join(os.path.abspath(os.path.join('..')), 'storage/storage_vector')), 'Do not have vector storage_context in disk'\n",
    "\n",
    "entries = os.listdir()\n",
    "folders = [entry for entry in entries if os.path.isdir(os.path.join(entry))]\n",
    "\n",
    "kg_index_ls = []\n",
    "vector_index_ls = []\n",
    "for nodes_group in folders:\n",
    "    space_name = f\"books_content_{nodes_group}\"\n",
    "    edge_types, rel_prop_names = [\"relationship\"], [\"relationship\"] # default, could be omit if create from an empty kg\n",
    "    tags = [\"entity\"] # default, could be omit if create from an empty kg\n",
    "\n",
    "    graph_store = NebulaGraphStore(\n",
    "        space_name=space_name,\n",
    "        edge_types=edge_types,\n",
    "        rel_prop_names=rel_prop_names,\n",
    "        tags=tags,\n",
    "    )\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=f'../storage/storage_graph/{nodes_group}', graph_store=graph_store)\n",
    "    kg_index = load_index_from_storage(\n",
    "        storage_context=storage_context,\n",
    "        space_name=space_name,\n",
    "        edge_types=edge_types,\n",
    "        rel_prop_names=rel_prop_names,\n",
    "        tags=tags,\n",
    "        include_embeddings=True,\n",
    "    )\n",
    "    kg_index_ls.append(kg_index)\n",
    "\n",
    "    storage_context_vector = StorageContext.from_defaults(persist_dir=f'../storage_vector/{nodes_group}')\n",
    "    vector_index = load_index_from_storage(\n",
    "    #     service_context=service_context,\n",
    "        storage_context=storage_context_vector\n",
    "    )\n",
    "    vector_index_ls.append(vector_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare for different query approaches\n",
    "\n",
    "We will do 4 types of query approaches with LLM, KG, VectorDB:\n",
    "\n",
    "| QueryEngine | Knowledge Graph query engine                                 | Graph RAG query engine                                       | Vector RAG query engine                                      | Graph Vector RAG query engine                                |\n",
    "| ----------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n",
    "| Mechanism   | 1. **Text-to-GraphQuery** based on KG<br />2. Query KG with the result<br />3. Answer synthesis based on query result | 1. Get related entities of the question<br />2. Get n-depth **SubGraphs** of related entities from KG<br />3. Answer synthesis based on related SubGraphs | 1. Create embedding of question<br />2. Semantic search **top-k related doc chunks**<br />3. Answer synthesis based on related doc chunks | 1. Do retrieval as Vector and Graph RAG <br />2. Answer synthesis based on **both related chunks and SubGraphs** |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 text-to-NebulaGraphCypher\n",
    "\n",
    "Text-to-NebulaGraphCypher approach Translate task/question into a Graph Cypher Query, and answer based on its query result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import KnowledgeGraphQueryEngine\n",
    "\n",
    "from llama_index.core import load_index_from_storage\n",
    "\n",
    "assert os.path.exists(os.path.join(os.path.abspath(os.path.join('..')), 'storage/storage_graph')), 'Do not have graph storage_context in disk'\n",
    "assert os.path.exists(os.path.join(os.path.abspath(os.path.join('..')), 'storage/storage_vector')), 'Do not have vector storage_context in disk'\n",
    "\n",
    "entries = os.listdir()\n",
    "folders = [entry for entry in entries if os.path.isdir(os.path.join(entry))]\n",
    "\n",
    "nl2kg_qg_ls = []\n",
    "for nodes_group in folders:\n",
    "    space_name = f\"books_content_{nodes_group}\"\n",
    "    edge_types, rel_prop_names = [\"relationship\"], [\"relationship\"] # default, could be omit if create from an empty kg\n",
    "    tags = [\"entity\"] # default, could be omit if create from an empty kg\n",
    "\n",
    "    graph_store = NebulaGraphStore(\n",
    "        space_name=space_name,\n",
    "        edge_types=edge_types,\n",
    "        rel_prop_names=rel_prop_names,\n",
    "        tags=tags,\n",
    "    )\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=f'../storage/storage_graph/{nodes_group}', graph_store=graph_store)\n",
    "\n",
    "    nl2kg_query_engine = KnowledgeGraphQueryEngine(\n",
    "        storage_context=storage_context,\n",
    "    #     service_context=service_context,\n",
    "        verbose=True\n",
    "    )\n",
    "    nl2kg_qg_ls.append(nl2kg_query_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl2kg_query_engine.get_prompts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Graph RAG query engine\n",
    "\n",
    "Graph RAG takes SubGraphs related to entities of the task/question as Context.\n",
    "\n",
    "```\n",
    "           Graph RAG with Llama Index\n",
    "                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  \n",
    "                  ‚îÇ 1  ‚îÇ 2  ‚îÇ 3  ‚îÇ 4  ‚îÇ                  \n",
    "                  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î§                  \n",
    "                  ‚îÇ  Docs/Knowledge   ‚îÇ                  \n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ        ...        ‚îÇ       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ       ‚îÇ         ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î§       ‚îÇ         ‚îÇ\n",
    "‚îÇ       ‚îÇ         ‚îÇ 95 ‚îÇ 96 ‚îÇ    ‚îÇ    ‚îÇ       ‚îÇ         ‚îÇ\n",
    "‚îÇ       ‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ         ‚îÇ\n",
    "‚îÇ User  ‚îÇ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ‚ñ∂   LLM   ‚îÇ\n",
    "‚îÇ       ‚îÇ                                     ‚îÇ         ‚îÇ\n",
    "‚îÇ       ‚îÇ                                     ‚îÇ         ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îå ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îê  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "    ‚îÇ          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚ñ≤     \n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚ñ∂‚îÇ  Tell me about x, please ‚îÇ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     \n",
    "               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              \n",
    "             ‚îÇ Below are knowledge about x ‚îÇ             \n",
    "               x->y<-z,x->h->i, m<-n,...                            \n",
    "             ‚îÇ Please answer based on them ‚îÇ             \n",
    "              ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_rag_qg_ls = []\n",
    "for kg_index in kg_index_ls:\n",
    "    kg_rag_query_engine = kg_index.as_query_engine(\n",
    "        include_text=False,\n",
    "        retriever_mode=\"hybrid\",\n",
    "        response_mode=\"tree_summarize\",\n",
    "    )\n",
    "    kg_rag_qg_ls.append(kg_rag_query_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Vector RAG query engine\n",
    "\n",
    "Vector RAG is the common approach to find topK semantic related doc chunks as context to synthesize the answer.\n",
    "\n",
    "```\n",
    "                  RAG with Llama Index\n",
    "                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  \n",
    "                  ‚îÇ 1  ‚îÇ 2  ‚îÇ 3  ‚îÇ 4  ‚îÇ                  \n",
    "                  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î§                  \n",
    "                  ‚îÇ  Docs/Knowledge   ‚îÇ                  \n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ        ...        ‚îÇ       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ       ‚îÇ         ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î§       ‚îÇ         ‚îÇ\n",
    "‚îÇ       ‚îÇ         ‚îÇ 95 ‚îÇ 96 ‚îÇ    ‚îÇ    ‚îÇ       ‚îÇ         ‚îÇ\n",
    "‚îÇ       ‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ         ‚îÇ\n",
    "‚îÇ User  ‚îÇ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ‚ñ∂   LLM   ‚îÇ\n",
    "‚îÇ       ‚îÇ                                     ‚îÇ         ‚îÇ\n",
    "‚îÇ       ‚îÇ                                     ‚îÇ         ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îå ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îê  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "    ‚îÇ          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚ñ≤     \n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚ñ∂‚îÇ  Tell me ....., please   ‚îÇ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     \n",
    "               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              \n",
    "             ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îÇ             \n",
    "               ‚îÇ 3  ‚îÇ ‚îÇ 96 ‚îÇ                             \n",
    "             ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ             \n",
    "              ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_rag_qg_ls = []\n",
    "for vetor_index in vector_index_ls:\n",
    "    vector_rag_query_engine = vector_index.as_query_engine()\n",
    "    vector_rag_qg_ls.append(vector_rag_query_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Graph+Vector RAG query engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a combined Graph+Vector Based RAG, where we will retrieve both VectorDB and KG SubGraphs as the context, for synthesis of the answer.\n",
    "\n",
    "```\n",
    "           Graph + Vector RAG with Llama Index\n",
    "                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  \n",
    "                  ‚îÇ 1  ‚îÇ 2  ‚îÇ 3  ‚îÇ 4  ‚îÇ                  \n",
    "                  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î§                  \n",
    "                  ‚îÇ  Docs/Knowledge   ‚îÇ                  \n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ        ...        ‚îÇ       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ       ‚îÇ         ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î§       ‚îÇ         ‚îÇ\n",
    "‚îÇ       ‚îÇ         ‚îÇ 95 ‚îÇ 96 ‚îÇ    ‚îÇ    ‚îÇ       ‚îÇ         ‚îÇ\n",
    "‚îÇ       ‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ         ‚îÇ\n",
    "‚îÇ User  ‚îÇ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ‚ñ∂   LLM   ‚îÇ\n",
    "‚îÇ       ‚îÇ                                     ‚îÇ         ‚îÇ\n",
    "‚îÇ       ‚îÇ                                     ‚îÇ         ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îå ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îê  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "    ‚îÇ          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚ñ≤     \n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚ñ∂‚îÇ  Tell me ....., please   ‚îÇ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     \n",
    "               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              \n",
    "             ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îÇ             \n",
    "               ‚îÇ 3  ‚îÇ‚îÇ 96 ‚îÇ x->y<-z,x->h...                            \n",
    "             ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ             \n",
    "              ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ \n",
    "```\n",
    "\n",
    "To implement that in Llama Index, we create a `CustomRetriever` to comebine the two: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import QueryBundle\n",
    "from llama_index.core import QueryBundle\n",
    "\n",
    "# import NodeWithScore\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "\n",
    "# Retrievers\n",
    "from llama_index.core.retrievers import BaseRetriever, VectorIndexRetriever, KGTableRetriever\n",
    "\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class CustomRetriever(BaseRetriever):\n",
    "    \"\"\"Custom retriever that performs both Vector search and Knowledge Graph search\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_retriever: VectorIndexRetriever,\n",
    "        kg_retriever: KGTableRetriever,\n",
    "        mode: str = \"OR\",\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "\n",
    "        self._vector_retriever = vector_retriever\n",
    "        self._kg_retriever = kg_retriever\n",
    "        if mode not in (\"AND\", \"OR\"):\n",
    "            raise ValueError(\"Invalid mode.\")\n",
    "        self._mode = mode\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query.\"\"\"\n",
    "\n",
    "        vector_nodes = self._vector_retriever.retrieve(query_bundle)\n",
    "        kg_nodes = self._kg_retriever.retrieve(query_bundle)\n",
    "\n",
    "        vector_ids = {n.node.node_id for n in vector_nodes}\n",
    "        kg_ids = {n.node.node_id for n in kg_nodes}\n",
    "\n",
    "        combined_dict = {n.node.node_id: n for n in vector_nodes}\n",
    "        combined_dict.update({n.node.node_id: n for n in kg_nodes})\n",
    "\n",
    "        if self._mode == \"AND\":\n",
    "            retrieve_ids = vector_ids.intersection(kg_ids)\n",
    "        else:\n",
    "            retrieve_ids = vector_ids.union(kg_ids)\n",
    "\n",
    "        retrieve_nodes = [combined_dict[rid] for rid in retrieve_ids]\n",
    "        return retrieve_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create instances of the Vector and KG retrievers, which will be used in the instantiation of the Custom Retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "kg_vec_rag_qg_ls = []\n",
    "for kg_index, vector_index in zip(kg_index_ls, vector_index_ls):\n",
    "    # create custom retriever\n",
    "    vector_retriever = VectorIndexRetriever(index=vector_index)\n",
    "    kg_retriever = KGTableRetriever(\n",
    "        index=kg_index, retriever_mode=\"keyword\", include_text=False\n",
    "    )\n",
    "    custom_retriever = CustomRetriever(vector_retriever, kg_retriever)\n",
    "\n",
    "    # create response synthesizer\n",
    "    response_synthesizer = get_response_synthesizer(\n",
    "    #     service_context=service_context,\n",
    "        response_mode=\"tree_summarize\",\n",
    "    )\n",
    "    kg_vector_rag_query_engine = RetrieverQueryEngine(\n",
    "    retriever=custom_retriever,\n",
    "    response_synthesizer=response_synthesizer\n",
    "    )\n",
    "    kg_vec_rag_qg_ls.append(kg_vector_rag_query_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ac5564",
   "metadata": {},
   "source": [
    "### 3.5 General load index from disk and get query engine function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e463125",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import QueryBundle\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "from llama_index.core.retrievers import BaseRetriever, VectorIndexRetriever, KGTableRetriever\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "def get_all_query_engine_from_cache_index(kg_index_folder_path, vector_index_folder_path, nodes_group: str|List[str]):\n",
    "    if isinstance(nodes_group, str):\n",
    "        nodes_group_ls = [nodes_group]\n",
    "    else:\n",
    "        nodes_group_ls = nodes_group\n",
    "    query_engine_dc = {\n",
    "        # 'nl2kg': {},\n",
    "        'kg_rag': {},\n",
    "        'vec_rag': {},\n",
    "        'kg_vec_rag': {}\n",
    "    }\n",
    "    for nodes_group in nodes_group_ls:\n",
    "        space_name = f\"books_content_{nodes_group}\"\n",
    "        edge_types, rel_prop_names = [\"relationship\"], [\"relationship\"] # default, could be omit if create from an empty kg\n",
    "        tags = [\"entity\"] # default, could be omit if create from an empty kg\n",
    "\n",
    "        graph_store = NebulaGraphStore(\n",
    "            space_name=space_name,\n",
    "            edge_types=edge_types,\n",
    "            rel_prop_names=rel_prop_names,\n",
    "            tags=tags,\n",
    "        )\n",
    "        storage_context_kg = StorageContext.from_defaults(persist_dir=kg_index_folder_path + f'/{nodes_group}', graph_store=graph_store)\n",
    "        kg_index = load_index_from_storage(\n",
    "            storage_context=storage_context_kg,\n",
    "            space_name=space_name,\n",
    "            edge_types=edge_types,\n",
    "            rel_prop_names=rel_prop_names,\n",
    "            tags=tags,\n",
    "            include_embeddings=True,\n",
    "        )\n",
    "\n",
    "        storage_context_vector = StorageContext.from_defaults(persist_dir=vector_index_folder_path + f'/{nodes_group}')\n",
    "        vector_index = load_index_from_storage(\n",
    "            storage_context=storage_context_vector\n",
    "        )\n",
    "\n",
    "        # # text2cypher query engine\n",
    "        # nl2kg_query_engine = KnowledgeGraphQueryEngine(\n",
    "        #     storage_context=storage_context_kg,\n",
    "        #     verbose=True\n",
    "        # )\n",
    "        # query_engine_dc['nl2kg'].append(nl2kg_query_engine)\n",
    "        \n",
    "        # kg_rag query engine\n",
    "        kg_rag_query_engine = kg_index.as_query_engine(\n",
    "            include_text=False,\n",
    "            response_mode=\"tree_summarize\"\n",
    "        )\n",
    "        query_engine_dc['kg_rag'][nodes_group] = kg_rag_query_engine\n",
    "        # vec_rag query engine\n",
    "        vec_rag_query_engine = vector_index.as_query_engine(response_mode=\"tree_summarize\")\n",
    "        query_engine_dc['vec_rag'][nodes_group] =  vec_rag_query_engine\n",
    "        # kg_vec_rag query engine\n",
    "        vector_retriever = VectorIndexRetriever(index=vector_index)\n",
    "        kg_retriever = KGTableRetriever(\n",
    "            index=kg_index, retriever_mode=\"keyword\", include_text=False\n",
    "        )\n",
    "        custom_retriever = CustomRetriever(vector_retriever, kg_retriever)\n",
    "\n",
    "        response_synthesizer = get_response_synthesizer(response_mode=\"tree_summarize\")\n",
    "        kg_vector_rag_query_engine = RetrieverQueryEngine(\n",
    "            retriever=custom_retriever,\n",
    "            response_synthesizer=response_synthesizer\n",
    "        )\n",
    "        query_engine_dc['kg_vec_rag'][nodes_group] = kg_vector_rag_query_engine\n",
    "    return query_engine_dc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Base Query with all the Engines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Text-to-GraphQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_nl2kg = nl2kg_query_engine.query(\"‰ªÄ‰πàÊòØÁªèÊµéÂ≠¶ÂçÅÂ§ßÂéüÁêÜ.\")\n",
    "\n",
    "\n",
    "display(Markdown(f\"<b>{response_nl2kg}</b>\"))\n",
    "\n",
    "# Cypher:\n",
    "\n",
    "print(\"Cypher Query:\")\n",
    "\n",
    "graph_query = nl2kg_query_engine.generate_query(\n",
    "    \"‰ªÄ‰πàÊòØÁªèÊµéÂ≠¶ÂçÅÂ§ßÂéüÁêÜ\",\n",
    ")\n",
    "graph_query = graph_query.replace(\"WHERE\", \"\\n  WHERE\").replace(\"RETURN\", \"\\nRETURN\")\n",
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        f\"\"\"\n",
    "```cypher\n",
    "{graph_query}\n",
    "```\n",
    "\"\"\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Graph RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_graph_rag = kg_rag_query_engine.query(\"‰ªÄ‰πàÊòØÁªèÊµéÂ≠¶ÂçÅÂ§ßÂéüÁêÜ\")\n",
    "\n",
    "display(Markdown(f\"<b>{response_graph_rag}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Vector RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_vector_rag = vector_rag_query_engine.query(\"‰ªÄ‰πàÊòØÁªèÊµéÂ≠¶ÂçÅÂ§ßÂéüÁêÜ\")\n",
    "\n",
    "display(Markdown(f\"<b>{response_vector_rag}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    Markdown(\n",
    "        llm.complete(f\"\"\"\n",
    "Compare the two QA result on \"‰ªÄ‰πàÊòØÁªèÊµéÂ≠¶ÂçÅÂ§ßÂéüÁêÜ\", list the differences between them, to help evalute them. Output in markdown table.\n",
    "\n",
    "Result from Graph: {response_graph_rag}\n",
    "---\n",
    "Result from Vector: {response_vector_rag}\n",
    "\n",
    "\"\"\"\n",
    "           ).text\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Graph + Vector RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_graph_vector_rag = graph_vector_rag_query_engine.query(\"‰ªÄ‰πàÊòØÁªèÊµéÂ≠¶ÂçÅÂ§ßÂéüÁêÜ\")\n",
    "\n",
    "display(Markdown(f\"<b>{response_graph_vector_rag}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Overall Comparision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the results of them.\n",
    "\n",
    "First check the information that were coverred by different approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    Markdown(\n",
    "        llm(f\"\"\"\n",
    "Compare the QA results on \"Tell me about Peter Quill.\", list the knowledge facts between them, to help evalute them. Output in markdown table.\n",
    "\n",
    "Result text2GraphQuery: {response_nl2kg}\n",
    "---\n",
    "Result Graph: {response_graph_rag}\n",
    "---\n",
    "Result Vector: {response_vector_rag}\n",
    "---\n",
    "Result Graph+Vector: {response_graph_vector_rag}\n",
    "---\n",
    "\n",
    "\"\"\"\n",
    "           )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "- The pure **KG**(both text2GraphQuery and Graph RAG) comes with **concise** results, and much **lower cost**(for cost comparision see our previous result [here](https://gpt-index.readthedocs.io/en/latest/examples/index_structs/knowledge_graph/KnowledgeGraphIndex_vs_VectorStoreIndex_vs_CustomIndex_combined.html#comparison-of-results) )\n",
    "- The **Graph+Vector** RAG could be more **comprehensive** in case the question envolves knowledge that's fine-grained **spread** across more chunks than top-K searching.\n",
    "\n",
    "\n",
    "| QueryEngine | Knowledge Graph query engine                                 | Graph RAG query engine                                       | Vector RAG query engine                                      | Graph Vector RAG query engine                                |\n",
    "| ----------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n",
    "| Mechanism   | 1. **Text-to-GraphQuery** based on KG<br />2. Query KG with the result<br />3. Answer synthesis based on query result | 1. Get related entities of the question<br />2. Get n-depth **SubGraphs** of related entities from KG<br />3. Answer synthesis based on related SubGraphs | 1. Create embedding of question<br />2. Semantic search **top-k related doc chunks**<br />3. Answer synthesis based on related doc chunks | 1. Do retrieval as Vector and Graph RAG <br />2. Answer synthesis based on **both related chunks and SubGraphs** |\n",
    "| Performance | Concise                                                      | Concise                                                      | Fruitful                                                     | Fruitful, could be more comprehensive                        |\n",
    "| Cost        | Low                                                          | Low                                                          | High                                                         | High                                                         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "For those tasks:\n",
    "\n",
    "- Potentially cares more relationed knowledge\n",
    "- Schema of the KG is sophisticated to be hard for text2cypher to express the task\n",
    "- KG quality isn't good enough\n",
    "- Multiple \"starting entities\" are involved\n",
    "\n",
    "Graph RAG could be a better approach to start with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b21d109",
   "metadata": {},
   "source": [
    "## 7. Financial Evaluation on four types of engines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7d3cd5",
   "metadata": {},
   "source": [
    "### 7.1 FinEval on query engines base on nodes of different chunk sizes and chunk overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590bcfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size_ls = [256, 512, 1024]\n",
    "chunk_overlap_pct_ls = [1/8, 1/4]\n",
    "nodes_group_ls = []\n",
    "for chunk_size in chunk_size_ls:\n",
    "    for chunk_overlap_pct in chunk_overlap_pct_ls:\n",
    "        chunk_overlap = int(chunk_size * chunk_overlap_pct)\n",
    "        nodes_group = f'size_{chunk_size}_overlap_{chunk_overlap}'\n",
    "        nodes_group_ls.append(nodes_group)\n",
    "query_engine_dc = get_all_query_engine_from_cache_index(kg_index_folder_path='../storage/storage_graph',\n",
    "                                                        vector_index_folder_path='../storage/storage_vector',\n",
    "                                                        nodes_group=nodes_group_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2911c5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from query_engine_evaluator import QueryEngineEvaluator\n",
    "\n",
    "choices = [\"A\", \"B\", \"C\", \"D\"]\n",
    "eval_path = ''\n",
    "\n",
    "def fineval(args, evaluator, take):\n",
    "    assert os.path.exists(eval_path + \"subject_mapping.json\"), \"subject_mapping.json not found!\"\n",
    "    with open(eval_path+ \"subject_mapping.json\") as f:\n",
    "        subject_mapping = json.load(f)\n",
    "    filenames = os.listdir(eval_path + \"data/val\")\n",
    "    subject_list = [val_file.replace(\"_val.csv\", \"\") for val_file in filenames]\n",
    "    accuracy, summary = {}, {}\n",
    "\n",
    "    run_date = time.strftime('%Y-%m-%d_%H-%M-%S', time.localtime(time.time()))\n",
    "    output_dir = args['output_dir']\n",
    "    save_result_dir = os.path.join(output_dir, f\"take{take}\")\n",
    "    if not os.path.exists(save_result_dir):\n",
    "        os.makedirs(save_result_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"############# nodes group: {args['nodes_group']} ###############\")\n",
    "\n",
    "    all_answers = {}\n",
    "    for index, subject_name in enumerate(subject_list):\n",
    "        print(\n",
    "            f\"{index / len(subject_list)} Inference starts at {run_date} on {args['model_name']} with subject of {subject_name}!\")\n",
    "        val_file_path = os.path.join('data/val', f'{subject_name}_val.csv')\n",
    "        dev_file_path = os.path.join('data/dev', f'{subject_name}_dev.csv')\n",
    "        test_file_path = os.path.join('data/test', f'{subject_name}_test.csv')\n",
    "\n",
    "        val_df = pd.read_csv(val_file_path) if args['do_test'] is False else pd.read_csv(test_file_path)\n",
    "        dev_df = pd.read_csv(dev_file_path) if args['few_shot'] else None\n",
    "\n",
    "        correct_ratio, answers = evaluator.eval_subject(subject_name, val_df, dev_df,\n",
    "                                                        save_result_dir=save_result_dir if args['do_save_csv'] else None,\n",
    "                                                        few_shot=args['few_shot'],\n",
    "                                                        cot=args['cot'],\n",
    "                                                        )\n",
    "        print(f\"Subject: {subject_name}\")\n",
    "        print(f\"Acc: {correct_ratio}\")\n",
    "        accuracy[subject_name] = correct_ratio\n",
    "        summary[subject_name] = {\"score\": correct_ratio,\n",
    "                                 \"num\": len(val_df),\n",
    "                                 \"correct\": correct_ratio * len(val_df) / 100}\n",
    "        all_answers[subject_name] = answers\n",
    "\n",
    "    json.dump(all_answers, open(save_result_dir + '/submission.json', 'w'), ensure_ascii=False, indent=4)\n",
    "    print(\"Accuracy:\")\n",
    "    for k, v in accuracy.items():\n",
    "        print(k, \": \", v)\n",
    "\n",
    "    total_num = 0\n",
    "    total_correct = 0\n",
    "    summary['grouped'] = {\n",
    "        \"Accounting\": {\"correct\": 0.0, \"num\": 0},\n",
    "        \"Finance\": {\"correct\": 0.0, \"num\": 0},\n",
    "        \"Economy\": {\"correct\": 0.0, \"num\": 0},\n",
    "        \"Certificate\": {\"correct\": 0.0, \"num\": 0}\n",
    "    }\n",
    "    for subj, info in subject_mapping.items():\n",
    "        group = info[2]\n",
    "        summary['grouped'][group][\"num\"] += summary[subj]['num']\n",
    "        summary['grouped'][group][\"correct\"] += summary[subj]['correct']\n",
    "    for group, info in summary['grouped'].items():\n",
    "        info['score'] = info[\"correct\"] / info[\"num\"]\n",
    "        total_num += info[\"num\"]\n",
    "        total_correct += info[\"correct\"]\n",
    "    summary['All'] = {\"score\": total_correct / total_num, \"num\": total_num, \"correct\": total_correct}\n",
    "\n",
    "    print('-' * 80)\n",
    "    print(\"Accuracy_subject:\")\n",
    "    for k, v in accuracy.items():\n",
    "        print(k, \": \", v)\n",
    "    print('-' * 80)\n",
    "    print(\"Accuracy_grouped:\")\n",
    "    for k, v in summary['grouped'].items():\n",
    "        print(k, \": \", v['score'])\n",
    "\n",
    "    print(\"Avg: \")\n",
    "    print(summary['All']['score'])\n",
    "\n",
    "    json.dump(summary, open(save_result_dir + '/summary.json', 'w'), ensure_ascii=False, indent=2)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19908354",
   "metadata": {},
   "outputs": [],
   "source": [
    "cot = False\n",
    "few_shot = False\n",
    "ntrain = 5\n",
    "n_times = 1\n",
    "do_save_csv = False\n",
    "output_dir = eval_path + 'output'\n",
    "model_name = 'chatglm'\n",
    "do_test = False\n",
    "args = dict(\n",
    "    cot=cot,\n",
    "    few_shot = few_shot,\n",
    "    ntrain = ntrain,\n",
    "    n_times = n_times,\n",
    "    do_save_csv = do_save_csv,\n",
    "    output_dir = output_dir,\n",
    "    model_name = model_name,\n",
    "    do_test = do_test\n",
    ")\n",
    "\n",
    "tree_summary_template = \\\n",
    "    \"‰ªé‰∏çÂêåÊù•Ê∫êËé∑ÂèñÁöÑÂèÇËÄÉ‰ø°ÊÅØÂ¶Ç‰∏ã:\\n\" \\\n",
    "    \"---------------------\\n\" \\\n",
    "    \"{context_str}\\n\" \\\n",
    "    \"---------------------\\n\" \\\n",
    "    \"È¢òÁõÆ:{query_str}\" \n",
    "\n",
    "graph_query_synthesis_prompt = \\\n",
    "    \"\"\"\n",
    "    ‰Ω†ÊòØ‰∏Ä‰∏™‰∏ì‰∏öÁöÑ‰∫∫Â∑•Êô∫ËÉΩÂä©Êâã\n",
    "    Áé∞Âú®Êúâ‰∏Ä‰∏™Âü∫‰∫éNebulaÊê≠Âª∫ÁöÑË¥¢ÁªèÁôæÁßëÁü•ËØÜÂõæË∞±ÔºåÁªôÂÆöËøô‰∏™ÂõæË∞±Ê°ÜÊû∂ÁªìÊûÑÔºåËØ∑‰Ω†Ê†πÊçÆÊ°ÜÊû∂ÁªìÊûÑÂ∞ÜÈóÆÈ¢òÊñáÊú¨ËΩ¨ÂåñÊàêNebula CypherÊü•ËØ¢ËØ≠Âè•\n",
    "    ‰øùËØÅÊü•ËØ¢ËØ≠ÂèØ‰ª•Áõ¥Êé•Âú®NebulaÁªàÁ´Ø‰∏≠ËøêË°å\n",
    "\n",
    "    Áü•ËØÜÂõæË∞±Ê°ÜÊû∂:{schema}\n",
    "    ÈóÆÈ¢ò: {query_str}\n",
    "    \"\"\"\n",
    "\n",
    "graph_response_answer_prompt = \\\n",
    "    \"\"\"\n",
    "    ÂéüÈóÆÈ¢òË¢´ËΩ¨ÂåñÊàê‰∫ÜÊü•ËØ¢ËØ≠Âè•ÔºåÊü•ËØ¢ËØ≠Âè•ÂíåÊü•ËØ¢ÁªìÊûúÂ∞Ü‰Ωú‰∏∫ÂèÇËÄÉ‰ø°ÊÅØÔºåÂ¶Ç‰∏ã:\n",
    "\n",
    "    Êü•ËØ¢ËØ≠Âè•: {kg_query_str}\n",
    "    Êü•ËØ¢ÁªìÊûú: {kg_response_str}\n",
    "    È¢òÁõÆ: {query_str}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d971610a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for nodes_group in nodes_group_ls:\n",
    "    args['nodes_group'] = nodes_group\n",
    "    query_engine = query_engine_dc['kg_rag'][nodes_group]\n",
    "    prompt_dict = dict(\n",
    "        summary_template = [\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': tree_summary_template\n",
    "             }])\n",
    "    evaluator = QueryEngineEvaluator(query_engine=query_engine, prompt_dict=prompt_dict, choices=choices,\n",
    "                                     k=args['ntrain'], model_name=args['model_name'])\n",
    "    fineval(args=args, evaluator=evaluator, take=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3962596a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>}, template_vars=['query'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, message_templates=[ChatMessage(role=<MessageRole.SYSTEM: 'system'>, content=\"Always answer the question, even if the context isn't helpful.\", additional_kwargs={}), ChatMessage(role=<MessageRole.USER: 'user'>, content='{query}', additional_kwargs={})])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core import ChatPromptTemplate\n",
    "\n",
    "# Text QA Prompt\n",
    "chat_text_qa_msgs = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"Always answer the question, even if the context isn't helpful.\",\n",
    "    ),\n",
    "    (\"user\", '{query}'),\n",
    "]\n",
    "text_qa_template = ChatPromptTemplate.from_messages(chat_text_qa_msgs)\n",
    "text_qa_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4a6953",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = QueryEngineEvaluator(\n",
    "    choices=choices,\n",
    "    k=args.ntrain,\n",
    "    model_name=args.model_name\n",
    ")\n",
    "for i in range(args.n_times):\n",
    "    fineval(args, evaluator=evaluator, take=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063d5cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatglm3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
