{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b713e1a",
   "metadata": {},
   "source": [
    "# 1. Preparation\n",
    "\n",
    "## 1.1 Prepare for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99a3dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install llama-index-llms-azure-openai\n",
    "# %pip install llama-index-graph-stores-nebula\n",
    "# %pip install llama-index-llms-openai\n",
    "# %pip install llama-index-embeddings-azure-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a002bd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For OpenAI\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "# logging.basicConfig(\n",
    "#     stream=sys.stdout, level=logging.INFO\n",
    "# )  # logging.DEBUG for more verbose output\n",
    "\n",
    "from llama_index.core import (\n",
    "    KnowledgeGraphIndex,\n",
    "    VectorStoreIndex,\n",
    "    ServiceContext,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    PromptTemplate,\n",
    "    load_index_from_storage\n",
    ")\n",
    "from llama_index.core.query_engine import KnowledgeGraphQueryEngine\n",
    "\n",
    "from llama_index.graph_stores.nebula import NebulaGraphStore\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from finllmqa.api.core import LLM_API_URL\n",
    "\n",
    "from llama_index.core import Settings\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", api_base='http://gemini2.sufe.edu.cn:27282/v1', api_key='null')\n",
    "embed_model = OpenAIEmbedding(api_base='http://gemini2.sufe.edu.cn:27282/v1', api_key='null')\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafd15f5",
   "metadata": {},
   "source": [
    "## 1.2. Prepare for NebulaGraph as Graph Store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa40d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install nebula3-python ipython-ngql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801f88ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['NEBULA_USER'] = \"root\"\n",
    "os.environ['NEBULA_PASSWORD'] = \"nebula\" # default password\n",
    "os.environ['NEBULA_ADDRESS'] = \"127.0.0.1:9669\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ede3c10",
   "metadata": {},
   "source": [
    "## 2. Load from disk Llama Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bc1c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import load_index_from_storage\n",
    "\n",
    "assert os.path.exists(os.path.join(os.path.abspath(os.path.join('..')), 'storage/storage_graph')), 'Do not have graph storage_context in disk'\n",
    "assert os.path.exists(os.path.join(os.path.abspath(os.path.join('..')), 'storage/storage_vector')), 'Do not have vector storage_context in disk'\n",
    "\n",
    "entries = os.listdir()\n",
    "folders = [entry for entry in entries if os.path.isdir(os.path.join(entry))]\n",
    "\n",
    "kg_index_ls = []\n",
    "vector_index_ls = []\n",
    "for nodes_group in folders:\n",
    "    space_name = f\"books_content_{nodes_group}\"\n",
    "    edge_types, rel_prop_names = [\"relationship\"], [\"relationship\"] # default, could be omit if create from an empty kg\n",
    "    tags = [\"entity\"] # default, could be omit if create from an empty kg\n",
    "\n",
    "    graph_store = NebulaGraphStore(\n",
    "        space_name=space_name,\n",
    "        edge_types=edge_types,\n",
    "        rel_prop_names=rel_prop_names,\n",
    "        tags=tags,\n",
    "    )\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=f'../storage/storage_graph/{nodes_group}', graph_store=graph_store)\n",
    "    kg_index = load_index_from_storage(\n",
    "        storage_context=storage_context,\n",
    "        space_name=space_name,\n",
    "        edge_types=edge_types,\n",
    "        rel_prop_names=rel_prop_names,\n",
    "        tags=tags,\n",
    "        include_embeddings=True,\n",
    "    )\n",
    "    kg_index_ls.append(kg_index)\n",
    "\n",
    "    storage_context_vector = StorageContext.from_defaults(persist_dir=f'../storage_vector/{nodes_group}')\n",
    "    vector_index = load_index_from_storage(\n",
    "    #     service_context=service_context,\n",
    "        storage_context=storage_context_vector\n",
    "    )\n",
    "    vector_index_ls.append(vector_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare for different query approaches\n",
    "\n",
    "We will do 3 types of query approaches with LLM, KG, VectorDB:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Graph RAG query engine\n",
    "\n",
    "Graph RAG takes SubGraphs related to entities of the task/question as Context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_rag_qg_ls = []\n",
    "for kg_index in kg_index_ls:\n",
    "    kg_rag_query_engine = kg_index.as_query_engine(\n",
    "        include_text=False,\n",
    "        retriever_mode=\"hybrid\",\n",
    "        response_mode=\"tree_summarize\",\n",
    "    )\n",
    "    kg_rag_qg_ls.append(kg_rag_query_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Vector RAG query engine\n",
    "\n",
    "Vector RAG is the common approach to find topK semantic related doc chunks as context to synthesize the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_rag_qg_ls = []\n",
    "for vetor_index in vector_index_ls:\n",
    "    vector_rag_query_engine = vector_index.as_query_engine()\n",
    "    vector_rag_qg_ls.append(vector_rag_query_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Graph+Vector RAG query engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a combined Graph+Vector Based RAG, where we will retrieve both VectorDB and KG SubGraphs as the context, for synthesis of the answer.\n",
    "\n",
    "In Llama Index, we set include_text = True in KGTableRetriever to get the combination of kg and vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "kg_vec_rag_qg_ls = []\n",
    "nodes_group = 'size_512_overlap_64'\n",
    "space_name = f\"book_微观经济学_{nodes_group}\"\n",
    "edge_types, rel_prop_names = [\"关系\"], [\"关系\"] # default, could be omit if create from an empty kg\n",
    "tags = [\"实体\"] # default, could be omit if create from an empty kg\n",
    "\n",
    "graph_store = NebulaGraphStore(\n",
    "    space_name=space_name,\n",
    "    edge_types=edge_types,\n",
    "    rel_prop_names=rel_prop_names,\n",
    "    tags=tags,\n",
    ")\n",
    "storage_context_kg = StorageContext.from_defaults(persist_dir='../storage/storage_graph' + f'/{nodes_group}', graph_store=graph_store)\n",
    "z\n",
    "\n",
    "\n",
    "kg_index = load_index_from_storage(\n",
    "    storage_context=storage_context,\n",
    "    space_name=space_name,\n",
    "    edge_types=edge_types,\n",
    "    rel_prop_names=rel_prop_names,\n",
    "    tags=tags,\n",
    "    include_embeddings=True,\n",
    ")\n",
    "kg_vector_rag_query_engine = kg_index.as_query_engine(\n",
    "        include_text=True,\n",
    "        response_mode=\"tree_summarize\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ac5564",
   "metadata": {},
   "source": [
    "### 3.5 General load index from disk and get query engine function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e463125",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import QueryBundle\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "from llama_index.core.retrievers import BaseRetriever, VectorIndexRetriever, KGTableRetriever\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "def get_all_query_engine_from_cache_index(kg_index_folder_path, vector_index_folder_path, nodes_group: str|List[str]):\n",
    "    if isinstance(nodes_group, str):\n",
    "        nodes_group_ls = [nodes_group]\n",
    "    else:\n",
    "        nodes_group_ls = nodes_group\n",
    "    query_engine_dc = {\n",
    "        # 'nl2kg': {},\n",
    "        'kg_rag': {},\n",
    "        'vec_rag': {},\n",
    "        'kg_vec_rag': {}\n",
    "    }\n",
    "    for nodes_group in nodes_group_ls:\n",
    "        space_name = f\"book_{nodes_group}\"\n",
    "        edge_types, rel_prop_names = [\"关系\"], [\"关系\"] # default, could be omit if create from an empty kg\n",
    "        tags = [\"实体\"] # default, could be omit if create from an empty kg\n",
    "\n",
    "        graph_store = NebulaGraphStore(\n",
    "            space_name=space_name,\n",
    "            edge_types=edge_types,\n",
    "            rel_prop_names=rel_prop_names,\n",
    "            tags=tags,\n",
    "        )\n",
    "        storage_context_kg = StorageContext.from_defaults(persist_dir=kg_index_folder_path + f'/{nodes_group}', graph_store=graph_store)\n",
    "        kg_index = load_index_from_storage(\n",
    "            storage_context=storage_context_kg,\n",
    "            space_name=space_name,\n",
    "            edge_types=edge_types,\n",
    "            rel_prop_names=rel_prop_names,\n",
    "            tags=tags,\n",
    "            include_embeddings=True,\n",
    "        )\n",
    "\n",
    "        storage_context_vector = StorageContext.from_defaults(persist_dir=vector_index_folder_path + f'/{nodes_group}')\n",
    "        vector_index = load_index_from_storage(\n",
    "            storage_context=storage_context_vector\n",
    "        )\n",
    "\n",
    "        # # text2cypher query engine\n",
    "        # nl2kg_query_engine = KnowledgeGraphQueryEngine(\n",
    "        #     storage_context=storage_context_kg,\n",
    "        #     verbose=True\n",
    "        # )\n",
    "        # query_engine_dc['nl2kg'].append(nl2kg_query_engine)\n",
    "        \n",
    "        # kg_rag query engine\n",
    "        kg_rag_query_engine = kg_index.as_query_engine(\n",
    "            include_text=False,\n",
    "            response_mode=\"tree_summarize\"\n",
    "        )\n",
    "        query_engine_dc['kg_rag'][nodes_group] = kg_rag_query_engine\n",
    "        # vec_rag query engine\n",
    "        vec_rag_query_engine = vector_index.as_query_engine(response_mode=\"tree_summarize\")\n",
    "        query_engine_dc['vec_rag'][nodes_group] =  vec_rag_query_engine\n",
    "        # kg_vec_rag query engine\n",
    "        kg_vector_rag_query_engine = kg_index.as_query_engine(\n",
    "            include_text=True,\n",
    "            response_mode=\"tree_summarize\"\n",
    "        )\n",
    "        query_engine_dc['kg_vec_rag'][nodes_group] = kg_vector_rag_query_engine\n",
    "    return query_engine_dc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Base Query with all the Engines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Text-to-GraphQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_nl2kg = nl2kg_query_engine.query(\"什么是经济学十大原理.\")\n",
    "\n",
    "\n",
    "display(Markdown(f\"<b>{response_nl2kg}</b>\"))\n",
    "\n",
    "# Cypher:\n",
    "\n",
    "print(\"Cypher Query:\")\n",
    "\n",
    "graph_query = nl2kg_query_engine.generate_query(\n",
    "    \"什么是经济学十大原理\",\n",
    ")\n",
    "graph_query = graph_query.replace(\"WHERE\", \"\\n  WHERE\").replace(\"RETURN\", \"\\nRETURN\")\n",
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        f\"\"\"\n",
    "```cypher\n",
    "{graph_query}\n",
    "```\n",
    "\"\"\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Graph RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660aa6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_rag_query_engine = query_engine_dc['kg_rag']['size_512_overlap_64']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_graph_rag = kg_rag_query_engine.query(\"生产要素分为哪几种\")\n",
    "\n",
    "display(Markdown(f\"<b>{response_graph_rag}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8f8abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response_graph_rag.source_nodes[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Vector RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fcc383",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_rag_query_engine = query_engine_dc['vec_rag']['size_256_overlap_16']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_vector_rag = vector_rag_query_engine.query(\"生产要素分为哪几种\")\n",
    "\n",
    "display(Markdown(f\"<b>{response_vector_rag}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Graph + Vector RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54641473",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_vector_rag_query_engine = query_engine_dc['kg_vec_rag']['size_512_overlap_64']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_graph_vector_rag = graph_vector_rag_query_engine.query(\"厂商的要素需求曲线向右下方倾斜的原因在于？\")\n",
    "\n",
    "display(Markdown(f\"<b>{response_graph_vector_rag}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730f44ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response_graph_vector_rag.source_nodes[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b21d109",
   "metadata": {},
   "source": [
    "## 5. Financial Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427acb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "\n",
    "choices = [\"A\", \"B\", \"C\", \"D\"]\n",
    "eval_path = ''\n",
    "\n",
    "def fineval(args, evaluator, take, subject: str | List[str] = None):\n",
    "    assert 'nodes_group' in args.keys(), 'you must assign nodes_group in args!'\n",
    "    nodes_group = args['nodes_group']\n",
    "    if subject is not None:\n",
    "        if isinstance(subject, str):\n",
    "            subject_list = [subject]\n",
    "        else:\n",
    "            subject_list = subject\n",
    "    assert os.path.exists(eval_path + \"subject_mapping.json\"), \"subject_mapping.json not found!\"\n",
    "    with open(eval_path+ \"subject_mapping.json\") as f:\n",
    "        subject_mapping = json.load(f)\n",
    "    if subject_list is not None:\n",
    "        subject_mapping_tmp = subject_mapping.copy()\n",
    "        for subject in subject_mapping_tmp.keys():\n",
    "            if subject not in subject_list:\n",
    "                del subject_mapping[subject]\n",
    "    filenames = os.listdir(eval_path + \"data/val\")\n",
    "    subject_list = [val_file.replace(\"_val.csv\", \"\") for val_file in filenames if val_file.replace(\"_val.csv\", \"\") in subject_list]\n",
    "    accuracy, summary = {}, {}\n",
    "\n",
    "    run_date = time.strftime('%Y-%m-%d_%H-%M-%S', time.localtime(time.time()))\n",
    "    output_dir = args['output_dir']\n",
    "    save_result_dir = os.path.join(output_dir, take)\n",
    "    if not os.path.exists(save_result_dir):\n",
    "        os.makedirs(save_result_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"############# nodes group: {nodes_group} ###############\")\n",
    "\n",
    "    all_answers = {}\n",
    "    for index, subject_name in enumerate(subject_list):\n",
    "        print(\n",
    "            f\"{index / len(subject_list)} Inference starts at {run_date} on {args['model_name']} with subject of {subject_name}!\")\n",
    "        val_file_path = os.path.join('data/val', f'{subject_name}_val.csv')\n",
    "        dev_file_path = os.path.join('data/dev', f'{subject_name}_dev.csv')\n",
    "        test_file_path = os.path.join('data/test', f'{subject_name}_test.csv')\n",
    "\n",
    "        val_df = pd.read_csv(val_file_path) if args['do_test'] is False else pd.read_csv(test_file_path)\n",
    "        dev_df = pd.read_csv(dev_file_path) if args['few_shot'] else None\n",
    "\n",
    "        correct_ratio, answers = evaluator.eval_subject(subject_name, val_df, dev_df,\n",
    "                                                        save_result_dir=save_result_dir if args['do_save_csv'] else None,\n",
    "                                                        few_shot=args['few_shot'],\n",
    "                                                        cot=args['cot'],\n",
    "                                                        )\n",
    "        print(f\"Subject: {subject_name}\")\n",
    "        print(f\"Acc: {correct_ratio}\")\n",
    "        accuracy[subject_name] = correct_ratio\n",
    "        summary[subject_name] = {\"score\": correct_ratio,\n",
    "                                 \"num\": len(val_df),\n",
    "                                 \"correct\": correct_ratio * len(val_df) / 100}\n",
    "        all_answers[subject_name] = answers\n",
    "\n",
    "    # json.dump(all_answers, open(save_result_dir + f'/{nodes_group}_submission.json', 'w'), ensure_ascii=False, indent=4)\n",
    "    print(\"Accuracy:\")\n",
    "    for k, v in accuracy.items():\n",
    "        print(k, \": \", v)\n",
    "\n",
    "    total_num = 0\n",
    "    total_correct = 0\n",
    "    summary['grouped'] = {\n",
    "        \"Accounting\": {\"correct\": 0.0, \"num\": 0},\n",
    "        \"Finance\": {\"correct\": 0.0, \"num\": 0},\n",
    "        \"Economy\": {\"correct\": 0.0, \"num\": 0},\n",
    "        \"Certificate\": {\"correct\": 0.0, \"num\": 0}\n",
    "    }\n",
    "    for subj, info in subject_mapping.items():\n",
    "        group = info[2]\n",
    "        summary['grouped'][group][\"num\"] += summary[subj]['num']\n",
    "        summary['grouped'][group][\"correct\"] += summary[subj]['correct']\n",
    "    for group, info in summary['grouped'].items():\n",
    "        info['score'] = info[\"correct\"] / info[\"num\"] if info[\"num\"] != 0 else 0\n",
    "        total_num += info[\"num\"]\n",
    "        total_correct += info[\"correct\"]\n",
    "    summary['All'] = {\"score\": total_correct / total_num, \"num\": total_num, \"correct\": total_correct}\n",
    "\n",
    "    print('-' * 80)\n",
    "    print(\"Accuracy_subject:\")\n",
    "    for k, v in accuracy.items():\n",
    "        print(k, \": \", v)\n",
    "    print('-' * 80)\n",
    "    print(\"Accuracy_grouped:\")\n",
    "    for k, v in summary['grouped'].items():\n",
    "        print(k, \": \", v['score'])\n",
    "\n",
    "    print(\"Avg: \")\n",
    "    print(summary['All']['score'])\n",
    "\n",
    "    json.dump(summary, open(save_result_dir + f'/{nodes_group}_summary.json', 'w'), ensure_ascii=False, indent=2)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18e3117",
   "metadata": {},
   "outputs": [],
   "source": [
    "cot = False\n",
    "few_shot = False\n",
    "ntrain = 5\n",
    "n_times = 1\n",
    "do_save_csv = False\n",
    "output_dir = eval_path + 'output'\n",
    "model_name = 'chatglm'\n",
    "do_test = False\n",
    "args = dict(\n",
    "    cot=cot,\n",
    "    few_shot = few_shot,\n",
    "    ntrain = ntrain,\n",
    "    n_times = n_times,\n",
    "    do_save_csv = do_save_csv,\n",
    "    output_dir = output_dir,\n",
    "    model_name = model_name,\n",
    "    do_test = do_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6a58ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when we evaluate which group of parameter is better, we take 'microeconomics' for instance  \n",
    "subject = 'microeconomics'\n",
    "\n",
    "# record all eval summary\n",
    "all_summary_records = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89791020",
   "metadata": {},
   "source": [
    "#### ChatGLM3-6B baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df3e1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_summary_dc = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f32b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai_evaluator import OpenAI_Evaluator\n",
    "nodes_group = 'benchmark'\n",
    "for cot in [False, True]:\n",
    "    for few_shot in [False, True]:\n",
    "        args['cot'] = cot\n",
    "        args['few_shot'] = few_shot\n",
    "        args['nodes_group'] = nodes_group\n",
    "        evaluator = OpenAI_Evaluator(choices=choices, k=args['ntrain'], model_name=args['model_name'])\n",
    "        # for file path detemined by whether we use cot or few shot in prompt\n",
    "        prompt_type = ('' if cot else 'no_') + 'cot_' + \\\n",
    "            (('and_' if cot else 'but_') if few_shot else ('but_' if cot else 'and_') + 'no_') + 'few_shot' \n",
    "        take = f'{nodes_group}/' + prompt_type\n",
    "        benchmark_summary = fineval(args=args, evaluator=evaluator, take=take, \n",
    "                                    subject=subject)\n",
    "        benchmark_summary_dc[prompt_type] = benchmark_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db20c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_group = 'benchmark'\n",
    "if not benchmark_summary_dc:\n",
    "    for cot in [False, True]:\n",
    "        for few_shot in [False, True]:\n",
    "            # for file path detemined by whether we use cot or few shot in prompt\n",
    "            prompt_type = ('' if cot else 'no_') + 'cot_' + \\\n",
    "            (('and_' if cot else 'but_') if few_shot else ('but_' if cot else 'and_') + 'no_') + 'few_shot' \n",
    "            take = f'{nodes_group}/' + prompt_type\n",
    "            with open(os.path.join(args['output_dir'],take) + f'/{nodes_group}_summary.json') as f:\n",
    "                benchmark_summary = json.load(f)\n",
    "            benchmark_summary_dc[prompt_type] = benchmark_summary\n",
    "\n",
    "benchmark_summary_record = {'method': nodes_group, 'nodes_group': nodes_group, 'retrieve_mode': 'not retrieve'}\n",
    "for cot in [False, True]:\n",
    "    for few_shot in [False, True]:\n",
    "        prompt_type = ('' if cot else 'no_') + 'cot_' + \\\n",
    "            (('and_' if cot else 'but_') if few_shot else ('but_' if cot else 'and_') + 'no_') + 'few_shot' \n",
    "        benchmark_summary_record[prompt_type] = benchmark_summary_dc[prompt_type][subject]['score']\n",
    "all_summary_records.append(benchmark_summary_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7d3cd5",
   "metadata": {},
   "source": [
    "### 5.1 Compare different query engines base on different retrieve mode and nodes of different chunk sizes and chunk overlaps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c0d779",
   "metadata": {},
   "source": [
    "#### Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6921ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size_ls = [256, 512, 1024]\n",
    "chunk_overlap_pct_ls = [1/16, 1/8]\n",
    "nodes_group_ls = []\n",
    "for chunk_size in chunk_size_ls:\n",
    "    for chunk_overlap_pct in chunk_overlap_pct_ls:\n",
    "        chunk_overlap = int(chunk_size * chunk_overlap_pct)\n",
    "        nodes_group = f'size_{chunk_size}_overlap_{chunk_overlap}'\n",
    "        nodes_group_ls.append(nodes_group)\n",
    "query_engine_dc = get_all_query_engine_from_cache_index(kg_index_folder_path='../storage/storage_graph',\n",
    "                                                        vector_index_folder_path='../storage/storage_vector',\n",
    "                                                        nodes_group=nodes_group_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75a8fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_summary_template = \\\n",
    "    \"从不同来源获取的参考信息如下:\\n\" \\\n",
    "    \"---------------------\\n\" \\\n",
    "    \"{context_str}\\n\" \\\n",
    "    \"---------------------\\n\" \\\n",
    "    \"题目:{query_str}\" \n",
    "\n",
    "prompt_dict = dict(\n",
    "    summary_template = [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': tree_summary_template\n",
    "        }])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4600aa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine_summary_dc = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3eb89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from query_engine_evaluator import QueryEngineEvaluator\n",
    "for query_engine_type in query_engine_dc.keys():\n",
    "    if query_engine_type not in query_engine_summary_dc.keys():\n",
    "        query_engine_summary_dc[query_engine_type] = {}\n",
    "    for cot in [False, True]:\n",
    "        for few_shot in [False, True]:\n",
    "            prompt_type = ('' if cot else 'no_') + 'cot_' + \\\n",
    "                    (('and_' if cot else 'but_') if few_shot else ('but_' if cot else 'and_') + 'no_') + 'few_shot' \n",
    "            if prompt_type not in query_engine_summary_dc[query_engine_type].keys():\n",
    "                query_engine_summary_dc[query_engine_type][prompt_type] = {}\n",
    "            for retrieve_mode in ['retrieve_only_question', 'retrieve_with_choices']:\n",
    "                if retrieve_mode not in query_engine_summary_dc[query_engine_type][prompt_type].keys():\n",
    "                    query_engine_summary_dc[query_engine_type][prompt_type][retrieve_mode] = {}\n",
    "                for nodes_group in nodes_group_ls:\n",
    "                    if nodes_group not in query_engine_summary_dc[query_engine_type][prompt_type][retrieve_mode].keys():\n",
    "                        query_engine_summary_dc[query_engine_type][prompt_type][retrieve_mode][nodes_group] = {}\n",
    "                    args['nodes_group'] = nodes_group\n",
    "                    args['cot'] = cot\n",
    "                    args['few_shot'] = few_shot\n",
    "                    query_engine = query_engine_dc[query_engine_type][nodes_group]\n",
    "                    retrieve_choice = retrieve_mode == 'retrieve_with_choices'\n",
    "                    evaluator = QueryEngineEvaluator(query_engine=query_engine, prompt_dict=prompt_dict, retrieve_choice=retrieve_choice,\n",
    "                                                    choices=choices, k=args['ntrain'], model_name=args['model_name'])\n",
    "                    \n",
    "                    take = f'{query_engine_type}/{prompt_type}/{retrieve_mode}'\n",
    "                    summary = fineval(args=args, evaluator=evaluator, take=take, subject=subject)\n",
    "                    query_engine_summary_dc[query_engine_type][prompt_type][retrieve_mode][nodes_group] = summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed485e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not query_engine_summary_dc:\n",
    "    for query_engine_type in query_engine_dc.keys():\n",
    "        if query_engine_type not in query_engine_summary_dc.keys():\n",
    "            query_engine_summary_dc[query_engine_type] = {}\n",
    "        for cot in [False, True]:\n",
    "            for few_shot in [False, True]:\n",
    "                prompt_type = ('' if cot else 'no_') + 'cot_' + \\\n",
    "                        (('and_' if cot else 'but_') if few_shot else ('but_' if cot else 'and_') + 'no_') + 'few_shot' \n",
    "                if prompt_type not in query_engine_summary_dc[query_engine_type].keys():\n",
    "                    query_engine_summary_dc[query_engine_type][prompt_type] = {}\n",
    "                for retrieve_mode in ['retrieve_only_question', 'retrieve_with_choices']:\n",
    "                    if retrieve_mode not in query_engine_summary_dc[query_engine_type][prompt_type].keys():\n",
    "                        query_engine_summary_dc[query_engine_type][prompt_type][retrieve_mode] = {}\n",
    "                    for nodes_group in nodes_group_ls:\n",
    "                        if nodes_group not in query_engine_summary_dc[query_engine_type][prompt_type][retrieve_mode].keys():\n",
    "                            query_engine_summary_dc[query_engine_type][prompt_type][retrieve_mode][nodes_group] = {}\n",
    "                        take = f\"{query_engine_type}/{prompt_type}/{retrieve_mode}\"\n",
    "                        with open(f\"{args['output_dir']}/{take}/{nodes_group}_summary.json\") as f:\n",
    "                            summary = json.load(f)\n",
    "                        query_engine_summary_dc[query_engine_type][prompt_type][retrieve_mode][nodes_group] = summary\n",
    "\n",
    "prompt_type_ls = []\n",
    "for query_engine_type in query_engine_dc.keys():\n",
    "    for retrieve_mode in ['retrieve_only_question', 'retrieve_with_choices']:\n",
    "        for nodes_group in nodes_group_ls:\n",
    "            query_engine_summary_record = {'method': query_engine_type, 'nodes_group': nodes_group, 'retrieve_mode': retrieve_mode}\n",
    "            for cot in [False, True]:\n",
    "                for few_shot in [False, True]:\n",
    "                    prompt_type = ('' if cot else 'no_') + 'cot_' + \\\n",
    "                            (('and_' if cot else 'but_') if few_shot else ('but_' if cot else 'and_') + 'no_') + 'few_shot' \n",
    "                    prompt_type_ls.append(prompt_type)\n",
    "                    query_engine_summary_record[prompt_type] = \\\n",
    "                        query_engine_summary_dc[query_engine_type][prompt_type][retrieve_mode][nodes_group][subject]['score']\n",
    "            all_summary_records.append(query_engine_summary_record) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1293f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df_all_summary = pd.DataFrame(all_summary_records)\n",
    "# display(Markdown(f\"\"\"\n",
    "#     ** Financial Evaluation on different query engine with different parameters and baseline ChatGLM3-6B: **\n",
    "#     {df_all_summary}\n",
    "# \"\"\"))\n",
    "df_all_summary['Avg'] = np.mean(df_all_summary[prompt_type_ls], axis=1)\n",
    "df_all_summary['Max'] = np.max(df_all_summary[prompt_type_ls], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d280d60",
   "metadata": {},
   "source": [
    "**Compare different chunk size and chunk overlap**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2972c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_summary[df_all_summary['retrieve_mode'].isin(['not retrieve', 'retrieve_with_choices'])].drop(['retrieve_mode', 'nodes_group'], axis=1).groupby(['method']).mean().round(2).reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdb2076",
   "metadata": {},
   "source": [
    "### 5.2 Compare retriever parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee510d2",
   "metadata": {},
   "source": [
    "**Graph RAG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a89761",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_group = 'benchmark'\n",
    "benchmark_summary_dc = {}\n",
    "if not benchmark_summary_dc:\n",
    "    for cot in [False, True]:\n",
    "        for few_shot in [False, True]:\n",
    "            # for file path detemined by whether we use cot or few shot in prompt\n",
    "            prompt_type = ('' if cot else 'no_') + 'cot_' + \\\n",
    "            (('and_' if cot else 'but_') if few_shot else ('but_' if cot else 'and_') + 'no_') + 'few_shot' \n",
    "            take = f'{nodes_group}/' + prompt_type\n",
    "            with open(os.path.join(args['output_dir'],take) + f'/{nodes_group}_summary.json') as f:\n",
    "                benchmark_summary = json.load(f)\n",
    "            benchmark_summary_dc[prompt_type] = benchmark_summary\n",
    "\n",
    "benchmark_summary_record = {'method': nodes_group, 'subgraph_size': 'no_subgraph'}\n",
    "for cot in [False, True]:\n",
    "    for few_shot in [False, True]:\n",
    "        prompt_type = ('' if cot else 'no_') + 'cot_' + \\\n",
    "            (('and_' if cot else 'but_') if few_shot else ('but_' if cot else 'and_') + 'no_') + 'few_shot' \n",
    "        benchmark_summary_record[prompt_type] = benchmark_summary_dc[prompt_type][subject]['score']\n",
    "all_summary_records.append(benchmark_summary_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f896e066",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_group = '微观经济学'\n",
    "space_name = f\"book_{nodes_group}_size_512_overlap_64\"\n",
    "edge_types, rel_prop_names = [\"关系\"], [\"关系\"] # default, could be omit if create from an empty kg\n",
    "tags = [\"实体\"] # default, could be omit if create from an empty kg\n",
    "\n",
    "graph_store = NebulaGraphStore(\n",
    "    space_name=space_name,\n",
    "    edge_types=edge_types,\n",
    "    rel_prop_names=rel_prop_names,\n",
    "    tags=tags,\n",
    ")\n",
    "storage_context_kg = StorageContext.from_defaults(persist_dir=f'../storage/storage_graph/{nodes_group}', graph_store=graph_store)\n",
    "kg_index = load_index_from_storage(\n",
    "    storage_context=storage_context_kg,\n",
    "    space_name=space_name,\n",
    "    edge_types=edge_types,\n",
    "    rel_prop_names=rel_prop_names,\n",
    "    tags=tags,\n",
    "    include_embeddings=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3369d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_summary_template = \\\n",
    "    \"从不同来源获取的参考信息如下:\\n\" \\\n",
    "    \"---------------------\\n\" \\\n",
    "    \"{context_str}\\n\" \\\n",
    "    \"---------------------\\n\" \\\n",
    "    \"题目:{query_str}\" \n",
    "\n",
    "prompt_dict = dict(\n",
    "    summary_template = [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': tree_summary_template\n",
    "        }])\n",
    "query_engine_summary_dc = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773fe1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from query_engine_evaluator import QueryEngineEvaluator\n",
    "query_engine_dc = {}\n",
    "for depth, breadth in zip([1, 2, 3, 4], [60, 30, 20, 15]):\n",
    "    subgraph_size = f'depth_{depth}_breadth_{breadth}'\n",
    "    # kg_rag query engine\n",
    "    kg_rag_query_engine = kg_index.as_query_engine(\n",
    "        include_text=False,\n",
    "        graph_store_query_depth=depth,\n",
    "        max_knowledge_sequence=breadth,\n",
    "        response_mode=\"tree_summarize\"\n",
    "    )\n",
    "    query_engine_dc[subgraph_size] = kg_rag_query_engine\n",
    "    \n",
    "for depth, breadth in zip([1, 2, 3, 4], [60, 30, 20, 15]):\n",
    "    subgraph_size = f'depth_{depth}_breadth_{breadth}'\n",
    "    if subgraph_size not in query_engine_summary_dc.keys():\n",
    "        query_engine_summary_dc[subgraph_size] = {}\n",
    "    for cot in [False, True]:\n",
    "        for few_shot in [False, True]:\n",
    "            prompt_type = ('' if cot else 'no_') + 'cot_' + \\\n",
    "                    (('and_' if cot else 'but_') if few_shot else ('but_' if cot else 'and_') + 'no_') + 'few_shot' \n",
    "            if prompt_type not in query_engine_summary_dc[subgraph_size].keys():\n",
    "                query_engine_summary_dc[subgraph_size][prompt_type] = {}\n",
    "            args['nodes_group'] = subgraph_size\n",
    "            args['cot'] = cot\n",
    "            args['few_shot'] = few_shot\n",
    "            query_engine = query_engine_dc[subgraph_size]\n",
    "            evaluator = QueryEngineEvaluator(query_engine=query_engine, prompt_dict=prompt_dict, retrieve_choice='True',\n",
    "                                            choices=choices, k=args['ntrain'], model_name=args['model_name'])\n",
    "            \n",
    "            take = f'kg_rag/{prompt_type}/subgraph_size'\n",
    "            summary = fineval(args=args, evaluator=evaluator, take=take, subject=subject)\n",
    "            query_engine_summary_dc[subgraph_size][prompt_type] = summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaca6943",
   "metadata": {},
   "source": [
    "## Vector RAG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab68215",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_group = '微观经济学'\n",
    "storage_context_vec = StorageContext.from_defaults(persist_dir=f'../storage/storage_vector/{nodes_group}', graph_store=graph_store)\n",
    "vector_index = load_index_from_storage(\n",
    "    storage_context=storage_context_vec\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e75611",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_summary_template = \\\n",
    "    \"从不同来源获取的参考信息如下:\\n\" \\\n",
    "    \"---------------------\\n\" \\\n",
    "    \"{context_str}\\n\" \\\n",
    "    \"---------------------\\n\" \\\n",
    "    \"题目:{query_str}\" \n",
    "\n",
    "prompt_dict = dict(\n",
    "    summary_template = [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': tree_summary_template\n",
    "        }])\n",
    "query_engine_summary_dc = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26a6480",
   "metadata": {},
   "outputs": [],
   "source": [
    "from query_engine_evaluator import QueryEngineEvaluator\n",
    "query_engine_dc = {}\n",
    "for similarity_top_k in [3,4,5,6]:\n",
    "    top_k = f'top_{similarity_top_k}'\n",
    "    vec_rag_query_engine = vector_index.as_query_engine(\n",
    "        similarity_top_k = similarity_top_k,\n",
    "        response_mode=\"tree_summarize\"\n",
    "    )\n",
    "    query_engine_dc[top_k] = vec_rag_query_engine\n",
    "    \n",
    "for similarity_top_k in [3,4,5,6]:\n",
    "    top_k = f'top_{similarity_top_k}'\n",
    "    if top_k not in query_engine_summary_dc.keys():\n",
    "        query_engine_summary_dc[top_k] = {}\n",
    "    for cot in [False, True]:\n",
    "        for few_shot in [False, True]:\n",
    "            prompt_type = ('' if cot else 'no_') + 'cot_' + \\\n",
    "                    (('and_' if cot else 'but_') if few_shot else ('but_' if cot else 'and_') + 'no_') + 'few_shot' \n",
    "            if prompt_type not in query_engine_summary_dc[top_k].keys():\n",
    "                query_engine_summary_dc[top_k][prompt_type] = {}\n",
    "            args['nodes_group'] = subgraph_size\n",
    "            args['cot'] = cot\n",
    "            args['few_shot'] = few_shot\n",
    "            query_engine = query_engine_dc[top_k]\n",
    "            evaluator = QueryEngineEvaluator(query_engine=query_engine, prompt_dict=prompt_dict, retrieve_choice='True',\n",
    "                                            choices=choices, k=args['ntrain'], model_name=args['model_name'])\n",
    "            \n",
    "            take = f'vec_rag/{prompt_type}/top_k'\n",
    "            summary = fineval(args=args, evaluator=evaluator, take=take, subject=subject)\n",
    "            query_engine_summary_dc[top_k][prompt_type] = summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c2643f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_summary_records = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84216edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_group = 'benchmark'\n",
    "benchmark_summary_dc = {}\n",
    "if not benchmark_summary_dc:\n",
    "    for cot in [False, True]:\n",
    "        for few_shot in [False, True]:\n",
    "            # for file path detemined by whether we use cot or few shot in prompt\n",
    "            prompt_type = ('' if cot else 'no_') + 'cot_' + \\\n",
    "            (('and_' if cot else 'but_') if few_shot else ('but_' if cot else 'and_') + 'no_') + 'few_shot' \n",
    "            take = f'{nodes_group}/' + prompt_type\n",
    "            with open(os.path.join(args['output_dir'],take) + f'/{nodes_group}_summary.json') as f:\n",
    "                benchmark_summary = json.load(f)\n",
    "            benchmark_summary_dc[prompt_type] = benchmark_summary\n",
    "\n",
    "benchmark_summary_record = {'method': nodes_group, 'top_k': '0'}\n",
    "for cot in [False, True]:\n",
    "    for few_shot in [False, True]:\n",
    "        prompt_type = ('' if cot else 'no_') + 'cot_' + \\\n",
    "            (('and_' if cot else 'but_') if few_shot else ('but_' if cot else 'and_') + 'no_') + 'few_shot' \n",
    "        benchmark_summary_record[prompt_type] = benchmark_summary_dc[prompt_type][subject]['score']\n",
    "all_summary_records.append(benchmark_summary_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4cfe42",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_group = 'vector_rag'\n",
    "prompt_type_ls = []\n",
    "for similarity_top_k in [3,4,5,6]:\n",
    "    top_k = f'top_{similarity_top_k}'\n",
    "    vec_record = {'method': nodes_group}\n",
    "    for cot in [False, True]:\n",
    "        for few_shot in [False, True]:\n",
    "            prompt_type = ('' if cot else 'no_') + 'cot_' + \\\n",
    "                (('and_' if cot else 'but_') if few_shot else ('but_' if cot else 'and_') + 'no_') + 'few_shot' \n",
    "            prompt_type_ls.append(prompt_type)\n",
    "            vec_record['top_k'] = top_k\n",
    "            vec_record[prompt_type] = query_engine_summary_dc[top_k][prompt_type][subject]['score']\n",
    "    all_summary_records.append(vec_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055d6ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df_all_summary = pd.DataFrame(all_summary_records)\n",
    "df_all_summary['Avg'] = np.mean(df_all_summary[prompt_type_ls], axis=1)\n",
    "df_all_summary['Max'] = np.max(df_all_summary[prompt_type_ls], axis=1)\n",
    "df_all_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d34a89",
   "metadata": {},
   "source": [
    "## 5.3 Systematic Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f004a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_subject_mapping = {\n",
    "    '金融学': 'Finance',\n",
    "    '投资学': 'Finance',\n",
    "    '货币金融学': 'Finance',\n",
    "    '公司理财': 'Finance',\n",
    "    'CPA战略': 'Finance',\n",
    "    'CPA会计': 'accounting',\n",
    "    'CPA审计': 'auditing',\n",
    "    'CPA税法': 'Accounting',\n",
    "    'CPA财务成本管理': 'Accounting',\n",
    "    'CPA经济法': 'Economy',\n",
    "    '宏观经济学': 'Economy',\n",
    "    '微观经济学': 'Economy',\n",
    "    '计量经济学': 'Economy',\n",
    "}\n",
    "with open('book_subject_mapping.json', 'w') as f:\n",
    "    f.write(json.dumps(book_subject_mapping, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5aec5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('subject_mapping.json', 'r') as f:\n",
    "    subject_mapping = json.load(f)\n",
    "subjects = subject_mapping.keys()\n",
    "subjects = [subject for subject in subjects if subject not in ['banking_practitioner_qualification_certificate', 'fund_qualification_certificate', 'futures_practitioner_qualification_certificate',\n",
    " 'securities_practitioner_qualification_certificate','statistics', 'financial_engineering', 'investments', 'monetary_finance']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0c2c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_summary_records = []\n",
    "benchmark_summary_dc = {}\n",
    "query_engine_summary_dc = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc818b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai_evaluator import OpenAI_Evaluator\n",
    "nodes_group = 'benchmark'\n",
    "for cot in [False, True]:\n",
    "    for few_shot in [False, True]:\n",
    "        args['cot'] = cot\n",
    "        args['few_shot'] = few_shot\n",
    "        args['nodes_group'] = nodes_group\n",
    "        evaluator = OpenAI_Evaluator(choices=choices, k=args['ntrain'], model_name=args['model_name'])\n",
    "        # for file path detemined by whether we use cot or few shot in prompt\n",
    "        prompt_type = ('' if cot else 'no_') + 'cot_' + \\\n",
    "            (('and_' if cot else 'but_') if few_shot else ('but_' if cot else 'and_') + 'no_') + 'few_shot' \n",
    "        take = f'{nodes_group}/' + prompt_type\n",
    "        benchmark_summary = fineval(args=args, evaluator=evaluator, take=take, \n",
    "                                    subject=subjects)\n",
    "        benchmark_summary_dc[prompt_type] = benchmark_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368a3bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not benchmark_summary_dc:\n",
    "    for cot in [False, True]:\n",
    "        for few_shot in [False, True]:\n",
    "            # for file path detemined by whether we use cot or few shot in prompt\n",
    "            prompt_type = ('' if cot else 'no_') + 'cot_' + \\\n",
    "            (('and_' if cot else 'but_') if few_shot else ('but_' if cot else 'and_') + 'no_') + 'few_shot' \n",
    "            take = f'{nodes_group}/' + prompt_type\n",
    "            with open(os.path.join(args['output_dir'],take) + f'/{nodes_group}_summary.json') as f:\n",
    "                benchmark_summary = json.load(f)\n",
    "            benchmark_summary_dc[prompt_type] = benchmark_summary\n",
    "\n",
    "benchmark_summary_record = {'method': nodes_group}\n",
    "subject_group_ls = ['Finance', 'Accounting', 'Economy', 'Certificate']\n",
    "for subject_group in subject_group_ls\n",
    "    for cot in [False, True]:\n",
    "        for few_shot in [False, True]:\n",
    "            prompt_type = ('' if cot else 'no_') + 'cot_' + \\\n",
    "                (('and_' if cot else 'but_') if few_shot else ('but_' if cot else 'and_') + 'no_') + 'few_shot' \n",
    "            benchmark_summary_record[prompt_type] = benchmark_summary_dc[prompt_type]['grouped'][subject_group]['score']\n",
    "all_summary_records.append(benchmark_summary_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d609d7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_summary_template = \\\n",
    "    \"从不同来源获取的参考信息如下:\\n\" \\\n",
    "    \"---------------------\\n\" \\\n",
    "    \"{context_str}\\n\" \\\n",
    "    \"---------------------\\n\" \\\n",
    "    \"题目:{query_str}\" \n",
    "\n",
    "prompt_dict = dict(\n",
    "    summary_template = [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': tree_summary_template\n",
    "        }])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0f6585",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine_dc = get_all_query_engine_from_cache_index(kg_index_folder_path='../storage/storage_graph',\n",
    "                                                        vector_index_folder_path='../storage/storage_vector',\n",
    "                                                        nodes_group='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a7514c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from query_engine_evaluator import QueryEngineEvaluator\n",
    "nodes_group = 'all'\n",
    "retrieve_mode = 'retrieve_with_choices'\n",
    "for query_engine_type in query_engine_dc.keys():\n",
    "    if query_engine_type not in query_engine_summary_dc.keys():\n",
    "        query_engine_summary_dc[query_engine_type] = {}\n",
    "    for cot in [False, True]:\n",
    "        for few_shot in [False, True]:\n",
    "            prompt_type = ('' if cot else 'no_') + 'cot_' + \\\n",
    "                    (('and_' if cot else 'but_') if few_shot else ('but_' if cot else 'and_') + 'no_') + 'few_shot' \n",
    "            if nodes_group not in query_engine_summary_dc[query_engine_type].keys():\n",
    "                query_engine_summary_dc[query_engine_type][nodes_group] = {}\n",
    "            if prompt_type not in query_engine_summary_dc[query_engine_type][nodes_group].keys():\n",
    "                query_engine_summary_dc[query_engine_type][nodes_group][prompt_type] = {}\n",
    "            args['nodes_group'] = nodes_group\n",
    "            args['cot'] = cot\n",
    "            args['few_shot'] = few_shot\n",
    "            query_engine = query_engine_dc[query_engine_type][nodes_group]\n",
    "            retrieve_choice = retrieve_mode == 'retrieve_with_choices'\n",
    "            evaluator = QueryEngineEvaluator(query_engine=query_engine, prompt_dict=prompt_dict, retrieve_choice=retrieve_choice,\n",
    "                                            choices=choices, k=args['ntrain'], model_name=args['model_name'])\n",
    "            \n",
    "            take = f'{query_engine_type}/{prompt_type}/{nodes_group}'\n",
    "            summary = fineval(args=args, evaluator=evaluator, take=take, subject=subjects)\n",
    "            query_engine_summary_dc[query_engine_type][nodes_group][prompt_type] = summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d7b0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not query_engine_summary_dc:\n",
    "    for query_engine_type in query_engine_dc.keys():\n",
    "        if query_engine_type not in query_engine_summary_dc.keys():\n",
    "            query_engine_summary_dc[query_engine_type] = {}\n",
    "        for cot in [False, True]:\n",
    "            for few_shot in [False, True]:\n",
    "                prompt_type = ('' if cot else 'no_') + 'cot_' + \\\n",
    "                        (('and_' if cot else 'but_') if few_shot else ('but_' if cot else 'and_') + 'no_') + 'few_shot' \n",
    "                if nodes_group not in query_engine_summary_dc[query_engine_type].keys():\n",
    "                    query_engine_summary_dc[query_engine_type][nodes_group] = {}\n",
    "                if prompt_type not in query_engine_summary_dc[query_engine_type][nodes_group].keys():\n",
    "                    query_engine_summary_dc[query_engine_type][nodes_group][prompt_type] = {}\n",
    "                take = f\"{query_engine_type}/{prompt_type}/{nodes_group}\"\n",
    "                with open(f\"{args['output_dir']}/{take}/{nodes_group}_summary.json\") as f:\n",
    "                    summary = json.load(f)\n",
    "                query_engine_summary_dc[query_engine_type][nodes_group][prompt_type] = summary\n",
    "\n",
    "prompt_type_ls = []\n",
    "subject_group_ls = ['Finance', 'Accounting', 'Economy', 'Certificate']\n",
    "for query_engine_type in query_engine_dc.keys():\n",
    "    query_engine_summary_record = {'method': query_engine_type}\n",
    "    for subject_group in subject_group_ls:\n",
    "        query_engine_summary_record['subject_group'] = subject_group\n",
    "        for cot in [False, True]:\n",
    "            for few_shot in [False, True]:\n",
    "                prompt_type = ('' if cot else 'no_') + 'cot_' + \\\n",
    "                        (('and_' if cot else 'but_') if few_shot else ('but_' if cot else 'and_') + 'no_') + 'few_shot' \n",
    "                prompt_type_ls.append(prompt_type)\n",
    "                query_engine_summary_record[prompt_type] = \\\n",
    "                    query_engine_summary_dc[query_engine_type][nodes_group][prompt_type]['grouped'][subject_group]['score']\n",
    "    all_summary_records.append(query_engine_summary_record) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatglm3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
